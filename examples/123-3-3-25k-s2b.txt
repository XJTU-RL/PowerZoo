choose to use gpu...
3 {'env_name': '123Bus', 'seed': 123456, 'num_steps': 1000, 'num_workers': 'None', 'use_plot': False, 'do_testing': False, 'mode': 'single', 'useS': True, 'big2small': False}
make_train_env= 3
make_eval_env= 3
share_observation_space:  [Box([ 0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.   0.
  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -1.   0.  -1.   0.
 -1.   0.  -1. ], [ 1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.   1.
  1.   1.  33.  33.  33.  33.  33.  33.  33.   1.   1.   1.   1.   1.
  1.   1.   1. ], (297,), float32), Box([ 0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.   0.
  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -1.   0.  -1.   0.
 -1.   0.  -1. ], [ 1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.   1.
  1.   1.  33.  33.  33.  33.  33.  33.  33.   1.   1.   1.   1.   1.
  1.   1.   1. ], (297,), float32), Box([ 0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.   0.
  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -1.   0.  -1.   0.
 -1.   0.  -1. ], [ 1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.   1.
  1.   1.  33.  33.  33.  33.  33.  33.  33.   1.   1.   1.   1.   1.
  1.   1.   1. ], (297,), float32), Box([ 0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.   0.
  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -1.   0.  -1.   0.
 -1.   0.  -1. ], [ 1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.   1.
  1.   1.  33.  33.  33.  33.  33.  33.  33.   1.   1.   1.   1.   1.
  1.   1.   1. ], (297,), float32), Box([ 0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.   0.
  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -1.   0.  -1.   0.
 -1.   0.  -1. ], [ 1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.   1.
  1.   1.  33.  33.  33.  33.  33.  33.  33.   1.   1.   1.   1.   1.
  1.   1.   1. ], (297,), float32), Box([ 0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.   0.
  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -1.   0.  -1.   0.
 -1.   0.  -1. ], [ 1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.   1.
  1.   1.  33.  33.  33.  33.  33.  33.  33.   1.   1.   1.   1.   1.
  1.   1.   1. ], (297,), float32), Box([ 0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.   0.
  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -1.   0.  -1.   0.
 -1.   0.  -1. ], [ 1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.   1.
  1.   1.  33.  33.  33.  33.  33.  33.  33.   1.   1.   1.   1.   1.
  1.   1.   1. ], (297,), float32), Box([ 0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.   0.
  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -1.   0.  -1.   0.
 -1.   0.  -1. ], [ 1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.   1.
  1.   1.  33.  33.  33.  33.  33.  33.  33.   1.   1.   1.   1.   1.
  1.   1.   1. ], (297,), float32), Box([ 0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.   0.
  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -1.   0.  -1.   0.
 -1.   0.  -1. ], [ 1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.   1.
  1.   1.  33.  33.  33.  33.  33.  33.  33.   1.   1.   1.   1.   1.
  1.   1.   1. ], (297,), float32), Box([ 0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.   0.
  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -1.   0.  -1.   0.
 -1.   0.  -1. ], [ 1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.   1.
  1.   1.  33.  33.  33.  33.  33.  33.  33.   1.   1.   1.   1.   1.
  1.   1.   1. ], (297,), float32), Box([ 0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.   0.
  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -1.   0.  -1.   0.
 -1.   0.  -1. ], [ 1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.   1.
  1.   1.  33.  33.  33.  33.  33.  33.  33.   1.   1.   1.   1.   1.
  1.   1.   1. ], (297,), float32), Box([ 0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.   0.
  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -1.   0.  -1.   0.
 -1.   0.  -1. ], [ 1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.   1.
  1.   1.  33.  33.  33.  33.  33.  33.  33.   1.   1.   1.   1.   1.
  1.   1.   1. ], (297,), float32), Box([ 0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.   0.
  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -1.   0.  -1.   0.
 -1.   0.  -1. ], [ 1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.   1.
  1.   1.  33.  33.  33.  33.  33.  33.  33.   1.   1.   1.   1.   1.
  1.   1.   1. ], (297,), float32), Box([ 0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.   0.
  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -1.   0.  -1.   0.
 -1.   0.  -1. ], [ 1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.   1.
  1.   1.  33.  33.  33.  33.  33.  33.  33.   1.   1.   1.   1.   1.
  1.   1.   1. ], (297,), float32), Box([ 0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.   0.
  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -1.   0.  -1.   0.
 -1.   0.  -1. ], [ 1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.   1.
  1.   1.  33.  33.  33.  33.  33.  33.  33.   1.   1.   1.   1.   1.
  1.   1.   1. ], (297,), float32)]
observation_space:  [Box([ 0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.   0.
  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -1.   0.  -1.   0.
 -1.   0.  -1. ], [ 1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.   1.
  1.   1.  33.  33.  33.  33.  33.  33.  33.   1.   1.   1.   1.   1.
  1.   1.   1. ], (297,), float32), Box([ 0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.   0.
  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -1.   0.  -1.   0.
 -1.   0.  -1. ], [ 1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.   1.
  1.   1.  33.  33.  33.  33.  33.  33.  33.   1.   1.   1.   1.   1.
  1.   1.   1. ], (297,), float32), Box([ 0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.   0.
  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -1.   0.  -1.   0.
 -1.   0.  -1. ], [ 1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.   1.
  1.   1.  33.  33.  33.  33.  33.  33.  33.   1.   1.   1.   1.   1.
  1.   1.   1. ], (297,), float32), Box([ 0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.   0.
  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -1.   0.  -1.   0.
 -1.   0.  -1. ], [ 1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.   1.
  1.   1.  33.  33.  33.  33.  33.  33.  33.   1.   1.   1.   1.   1.
  1.   1.   1. ], (297,), float32), Box([ 0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.   0.
  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -1.   0.  -1.   0.
 -1.   0.  -1. ], [ 1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.   1.
  1.   1.  33.  33.  33.  33.  33.  33.  33.   1.   1.   1.   1.   1.
  1.   1.   1. ], (297,), float32), Box([ 0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.   0.
  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -1.   0.  -1.   0.
 -1.   0.  -1. ], [ 1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.   1.
  1.   1.  33.  33.  33.  33.  33.  33.  33.   1.   1.   1.   1.   1.
  1.   1.   1. ], (297,), float32), Box([ 0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.   0.
  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -1.   0.  -1.   0.
 -1.   0.  -1. ], [ 1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.   1.
  1.   1.  33.  33.  33.  33.  33.  33.  33.   1.   1.   1.   1.   1.
  1.   1.   1. ], (297,), float32), Box([ 0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.   0.
  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -1.   0.  -1.   0.
 -1.   0.  -1. ], [ 1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.   1.
  1.   1.  33.  33.  33.  33.  33.  33.  33.   1.   1.   1.   1.   1.
  1.   1.   1. ], (297,), float32), Box([ 0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.   0.
  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -1.   0.  -1.   0.
 -1.   0.  -1. ], [ 1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.   1.
  1.   1.  33.  33.  33.  33.  33.  33.  33.   1.   1.   1.   1.   1.
  1.   1.   1. ], (297,), float32), Box([ 0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.   0.
  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -1.   0.  -1.   0.
 -1.   0.  -1. ], [ 1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.   1.
  1.   1.  33.  33.  33.  33.  33.  33.  33.   1.   1.   1.   1.   1.
  1.   1.   1. ], (297,), float32), Box([ 0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.   0.
  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -1.   0.  -1.   0.
 -1.   0.  -1. ], [ 1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.   1.
  1.   1.  33.  33.  33.  33.  33.  33.  33.   1.   1.   1.   1.   1.
  1.   1.   1. ], (297,), float32), Box([ 0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.   0.
  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -1.   0.  -1.   0.
 -1.   0.  -1. ], [ 1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.   1.
  1.   1.  33.  33.  33.  33.  33.  33.  33.   1.   1.   1.   1.   1.
  1.   1.   1. ], (297,), float32), Box([ 0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.   0.
  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -1.   0.  -1.   0.
 -1.   0.  -1. ], [ 1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.   1.
  1.   1.  33.  33.  33.  33.  33.  33.  33.   1.   1.   1.   1.   1.
  1.   1.   1. ], (297,), float32), Box([ 0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.   0.
  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -1.   0.  -1.   0.
 -1.   0.  -1. ], [ 1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.   1.
  1.   1.  33.  33.  33.  33.  33.  33.  33.   1.   1.   1.   1.   1.
  1.   1.   1. ], (297,), float32), Box([ 0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.   0.
  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -1.   0.  -1.   0.
 -1.   0.  -1. ], [ 1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.   1.
  1.   1.  33.  33.  33.  33.  33.  33.  33.   1.   1.   1.   1.   1.
  1.   1.   1. ], (297,), float32)]
action_space:  [Discrete(2), Discrete(2), Discrete(2), Discrete(2), Discrete(33), Discrete(33), Discrete(33), Discrete(33), Discrete(33), Discrete(33), Discrete(33), Discrete(33), Discrete(33), Discrete(33), Discrete(33)]
self.state_type= EP
start running
fixed_agent_order_reversed========================== [9, 8, 4, 10, 11, 3, 2, 6, 5, 13, 14, 1, 12, 7, 0]
Env powergym Task 123Bus Algo happo Exp test updates 1/333 episodes, total num timesteps 72/24000, FPS 3.
Average step reward is -7.179079532623291.
Some episodes done, average episode reward is -172.2979106837668.

Evaluation average episode reward is -122.62859789924657.

fixed_agent_order_reversed========================== [9, 3, 8, 4, 13, 12, 2, 1, 10, 0, 7, 11, 14, 5, 6]
Env powergym Task 123Bus Algo happo Exp test updates 2/333 episodes, total num timesteps 144/24000, FPS 1.
Average step reward is -6.800647258758545.
Some episodes done, average episode reward is -163.21553980874788.

Evaluation average episode reward is -104.32499881166376.

fixed_agent_order_reversed========================== [6, 8, 11, 2, 0, 3, 9, 4, 5, 1, 13, 10, 7, 12, 14]
Env powergym Task 123Bus Algo happo Exp test updates 3/333 episodes, total num timesteps 216/24000, FPS 1.
Average step reward is -7.454640865325928.
Some episodes done, average episode reward is -178.9113586466066.

Evaluation average episode reward is -100.30772300426649.

fixed_agent_order_reversed========================== [9, 12, 14, 7, 10, 4, 13, 5, 8, 3, 11, 2, 1, 6, 0]
Env powergym Task 123Bus Algo happo Exp test updates 4/333 episodes, total num timesteps 288/24000, FPS 1.
Average step reward is -6.80735969543457.
Some episodes done, average episode reward is -163.37664212537933.

Evaluation average episode reward is -102.22515179816855.

fixed_agent_order_reversed========================== [12, 14, 4, 10, 13, 9, 7, 5, 8, 3, 2, 1, 11, 0, 6]
Env powergym Task 123Bus Algo happo Exp test updates 5/333 episodes, total num timesteps 360/24000, FPS 1.
Average step reward is -6.926950931549072.
Some episodes done, average episode reward is -166.2468332120624.

Evaluation average episode reward is -98.36066251076757.

fixed_agent_order_reversed========================== [2, 8, 4, 9, 3, 1, 10, 0, 13, 12, 11, 7, 14, 5, 6]
Env powergym Task 123Bus Algo happo Exp test updates 6/333 episodes, total num timesteps 432/24000, FPS 1.
Average step reward is -6.948901653289795.
Some episodes done, average episode reward is -166.77364192715189.

Evaluation average episode reward is -121.42840184107546.

fixed_agent_order_reversed========================== [0, 7, 10, 1, 4, 8, 3, 13, 5, 2, 14, 11, 9, 6, 12]
Env powergym Task 123Bus Algo happo Exp test updates 7/333 episodes, total num timesteps 504/24000, FPS 1.
Average step reward is -6.145941734313965.
Some episodes done, average episode reward is -147.50260963392944.

Evaluation average episode reward is -130.20551069233395.

fixed_agent_order_reversed========================== [8, 9, 4, 10, 11, 12, 3, 14, 6, 2, 13, 5, 1, 7, 0]
Env powergym Task 123Bus Algo happo Exp test updates 8/333 episodes, total num timesteps 576/24000, FPS 1.
Average step reward is -6.903776168823242.
Some episodes done, average episode reward is -165.6906310140011.

Evaluation average episode reward is -141.46785438495485.

fixed_agent_order_reversed========================== [12, 14, 4, 3, 9, 10, 8, 13, 2, 5, 11, 1, 7, 6, 0]
Env powergym Task 123Bus Algo happo Exp test updates 9/333 episodes, total num timesteps 648/24000, FPS 1.
Average step reward is -6.808921813964844.
Some episodes done, average episode reward is -163.414131414984.

Evaluation average episode reward is -151.3158678688337.

fixed_agent_order_reversed========================== [10, 6, 11, 9, 8, 5, 4, 2, 3, 7, 13, 12, 1, 14, 0]
Env powergym Task 123Bus Algo happo Exp test updates 10/333 episodes, total num timesteps 720/24000, FPS 1.
Average step reward is -7.008293151855469.
Some episodes done, average episode reward is -168.19903738170706.

Evaluation average episode reward is -109.25359450606254.

fixed_agent_order_reversed========================== [6, 11, 0, 13, 10, 14, 1, 7, 2, 12, 4, 9, 5, 3, 8]
Env powergym Task 123Bus Algo happo Exp test updates 11/333 episodes, total num timesteps 792/24000, FPS 1.
Average step reward is -7.107099533081055.
Some episodes done, average episode reward is -170.57037948122453.

Evaluation average episode reward is -97.09807622971567.

fixed_agent_order_reversed========================== [4, 3, 1, 8, 11, 9, 2, 5, 6, 10, 7, 13, 12, 0, 14]
Env powergym Task 123Bus Algo happo Exp test updates 12/333 episodes, total num timesteps 864/24000, FPS 1.
Average step reward is -6.3972296714782715.
Some episodes done, average episode reward is -153.53350374246352.

Evaluation average episode reward is -119.00908032167466.

fixed_agent_order_reversed========================== [10, 14, 12, 5, 7, 4, 2, 13, 9, 3, 11, 1, 0, 6, 8]
Env powergym Task 123Bus Algo happo Exp test updates 13/333 episodes, total num timesteps 936/24000, FPS 1.
Average step reward is -7.027288913726807.
Some episodes done, average episode reward is -168.65493492880228.

Evaluation average episode reward is -118.96890455849878.

fixed_agent_order_reversed========================== [14, 4, 9, 10, 12, 8, 11, 5, 2, 6, 13, 3, 7, 1, 0]
Env powergym Task 123Bus Algo happo Exp test updates 14/333 episodes, total num timesteps 1008/24000, FPS 1.
Average step reward is -6.619028091430664.
Some episodes done, average episode reward is -158.856668540717.

Evaluation average episode reward is -93.90079192703699.

fixed_agent_order_reversed========================== [6, 3, 11, 2, 4, 9, 0, 1, 13, 5, 8, 10, 12, 14, 7]
Env powergym Task 123Bus Algo happo Exp test updates 15/333 episodes, total num timesteps 1080/24000, FPS 1.
Average step reward is -6.460425853729248.
Some episodes done, average episode reward is -155.0502227249859.

Evaluation average episode reward is -88.81307051965182.

fixed_agent_order_reversed========================== [0, 2, 1, 14, 13, 7, 9, 12, 3, 4, 10, 5, 11, 6, 8]
Env powergym Task 123Bus Algo happo Exp test updates 16/333 episodes, total num timesteps 1152/24000, FPS 1.
Average step reward is -6.223007678985596.
Some episodes done, average episode reward is -149.35218417774033.

Evaluation average episode reward is -117.58609514516549.

fixed_agent_order_reversed========================== [7, 5, 8, 6, 14, 0, 11, 2, 4, 10, 13, 1, 3, 12, 9]
Env powergym Task 123Bus Algo happo Exp test updates 17/333 episodes, total num timesteps 1224/24000, FPS 1.
Average step reward is -6.75338077545166.
Some episodes done, average episode reward is -162.08113326395122.

Evaluation average episode reward is -82.84991794883507.

fixed_agent_order_reversed========================== [13, 14, 7, 4, 12, 10, 2, 1, 5, 11, 9, 3, 0, 6, 8]
Env powergym Task 123Bus Algo happo Exp test updates 18/333 episodes, total num timesteps 1296/24000, FPS 1.
Average step reward is -6.859408378601074.
Some episodes done, average episode reward is -164.62580450323708.

Evaluation average episode reward is -85.26878648852086.

fixed_agent_order_reversed========================== [7, 14, 13, 12, 4, 5, 10, 1, 2, 11, 0, 8, 6, 3, 9]
Env powergym Task 123Bus Algo happo Exp test updates 19/333 episodes, total num timesteps 1368/24000, FPS 1.
Average step reward is -6.856912612915039.
Some episodes done, average episode reward is -164.56589008556205.

Evaluation average episode reward is -57.59261487616876.

fixed_agent_order_reversed========================== [14, 12, 7, 5, 13, 4, 10, 3, 8, 11, 6, 9, 1, 2, 0]
Env powergym Task 123Bus Algo happo Exp test updates 20/333 episodes, total num timesteps 1440/24000, FPS 1.
Average step reward is -6.482658386230469.
Some episodes done, average episode reward is -155.58381108174527.

Evaluation average episode reward is -42.59970334234098.

fixed_agent_order_reversed========================== [6, 5, 7, 11, 12, 13, 9, 4, 1, 14, 2, 10, 8, 3, 0]
Env powergym Task 123Bus Algo happo Exp test updates 21/333 episodes, total num timesteps 1512/24000, FPS 1.
Average step reward is -5.64048957824707.
Some episodes done, average episode reward is -135.37174811526492.

Evaluation average episode reward is -30.427186875742503.

fixed_agent_order_reversed========================== [14, 13, 12, 9, 10, 0, 7, 3, 1, 2, 4, 5, 8, 11, 6]
Env powergym Task 123Bus Algo happo Exp test updates 22/333 episodes, total num timesteps 1584/24000, FPS 1.
Average step reward is -5.651937484741211.
Some episodes done, average episode reward is -135.64649754204473.

Evaluation average episode reward is -36.606660991562336.

fixed_agent_order_reversed========================== [8, 6, 7, 10, 11, 9, 13, 4, 5, 1, 3, 14, 12, 0, 2]
Env powergym Task 123Bus Algo happo Exp test updates 23/333 episodes, total num timesteps 1656/24000, FPS 1.
Average step reward is -5.479525089263916.
Some episodes done, average episode reward is -131.50860837414442.

Evaluation average episode reward is -34.48976098063767.

fixed_agent_order_reversed========================== [14, 12, 6, 7, 13, 5, 11, 9, 10, 4, 8, 3, 1, 2, 0]
Env powergym Task 123Bus Algo happo Exp test updates 24/333 episodes, total num timesteps 1728/24000, FPS 1.
Average step reward is -5.229281425476074.
Some episodes done, average episode reward is -125.5027549933083.

Evaluation average episode reward is -64.02031492390864.

fixed_agent_order_reversed========================== [9, 7, 13, 8, 12, 14, 1, 6, 11, 10, 4, 0, 5, 3, 2]
Env powergym Task 123Bus Algo happo Exp test updates 25/333 episodes, total num timesteps 1800/24000, FPS 1.
Average step reward is -5.984212875366211.
Some episodes done, average episode reward is -143.6211153385262.

Evaluation average episode reward is -75.57234105656175.

fixed_agent_order_reversed========================== [2, 5, 9, 4, 10, 3, 11, 0, 1, 7, 6, 8, 13, 14, 12]
Env powergym Task 123Bus Algo happo Exp test updates 26/333 episodes, total num timesteps 1872/24000, FPS 1.
Average step reward is -5.22264289855957.
Some episodes done, average episode reward is -125.34343325595648.

Evaluation average episode reward is -44.74290488257993.

fixed_agent_order_reversed========================== [9, 8, 4, 2, 3, 10, 12, 11, 13, 1, 7, 0, 5, 6, 14]
Env powergym Task 123Bus Algo happo Exp test updates 27/333 episodes, total num timesteps 1944/24000, FPS 1.
Average step reward is -5.353444576263428.
Some episodes done, average episode reward is -128.48266537035695.

Evaluation average episode reward is -59.2396662588356.

fixed_agent_order_reversed========================== [9, 7, 0, 10, 6, 1, 3, 11, 13, 4, 2, 8, 5, 12, 14]
Env powergym Task 123Bus Algo happo Exp test updates 28/333 episodes, total num timesteps 2016/24000, FPS 1.
Average step reward is -5.2485880851745605.
Some episodes done, average episode reward is -125.96611517474322.

Evaluation average episode reward is -40.00619165165988.

fixed_agent_order_reversed========================== [6, 8, 11, 3, 2, 4, 5, 0, 1, 10, 13, 14, 9, 12, 7]
Env powergym Task 123Bus Algo happo Exp test updates 29/333 episodes, total num timesteps 2088/24000, FPS 1.
Average step reward is -5.535696029663086.
Some episodes done, average episode reward is -132.85669590315558.

Evaluation average episode reward is -101.46068713101307.

fixed_agent_order_reversed========================== [0, 3, 14, 2, 9, 13, 10, 4, 5, 6, 11, 1, 12, 7, 8]
Env powergym Task 123Bus Algo happo Exp test updates 30/333 episodes, total num timesteps 2160/24000, FPS 0.
Average step reward is -5.852171897888184.
Some episodes done, average episode reward is -140.45213858687205.

Evaluation average episode reward is -90.50959519434237.

fixed_agent_order_reversed========================== [7, 0, 1, 9, 6, 5, 2, 13, 11, 3, 10, 4, 8, 12, 14]
Env powergym Task 123Bus Algo happo Exp test updates 31/333 episodes, total num timesteps 2232/24000, FPS 0.
Average step reward is -5.248897075653076.
Some episodes done, average episode reward is -125.97353448541503.

Evaluation average episode reward is -72.46102493646349.

fixed_agent_order_reversed========================== [9, 14, 4, 10, 11, 6, 3, 12, 8, 13, 2, 5, 1, 7, 0]
Env powergym Task 123Bus Algo happo Exp test updates 32/333 episodes, total num timesteps 2304/24000, FPS 0.
Average step reward is -4.866607189178467.
Some episodes done, average episode reward is -116.79857232087107.

Evaluation average episode reward is -67.63361623683267.

fixed_agent_order_reversed========================== [6, 5, 7, 11, 0, 8, 10, 1, 13, 2, 3, 4, 14, 12, 9]
Env powergym Task 123Bus Algo happo Exp test updates 33/333 episodes, total num timesteps 2376/24000, FPS 0.
Average step reward is -5.434576034545898.
Some episodes done, average episode reward is -130.42982708372458.

Evaluation average episode reward is -66.525728216642.

fixed_agent_order_reversed========================== [8, 10, 9, 6, 11, 4, 14, 12, 2, 5, 7, 13, 3, 1, 0]
Env powergym Task 123Bus Algo happo Exp test updates 34/333 episodes, total num timesteps 2448/24000, FPS 0.
Average step reward is -5.425975799560547.
Some episodes done, average episode reward is -130.2234180248384.

Evaluation average episode reward is -92.0419301071006.

fixed_agent_order_reversed========================== [6, 10, 11, 5, 4, 7, 9, 8, 13, 12, 3, 14, 1, 2, 0]
Env powergym Task 123Bus Algo happo Exp test updates 35/333 episodes, total num timesteps 2520/24000, FPS 0.
Average step reward is -5.304162979125977.
Some episodes done, average episode reward is -127.2999194605898.

Evaluation average episode reward is -82.26599751960451.

fixed_agent_order_reversed========================== [12, 14, 5, 7, 8, 9, 6, 4, 11, 13, 3, 10, 2, 1, 0]
Env powergym Task 123Bus Algo happo Exp test updates 36/333 episodes, total num timesteps 2592/24000, FPS 0.
Average step reward is -5.43247652053833.
Some episodes done, average episode reward is -130.37944100268737.

Evaluation average episode reward is -79.62095039540942.

fixed_agent_order_reversed========================== [14, 12, 13, 7, 4, 8, 10, 9, 5, 11, 1, 3, 2, 6, 0]
Env powergym Task 123Bus Algo happo Exp test updates 37/333 episodes, total num timesteps 2664/24000, FPS 0.
Average step reward is -4.615238189697266.
Some episodes done, average episode reward is -110.76571977856203.

Evaluation average episode reward is -67.34118657533735.

fixed_agent_order_reversed========================== [6, 7, 8, 5, 11, 12, 4, 9, 2, 1, 13, 10, 14, 3, 0]
Env powergym Task 123Bus Algo happo Exp test updates 38/333 episodes, total num timesteps 2736/24000, FPS 0.
Average step reward is -5.3995866775512695.
Some episodes done, average episode reward is -129.59008254088405.

Evaluation average episode reward is -52.85808406922951.

fixed_agent_order_reversed========================== [0, 3, 2, 4, 1, 10, 9, 7, 13, 8, 5, 11, 12, 6, 14]
Env powergym Task 123Bus Algo happo Exp test updates 39/333 episodes, total num timesteps 2808/24000, FPS 0.
Average step reward is -5.212250709533691.
Some episodes done, average episode reward is -125.09401047834034.

Evaluation average episode reward is -62.53993471462348.

fixed_agent_order_reversed========================== [2, 6, 3, 11, 8, 4, 0, 13, 1, 10, 5, 14, 12, 7, 9]
Env powergym Task 123Bus Algo happo Exp test updates 40/333 episodes, total num timesteps 2880/24000, FPS 0.
Average step reward is -5.005419731140137.
Some episodes done, average episode reward is -120.13007374340395.

Evaluation average episode reward is -52.00048589211508.

fixed_agent_order_reversed========================== [7, 5, 12, 14, 9, 13, 6, 1, 11, 4, 10, 0, 3, 8, 2]
Env powergym Task 123Bus Algo happo Exp test updates 41/333 episodes, total num timesteps 2952/24000, FPS 0.
Average step reward is -5.113348007202148.
Some episodes done, average episode reward is -122.72035973046957.

Evaluation average episode reward is -60.297998703894045.

fixed_agent_order_reversed========================== [8, 1, 13, 4, 10, 7, 11, 3, 2, 9, 6, 0, 12, 5, 14]
Env powergym Task 123Bus Algo happo Exp test updates 42/333 episodes, total num timesteps 3024/24000, FPS 0.
Average step reward is -4.868555545806885.
Some episodes done, average episode reward is -116.8453385017114.

Evaluation average episode reward is -61.74645839652732.

fixed_agent_order_reversed========================== [12, 14, 7, 13, 10, 4, 5, 9, 8, 3, 11, 6, 1, 2, 0]
Env powergym Task 123Bus Algo happo Exp test updates 43/333 episodes, total num timesteps 3096/24000, FPS 0.
Average step reward is -4.759609699249268.
Some episodes done, average episode reward is -114.23062521168704.

Evaluation average episode reward is -67.55796908261.

fixed_agent_order_reversed========================== [0, 2, 12, 13, 3, 14, 1, 7, 4, 9, 10, 5, 11, 6, 8]
Env powergym Task 123Bus Algo happo Exp test updates 44/333 episodes, total num timesteps 3168/24000, FPS 0.
Average step reward is -4.262969970703125.
Some episodes done, average episode reward is -102.31127839935539.

Evaluation average episode reward is -65.2043509724795.

fixed_agent_order_reversed========================== [8, 6, 5, 11, 14, 4, 12, 7, 9, 13, 10, 3, 2, 1, 0]
Env powergym Task 123Bus Algo happo Exp test updates 45/333 episodes, total num timesteps 3240/24000, FPS 0.
Average step reward is -4.800375461578369.
Some episodes done, average episode reward is -115.20901760771625.

Evaluation average episode reward is -65.54621321511657.

fixed_agent_order_reversed========================== [0, 9, 7, 13, 1, 5, 2, 4, 12, 3, 10, 11, 8, 6, 14]
Env powergym Task 123Bus Algo happo Exp test updates 46/333 episodes, total num timesteps 3312/24000, FPS 0.
Average step reward is -4.34258508682251.
Some episodes done, average episode reward is -104.22203654032542.

Evaluation average episode reward is -59.54901025130692.

fixed_agent_order_reversed========================== [0, 6, 2, 5, 11, 7, 1, 3, 8, 4, 13, 10, 9, 14, 12]
Env powergym Task 123Bus Algo happo Exp test updates 47/333 episodes, total num timesteps 3384/24000, FPS 0.
Average step reward is -4.065032005310059.
Some episodes done, average episode reward is -97.56077054537788.

Evaluation average episode reward is -56.342286182669575.

fixed_agent_order_reversed========================== [7, 4, 5, 10, 1, 11, 3, 6, 8, 12, 2, 13, 9, 0, 14]
Env powergym Task 123Bus Algo happo Exp test updates 48/333 episodes, total num timesteps 3456/24000, FPS 0.
Average step reward is -4.898079872131348.
Some episodes done, average episode reward is -117.55391693865174.

Evaluation average episode reward is -55.67806545712077.

fixed_agent_order_reversed========================== [14, 12, 13, 4, 9, 7, 10, 5, 11, 1, 3, 8, 2, 6, 0]
Env powergym Task 123Bus Algo happo Exp test updates 49/333 episodes, total num timesteps 3528/24000, FPS 0.
Average step reward is -4.414300918579102.
Some episodes done, average episode reward is -105.94322140261694.

Evaluation average episode reward is -53.13931604250261.

fixed_agent_order_reversed========================== [9, 8, 10, 6, 14, 11, 4, 13, 12, 3, 1, 7, 2, 0, 5]
Env powergym Task 123Bus Algo happo Exp test updates 50/333 episodes, total num timesteps 3600/24000, FPS 0.
Average step reward is -4.708137035369873.
Some episodes done, average episode reward is -112.99529143205645.

Evaluation average episode reward is -55.47828004502364.

fixed_agent_order_reversed========================== [9, 14, 7, 13, 12, 4, 1, 10, 2, 11, 5, 8, 6, 0, 3]
Env powergym Task 123Bus Algo happo Exp test updates 51/333 episodes, total num timesteps 3672/24000, FPS 0.
Average step reward is -4.254465103149414.
Some episodes done, average episode reward is -102.10715912171912.

Evaluation average episode reward is -56.06343308541046.

fixed_agent_order_reversed========================== [8, 3, 4, 10, 11, 2, 6, 9, 1, 5, 0, 7, 13, 12, 14]
Env powergym Task 123Bus Algo happo Exp test updates 52/333 episodes, total num timesteps 3744/24000, FPS 0.
Average step reward is -4.643094062805176.
Some episodes done, average episode reward is -111.4342586378033.

Evaluation average episode reward is -47.2022624025334.

fixed_agent_order_reversed========================== [7, 6, 13, 12, 14, 9, 11, 5, 1, 10, 4, 8, 3, 2, 0]
Env powergym Task 123Bus Algo happo Exp test updates 53/333 episodes, total num timesteps 3816/24000, FPS 0.
Average step reward is -4.258977890014648.
Some episodes done, average episode reward is -102.2154566807704.

Evaluation average episode reward is -54.50960010964484.

fixed_agent_order_reversed========================== [7, 1, 4, 11, 2, 6, 0, 9, 12, 3, 13, 10, 5, 8, 14]
Env powergym Task 123Bus Algo happo Exp test updates 54/333 episodes, total num timesteps 3888/24000, FPS 0.
Average step reward is -4.008182048797607.
Some episodes done, average episode reward is -96.19636464777996.

Evaluation average episode reward is -51.00087658350029.

fixed_agent_order_reversed========================== [9, 8, 4, 14, 10, 11, 13, 12, 3, 6, 5, 2, 1, 7, 0]
Env powergym Task 123Bus Algo happo Exp test updates 55/333 episodes, total num timesteps 3960/24000, FPS 0.
Average step reward is -4.918923854827881.
Some episodes done, average episode reward is -118.05416819707331.

Evaluation average episode reward is -48.52184179318271.

fixed_agent_order_reversed========================== [6, 11, 2, 8, 3, 4, 10, 1, 9, 0, 5, 13, 7, 14, 12]
Env powergym Task 123Bus Algo happo Exp test updates 56/333 episodes, total num timesteps 4032/24000, FPS 0.
Average step reward is -4.687588214874268.
Some episodes done, average episode reward is -112.50211822371291.

Evaluation average episode reward is -49.60966159163951.

fixed_agent_order_reversed========================== [6, 11, 5, 8, 3, 4, 1, 7, 0, 10, 2, 12, 9, 13, 14]
Env powergym Task 123Bus Algo happo Exp test updates 57/333 episodes, total num timesteps 4104/24000, FPS 0.
Average step reward is -4.342209815979004.
Some episodes done, average episode reward is -104.21304185045157.

Evaluation average episode reward is -67.08792344772075.

fixed_agent_order_reversed========================== [2, 4, 12, 8, 3, 14, 11, 10, 5, 1, 6, 13, 0, 9, 7]
Env powergym Task 123Bus Algo happo Exp test updates 58/333 episodes, total num timesteps 4176/24000, FPS 0.
Average step reward is -3.991370677947998.
Some episodes done, average episode reward is -95.79288927788099.

Evaluation average episode reward is -63.10427857379199.

fixed_agent_order_reversed========================== [10, 13, 14, 9, 12, 3, 4, 11, 6, 2, 7, 5, 1, 0, 8]
Env powergym Task 123Bus Algo happo Exp test updates 59/333 episodes, total num timesteps 4248/24000, FPS 0.
Average step reward is -4.387638568878174.
Some episodes done, average episode reward is -105.30332627328983.

Evaluation average episode reward is -61.25632496202304.

fixed_agent_order_reversed========================== [7, 12, 5, 14, 1, 13, 6, 4, 11, 8, 10, 0, 2, 3, 9]
Env powergym Task 123Bus Algo happo Exp test updates 60/333 episodes, total num timesteps 4320/24000, FPS 0.
Average step reward is -4.816049575805664.
Some episodes done, average episode reward is -115.58518763809859.

Evaluation average episode reward is -56.03109009188097.

fixed_agent_order_reversed========================== [9, 13, 6, 8, 11, 3, 2, 4, 1, 0, 10, 7, 5, 14, 12]
Env powergym Task 123Bus Algo happo Exp test updates 61/333 episodes, total num timesteps 4392/24000, FPS 0.
Average step reward is -4.494789123535156.
Some episodes done, average episode reward is -107.87494396996811.

Evaluation average episode reward is -58.462709910013224.

fixed_agent_order_reversed========================== [12, 4, 14, 10, 8, 3, 5, 11, 2, 1, 9, 7, 6, 13, 0]
Env powergym Task 123Bus Algo happo Exp test updates 62/333 episodes, total num timesteps 4464/24000, FPS 0.
Average step reward is -4.4507927894592285.
Some episodes done, average episode reward is -106.81902807990332.

Evaluation average episode reward is -59.146203446527714.

fixed_agent_order_reversed========================== [9, 10, 6, 11, 13, 3, 4, 8, 2, 5, 1, 7, 0, 14, 12]
Env powergym Task 123Bus Algo happo Exp test updates 63/333 episodes, total num timesteps 4536/24000, FPS 0.
Average step reward is -4.345855712890625.
Some episodes done, average episode reward is -104.3005352681795.

Evaluation average episode reward is -63.81092226543607.

fixed_agent_order_reversed========================== [6, 0, 11, 2, 3, 1, 5, 13, 8, 4, 14, 12, 7, 10, 9]
Env powergym Task 123Bus Algo happo Exp test updates 64/333 episodes, total num timesteps 4608/24000, FPS 0.
Average step reward is -4.636390686035156.
Some episodes done, average episode reward is -111.27337921823357.

Evaluation average episode reward is -51.29212174710326.

fixed_agent_order_reversed========================== [8, 6, 11, 10, 4, 3, 2, 5, 9, 13, 14, 1, 0, 12, 7]
Env powergym Task 123Bus Algo happo Exp test updates 65/333 episodes, total num timesteps 4680/24000, FPS 0.
Average step reward is -4.217161655426025.
Some episodes done, average episode reward is -101.21188170419896.

Evaluation average episode reward is -39.64548609529489.

fixed_agent_order_reversed========================== [7, 6, 0, 5, 1, 11, 13, 10, 12, 2, 14, 8, 3, 4, 9]
Env powergym Task 123Bus Algo happo Exp test updates 66/333 episodes, total num timesteps 4752/24000, FPS 0.
Average step reward is -4.29847526550293.
Some episodes done, average episode reward is -103.16341658180433.

Evaluation average episode reward is -39.33322320291051.

fixed_agent_order_reversed========================== [2, 0, 3, 4, 5, 10, 1, 7, 12, 11, 13, 14, 9, 6, 8]
Env powergym Task 123Bus Algo happo Exp test updates 67/333 episodes, total num timesteps 4824/24000, FPS 0.
Average step reward is -3.74723219871521.
Some episodes done, average episode reward is -89.93357872918813.

Evaluation average episode reward is -36.80727919853007.

fixed_agent_order_reversed========================== [8, 2, 10, 4, 9, 3, 11, 13, 1, 5, 6, 0, 12, 7, 14]
Env powergym Task 123Bus Algo happo Exp test updates 68/333 episodes, total num timesteps 4896/24000, FPS 0.
Average step reward is -4.475901126861572.
Some episodes done, average episode reward is -107.42163278069081.

Evaluation average episode reward is -33.560599184123475.

fixed_agent_order_reversed========================== [8, 7, 6, 12, 1, 13, 14, 5, 11, 0, 10, 3, 4, 9, 2]
Env powergym Task 123Bus Algo happo Exp test updates 69/333 episodes, total num timesteps 4968/24000, FPS 0.
Average step reward is -4.079876899719238.
Some episodes done, average episode reward is -97.91704434673801.

Evaluation average episode reward is -37.043308165702534.

fixed_agent_order_reversed========================== [6, 7, 11, 5, 8, 1, 0, 3, 4, 12, 13, 9, 2, 10, 14]
Env powergym Task 123Bus Algo happo Exp test updates 70/333 episodes, total num timesteps 5040/24000, FPS 0.
Average step reward is -4.158487319946289.
Some episodes done, average episode reward is -99.80368346610192.

Evaluation average episode reward is -37.304182611656664.

fixed_agent_order_reversed========================== [5, 12, 4, 9, 14, 3, 13, 11, 10, 2, 8, 6, 1, 7, 0]
Env powergym Task 123Bus Algo happo Exp test updates 71/333 episodes, total num timesteps 5112/24000, FPS 0.
Average step reward is -4.102211952209473.
Some episodes done, average episode reward is -98.45307756586836.

Evaluation average episode reward is -37.69183933729012.

fixed_agent_order_reversed========================== [7, 12, 14, 4, 13, 10, 8, 9, 1, 5, 11, 3, 6, 2, 0]
Env powergym Task 123Bus Algo happo Exp test updates 72/333 episodes, total num timesteps 5184/24000, FPS 0.
Average step reward is -3.746936321258545.
Some episodes done, average episode reward is -89.92647105227712.

Evaluation average episode reward is -32.64522006999048.

fixed_agent_order_reversed========================== [9, 13, 7, 10, 14, 0, 2, 3, 4, 11, 6, 1, 5, 12, 8]
Env powergym Task 123Bus Algo happo Exp test updates 73/333 episodes, total num timesteps 5256/24000, FPS 0.
Average step reward is -3.84375262260437.
Some episodes done, average episode reward is -92.25005733474228.

Evaluation average episode reward is -33.19283760763189.

fixed_agent_order_reversed========================== [7, 10, 9, 5, 13, 6, 11, 4, 12, 14, 1, 2, 3, 8, 0]
Env powergym Task 123Bus Algo happo Exp test updates 74/333 episodes, total num timesteps 5328/24000, FPS 0.
Average step reward is -3.7181472778320312.
Some episodes done, average episode reward is -89.23554387771655.

Evaluation average episode reward is -32.036219103528595.

fixed_agent_order_reversed========================== [0, 6, 2, 3, 11, 1, 5, 8, 7, 10, 4, 13, 9, 12, 14]
Env powergym Task 123Bus Algo happo Exp test updates 75/333 episodes, total num timesteps 5400/24000, FPS 0.
Average step reward is -3.8924403190612793.
Some episodes done, average episode reward is -93.41856589450863.

Evaluation average episode reward is -30.01330455403853.

fixed_agent_order_reversed========================== [9, 13, 12, 10, 4, 14, 3, 1, 8, 0, 7, 2, 11, 5, 6]
Env powergym Task 123Bus Algo happo Exp test updates 76/333 episodes, total num timesteps 5472/24000, FPS 0.
Average step reward is -4.090871810913086.
Some episodes done, average episode reward is -98.18093817591209.

Evaluation average episode reward is -29.601759400329797.

fixed_agent_order_reversed========================== [2, 0, 6, 11, 1, 5, 3, 7, 13, 4, 9, 8, 10, 12, 14]
Env powergym Task 123Bus Algo happo Exp test updates 77/333 episodes, total num timesteps 5544/24000, FPS 0.
Average step reward is -3.87982177734375.
Some episodes done, average episode reward is -93.11572486645123.

Evaluation average episode reward is -28.6740375406437.

fixed_agent_order_reversed========================== [0, 3, 2, 1, 10, 8, 9, 4, 7, 13, 5, 11, 6, 12, 14]
Env powergym Task 123Bus Algo happo Exp test updates 78/333 episodes, total num timesteps 5616/24000, FPS 0.
Average step reward is -3.872549295425415.
Some episodes done, average episode reward is -92.94118658201148.

Evaluation average episode reward is -25.177052476127944.

fixed_agent_order_reversed========================== [7, 14, 12, 13, 0, 1, 9, 5, 2, 3, 10, 4, 8, 11, 6]
Env powergym Task 123Bus Algo happo Exp test updates 79/333 episodes, total num timesteps 5688/24000, FPS 0.
Average step reward is -3.4664525985717773.
Some episodes done, average episode reward is -83.19486040578445.

Evaluation average episode reward is -24.72940440274552.

fixed_agent_order_reversed========================== [6, 11, 3, 5, 8, 1, 2, 0, 4, 14, 13, 12, 10, 7, 9]
Env powergym Task 123Bus Algo happo Exp test updates 80/333 episodes, total num timesteps 5760/24000, FPS 0.
Average step reward is -3.797703742980957.
Some episodes done, average episode reward is -91.14488437357066.

Evaluation average episode reward is -26.243218163254.

fixed_agent_order_reversed========================== [0, 1, 13, 7, 3, 2, 14, 4, 11, 9, 5, 10, 6, 12, 8]
Env powergym Task 123Bus Algo happo Exp test updates 81/333 episodes, total num timesteps 5832/24000, FPS 0.
Average step reward is -3.577165365219116.
Some episodes done, average episode reward is -85.8519733892493.

Evaluation average episode reward is -27.512048058752345.

fixed_agent_order_reversed========================== [14, 12, 7, 13, 10, 5, 1, 4, 0, 2, 3, 11, 9, 6, 8]
Env powergym Task 123Bus Algo happo Exp test updates 82/333 episodes, total num timesteps 5904/24000, FPS 0.
Average step reward is -3.2217745780944824.
Some episodes done, average episode reward is -77.3225911289074.

Evaluation average episode reward is -25.947047748186133.

fixed_agent_order_reversed========================== [6, 9, 11, 5, 7, 8, 13, 10, 1, 0, 4, 3, 2, 12, 14]
Env powergym Task 123Bus Algo happo Exp test updates 83/333 episodes, total num timesteps 5976/24000, FPS 0.
Average step reward is -3.5672171115875244.
Some episodes done, average episode reward is -85.61321229306263.

Evaluation average episode reward is -27.029928939770926.

fixed_agent_order_reversed========================== [6, 9, 8, 11, 3, 10, 0, 2, 4, 5, 1, 13, 7, 14, 12]
Env powergym Task 123Bus Algo happo Exp test updates 84/333 episodes, total num timesteps 6048/24000, FPS 0.
Average step reward is -3.3694214820861816.
Some episodes done, average episode reward is -80.86611753960075.

Evaluation average episode reward is -27.38726801441776.

fixed_agent_order_reversed========================== [6, 5, 11, 7, 1, 0, 10, 14, 13, 8, 2, 3, 4, 9, 12]
Env powergym Task 123Bus Algo happo Exp test updates 85/333 episodes, total num timesteps 6120/24000, FPS 0.
Average step reward is -3.196812629699707.
Some episodes done, average episode reward is -76.72350620509418.

Evaluation average episode reward is -26.878761130407913.

fixed_agent_order_reversed========================== [0, 2, 3, 6, 11, 1, 4, 8, 9, 5, 13, 10, 12, 7, 14]
Env powergym Task 123Bus Algo happo Exp test updates 86/333 episodes, total num timesteps 6192/24000, FPS 0.
Average step reward is -3.1820759773254395.
Some episodes done, average episode reward is -76.3698193650722.

Evaluation average episode reward is -26.41751640565917.

fixed_agent_order_reversed========================== [9, 10, 4, 12, 3, 13, 7, 14, 2, 11, 5, 1, 8, 6, 0]
Env powergym Task 123Bus Algo happo Exp test updates 87/333 episodes, total num timesteps 6264/24000, FPS 0.
Average step reward is -2.8006160259246826.
Some episodes done, average episode reward is -67.21478748693961.

Evaluation average episode reward is -26.21182126087542.

fixed_agent_order_reversed========================== [9, 10, 7, 4, 3, 13, 11, 2, 5, 1, 12, 0, 6, 14, 8]
Env powergym Task 123Bus Algo happo Exp test updates 88/333 episodes, total num timesteps 6336/24000, FPS 0.
Average step reward is -3.4540443420410156.
Some episodes done, average episode reward is -82.89706398713393.

Evaluation average episode reward is -25.615781487575163.

fixed_agent_order_reversed========================== [12, 14, 7, 13, 10, 5, 4, 9, 8, 11, 1, 3, 2, 6, 0]
Env powergym Task 123Bus Algo happo Exp test updates 89/333 episodes, total num timesteps 6408/24000, FPS 0.
Average step reward is -3.259580612182617.
Some episodes done, average episode reward is -78.22993437253342.

Evaluation average episode reward is -26.21103525243349.

fixed_agent_order_reversed========================== [6, 11, 9, 8, 0, 5, 1, 7, 3, 10, 2, 4, 13, 14, 12]
Env powergym Task 123Bus Algo happo Exp test updates 90/333 episodes, total num timesteps 6480/24000, FPS 0.
Average step reward is -3.139274835586548.
Some episodes done, average episode reward is -75.34259808130136.

Evaluation average episode reward is -29.610183753323096.

fixed_agent_order_reversed========================== [10, 4, 9, 11, 6, 5, 3, 14, 8, 13, 2, 12, 7, 1, 0]
Env powergym Task 123Bus Algo happo Exp test updates 91/333 episodes, total num timesteps 6552/24000, FPS 0.
Average step reward is -2.881986379623413.
Some episodes done, average episode reward is -69.16767266958747.

Evaluation average episode reward is -27.58332854135354.

fixed_agent_order_reversed========================== [4, 9, 8, 3, 2, 11, 10, 6, 14, 12, 13, 1, 5, 0, 7]
Env powergym Task 123Bus Algo happo Exp test updates 92/333 episodes, total num timesteps 6624/24000, FPS 0.
Average step reward is -2.803622245788574.
Some episodes done, average episode reward is -67.28693570476908.

Evaluation average episode reward is -28.68381799344374.

fixed_agent_order_reversed========================== [9, 8, 4, 3, 10, 2, 11, 6, 13, 1, 5, 14, 12, 0, 7]
Env powergym Task 123Bus Algo happo Exp test updates 93/333 episodes, total num timesteps 6696/24000, FPS 0.
Average step reward is -2.770918607711792.
Some episodes done, average episode reward is -66.50204740228108.

Evaluation average episode reward is -29.134360203532694.

fixed_agent_order_reversed========================== [12, 9, 14, 4, 2, 13, 7, 10, 3, 5, 8, 1, 0, 11, 6]
Env powergym Task 123Bus Algo happo Exp test updates 94/333 episodes, total num timesteps 6768/24000, FPS 0.
Average step reward is -2.816009759902954.
Some episodes done, average episode reward is -67.58423602962897.

Evaluation average episode reward is -27.94305840533991.

fixed_agent_order_reversed========================== [7, 6, 13, 11, 1, 14, 5, 12, 0, 8, 9, 4, 10, 2, 3]
Env powergym Task 123Bus Algo happo Exp test updates 95/333 episodes, total num timesteps 6840/24000, FPS 0.
Average step reward is -3.079148292541504.
Some episodes done, average episode reward is -73.89955422306532.

Evaluation average episode reward is -26.71071095160731.

fixed_agent_order_reversed========================== [9, 0, 3, 10, 2, 4, 13, 1, 8, 7, 11, 12, 5, 6, 14]
Env powergym Task 123Bus Algo happo Exp test updates 96/333 episodes, total num timesteps 6912/24000, FPS 0.
Average step reward is -2.6572585105895996.
Some episodes done, average episode reward is -63.774207392802715.

Evaluation average episode reward is -25.32482383417781.

fixed_agent_order_reversed========================== [3, 10, 4, 2, 11, 6, 5, 8, 9, 12, 1, 13, 0, 14, 7]
Env powergym Task 123Bus Algo happo Exp test updates 97/333 episodes, total num timesteps 6984/24000, FPS 0.
Average step reward is -2.7134761810302734.
Some episodes done, average episode reward is -65.12342896347258.

Evaluation average episode reward is -28.468742877125155.

fixed_agent_order_reversed========================== [6, 9, 11, 8, 0, 7, 1, 10, 3, 5, 2, 4, 13, 14, 12]
Env powergym Task 123Bus Algo happo Exp test updates 98/333 episodes, total num timesteps 7056/24000, FPS 0.
Average step reward is -2.8178749084472656.
Some episodes done, average episode reward is -67.62899906552919.

Evaluation average episode reward is -28.650480157814922.

fixed_agent_order_reversed========================== [5, 6, 10, 11, 7, 4, 9, 12, 3, 2, 8, 1, 13, 0, 14]
Env powergym Task 123Bus Algo happo Exp test updates 99/333 episodes, total num timesteps 7128/24000, FPS 0.
Average step reward is -2.758622407913208.
Some episodes done, average episode reward is -66.20693631323543.

Evaluation average episode reward is -28.199548321246553.

fixed_agent_order_reversed========================== [14, 9, 10, 13, 4, 8, 12, 11, 3, 5, 7, 6, 1, 2, 0]
Env powergym Task 123Bus Algo happo Exp test updates 100/333 episodes, total num timesteps 7200/24000, FPS 0.
Average step reward is -3.0846705436706543.
Some episodes done, average episode reward is -74.03209519500702.

Evaluation average episode reward is -27.98906034463484.

fixed_agent_order_reversed========================== [9, 10, 6, 8, 11, 5, 4, 7, 14, 13, 12, 3, 2, 1, 0]
Env powergym Task 123Bus Algo happo Exp test updates 101/333 episodes, total num timesteps 7272/24000, FPS 0.
Average step reward is -2.7280023097991943.
Some episodes done, average episode reward is -65.47205930346342.

Evaluation average episode reward is -25.790040886583995.

fixed_agent_order_reversed========================== [0, 2, 3, 12, 1, 4, 10, 9, 13, 7, 5, 8, 11, 6, 14]
Env powergym Task 123Bus Algo happo Exp test updates 102/333 episodes, total num timesteps 7344/24000, FPS 0.
Average step reward is -2.728025197982788.
Some episodes done, average episode reward is -65.47261092318728.

Evaluation average episode reward is -25.53028296494345.

fixed_agent_order_reversed========================== [14, 6, 8, 11, 5, 9, 10, 12, 4, 7, 13, 1, 3, 2, 0]
Env powergym Task 123Bus Algo happo Exp test updates 103/333 episodes, total num timesteps 7416/24000, FPS 0.
Average step reward is -2.4968698024749756.
Some episodes done, average episode reward is -59.9248765962135.

Evaluation average episode reward is -25.76841140166998.

fixed_agent_order_reversed========================== [12, 14, 4, 13, 10, 5, 2, 3, 9, 11, 8, 7, 1, 6, 0]
Env powergym Task 123Bus Algo happo Exp test updates 104/333 episodes, total num timesteps 7488/24000, FPS 0.
Average step reward is -2.406755208969116.
Some episodes done, average episode reward is -57.76212924574062.

Evaluation average episode reward is -26.053947805023526.

fixed_agent_order_reversed========================== [9, 4, 8, 10, 3, 2, 11, 13, 1, 6, 5, 7, 12, 14, 0]
Env powergym Task 123Bus Algo happo Exp test updates 105/333 episodes, total num timesteps 7560/24000, FPS 0.
Average step reward is -2.513152837753296.
Some episodes done, average episode reward is -60.31567035850215.

Evaluation average episode reward is -25.775592129692726.

fixed_agent_order_reversed========================== [12, 14, 9, 4, 13, 8, 2, 3, 10, 1, 11, 7, 0, 5, 6]
Env powergym Task 123Bus Algo happo Exp test updates 106/333 episodes, total num timesteps 7632/24000, FPS 0.
Average step reward is -2.528028964996338.
Some episodes done, average episode reward is -60.67269390988139.

Evaluation average episode reward is -24.92215095332524.

fixed_agent_order_reversed========================== [14, 9, 4, 12, 3, 10, 13, 2, 8, 1, 0, 7, 5, 11, 6]
Env powergym Task 123Bus Algo happo Exp test updates 107/333 episodes, total num timesteps 7704/24000, FPS 0.
Average step reward is -2.224348783493042.
Some episodes done, average episode reward is -53.38437164262425.

Evaluation average episode reward is -24.374223837130376.

fixed_agent_order_reversed========================== [6, 0, 5, 11, 7, 1, 2, 3, 8, 9, 13, 10, 4, 14, 12]
Env powergym Task 123Bus Algo happo Exp test updates 108/333 episodes, total num timesteps 7776/24000, FPS 0.
Average step reward is -2.039957046508789.
Some episodes done, average episode reward is -48.95897002401887.

Evaluation average episode reward is -25.972288751988753.

fixed_agent_order_reversed========================== [8, 2, 9, 3, 10, 4, 11, 0, 6, 13, 1, 5, 12, 14, 7]
Env powergym Task 123Bus Algo happo Exp test updates 109/333 episodes, total num timesteps 7848/24000, FPS 0.
Average step reward is -2.1292238235473633.
Some episodes done, average episode reward is -51.10136728689537.

Evaluation average episode reward is -26.671388238010085.

fixed_agent_order_reversed========================== [6, 11, 9, 8, 5, 0, 3, 10, 2, 1, 4, 7, 13, 14, 12]
Env powergym Task 123Bus Algo happo Exp test updates 110/333 episodes, total num timesteps 7920/24000, FPS 0.
Average step reward is -2.149275302886963.
Some episodes done, average episode reward is -51.582605970895315.

Evaluation average episode reward is -25.873466670956784.

fixed_agent_order_reversed========================== [12, 14, 4, 9, 10, 13, 8, 3, 7, 2, 1, 5, 11, 0, 6]
Env powergym Task 123Bus Algo happo Exp test updates 111/333 episodes, total num timesteps 7992/24000, FPS 0.
Average step reward is -2.1260480880737305.
Some episodes done, average episode reward is -51.02515272657602.

Evaluation average episode reward is -24.751630760018028.

fixed_agent_order_reversed========================== [3, 2, 4, 0, 11, 1, 12, 13, 6, 9, 5, 8, 10, 14, 7]
Env powergym Task 123Bus Algo happo Exp test updates 112/333 episodes, total num timesteps 8064/24000, FPS 0.
Average step reward is -2.0165185928344727.
Some episodes done, average episode reward is -48.39644540180938.

Evaluation average episode reward is -24.15130918689675.

fixed_agent_order_reversed========================== [0, 1, 3, 7, 8, 2, 6, 13, 11, 4, 9, 5, 10, 12, 14]
Env powergym Task 123Bus Algo happo Exp test updates 113/333 episodes, total num timesteps 8136/24000, FPS 0.
Average step reward is -2.015444040298462.
Some episodes done, average episode reward is -48.37065346966224.

Evaluation average episode reward is -23.66178684321902.

fixed_agent_order_reversed========================== [14, 9, 10, 4, 12, 8, 13, 5, 7, 11, 3, 6, 2, 1, 0]
Env powergym Task 123Bus Algo happo Exp test updates 114/333 episodes, total num timesteps 8208/24000, FPS 0.
Average step reward is -2.200376510620117.
Some episodes done, average episode reward is -52.809035793429224.

Evaluation average episode reward is -23.380354587496353.

fixed_agent_order_reversed========================== [8, 9, 10, 3, 4, 2, 11, 13, 12, 1, 0, 5, 6, 14, 7]
Env powergym Task 123Bus Algo happo Exp test updates 115/333 episodes, total num timesteps 8280/24000, FPS 0.
Average step reward is -1.8713908195495605.
Some episodes done, average episode reward is -44.9133756933734.

Evaluation average episode reward is -23.632568405024543.

fixed_agent_order_reversed========================== [9, 10, 4, 3, 8, 2, 11, 13, 5, 1, 12, 6, 0, 14, 7]
Env powergym Task 123Bus Algo happo Exp test updates 116/333 episodes, total num timesteps 8352/24000, FPS 0.
Average step reward is -1.8864729404449463.
Some episodes done, average episode reward is -45.275349511689285.

Evaluation average episode reward is -24.212311943987658.

fixed_agent_order_reversed========================== [9, 7, 14, 6, 10, 11, 8, 13, 4, 5, 1, 12, 3, 2, 0]
Env powergym Task 123Bus Algo happo Exp test updates 117/333 episodes, total num timesteps 8424/24000, FPS 0.
Average step reward is -1.971872091293335.
Some episodes done, average episode reward is -47.32492773061969.

Evaluation average episode reward is -24.07623219875984.

fixed_agent_order_reversed========================== [6, 5, 11, 3, 7, 10, 0, 9, 4, 13, 1, 2, 12, 8, 14]
Env powergym Task 123Bus Algo happo Exp test updates 118/333 episodes, total num timesteps 8496/24000, FPS 0.
Average step reward is -2.0183656215667725.
Some episodes done, average episode reward is -48.440773656562634.

Evaluation average episode reward is -23.97619199747487.

fixed_agent_order_reversed========================== [0, 5, 2, 7, 6, 1, 3, 11, 13, 4, 12, 10, 9, 14, 8]
Env powergym Task 123Bus Algo happo Exp test updates 119/333 episodes, total num timesteps 8568/24000, FPS 0.
Average step reward is -1.8370996713638306.
Some episodes done, average episode reward is -44.09039391248087.

Evaluation average episode reward is -23.94582148109868.

fixed_agent_order_reversed========================== [0, 6, 2, 3, 11, 5, 1, 7, 10, 4, 8, 9, 13, 12, 14]
Env powergym Task 123Bus Algo happo Exp test updates 120/333 episodes, total num timesteps 8640/24000, FPS 0.
Average step reward is -1.9603451490402222.
Some episodes done, average episode reward is -47.04828809502442.

Evaluation average episode reward is -24.22245326735647.

fixed_agent_order_reversed========================== [0, 2, 3, 1, 6, 5, 11, 7, 4, 10, 13, 9, 8, 12, 14]
Env powergym Task 123Bus Algo happo Exp test updates 121/333 episodes, total num timesteps 8712/24000, FPS 0.
Average step reward is -2.1055397987365723.
Some episodes done, average episode reward is -50.53295665318125.

Evaluation average episode reward is -24.42555939406886.

fixed_agent_order_reversed========================== [9, 10, 8, 4, 11, 3, 6, 2, 13, 14, 5, 1, 7, 12, 0]
Env powergym Task 123Bus Algo happo Exp test updates 122/333 episodes, total num timesteps 8784/24000, FPS 0.
Average step reward is -1.744445562362671.
Some episodes done, average episode reward is -41.86669524461005.

Evaluation average episode reward is -24.49494810137755.

fixed_agent_order_reversed========================== [2, 9, 3, 0, 10, 5, 6, 11, 4, 13, 1, 7, 12, 14, 8]
Env powergym Task 123Bus Algo happo Exp test updates 123/333 episodes, total num timesteps 8856/24000, FPS 0.
Average step reward is -1.9720818996429443.
Some episodes done, average episode reward is -47.329969615782204.

Evaluation average episode reward is -24.065404676302467.

fixed_agent_order_reversed========================== [7, 14, 13, 1, 12, 0, 6, 5, 8, 11, 9, 4, 10, 3, 2]
Env powergym Task 123Bus Algo happo Exp test updates 124/333 episodes, total num timesteps 8928/24000, FPS 0.
Average step reward is -1.898491382598877.
Some episodes done, average episode reward is -45.5637936360608.

Evaluation average episode reward is -23.77533491548321.

fixed_agent_order_reversed========================== [9, 5, 6, 11, 4, 2, 10, 3, 0, 8, 13, 7, 1, 12, 14]
Env powergym Task 123Bus Algo happo Exp test updates 125/333 episodes, total num timesteps 9000/24000, FPS 0.
Average step reward is -2.043489694595337.
Some episodes done, average episode reward is -49.043750180810775.

Evaluation average episode reward is -23.826537636427904.

fixed_agent_order_reversed========================== [0, 2, 1, 3, 7, 13, 4, 11, 9, 5, 6, 10, 12, 8, 14]
Env powergym Task 123Bus Algo happo Exp test updates 126/333 episodes, total num timesteps 9072/24000, FPS 0.
Average step reward is -1.9200102090835571.
Some episodes done, average episode reward is -46.08024611619063.

Evaluation average episode reward is -23.826537636427904.

fixed_agent_order_reversed========================== [10, 9, 7, 5, 6, 11, 4, 2, 3, 13, 8, 12, 1, 14, 0]
Env powergym Task 123Bus Algo happo Exp test updates 127/333 episodes, total num timesteps 9144/24000, FPS 0.
Average step reward is -1.7532035112380981.
Some episodes done, average episode reward is -42.07688444797669.

Evaluation average episode reward is -24.471822455861467.

fixed_agent_order_reversed========================== [9, 12, 4, 14, 3, 10, 8, 2, 13, 11, 5, 1, 6, 7, 0]
Env powergym Task 123Bus Algo happo Exp test updates 128/333 episodes, total num timesteps 9216/24000, FPS 0.
Average step reward is -1.7345904111862183.
Some episodes done, average episode reward is -41.63016859948258.

Evaluation average episode reward is -24.835377137762478.

fixed_agent_order_reversed========================== [14, 6, 11, 4, 12, 8, 9, 13, 10, 5, 3, 2, 7, 1, 0]
Env powergym Task 123Bus Algo happo Exp test updates 129/333 episodes, total num timesteps 9288/24000, FPS 0.
Average step reward is -1.7233458757400513.
Some episodes done, average episode reward is -41.360301757885544.

Evaluation average episode reward is -24.740005879065.

fixed_agent_order_reversed========================== [0, 8, 6, 3, 2, 7, 1, 11, 13, 5, 4, 10, 9, 12, 14]
Env powergym Task 123Bus Algo happo Exp test updates 130/333 episodes, total num timesteps 9360/24000, FPS 0.
Average step reward is -1.7496733665466309.
Some episodes done, average episode reward is -41.99215961374308.

Evaluation average episode reward is -24.544827579963115.

fixed_agent_order_reversed========================== [9, 14, 10, 4, 12, 13, 8, 7, 3, 1, 5, 11, 2, 6, 0]
Env powergym Task 123Bus Algo happo Exp test updates 131/333 episodes, total num timesteps 9432/24000, FPS 0.
Average step reward is -1.4651190042495728.
Some episodes done, average episode reward is -35.16285571767475.

Evaluation average episode reward is -24.10581771762914.

fixed_agent_order_reversed========================== [8, 9, 3, 1, 2, 13, 4, 0, 11, 10, 6, 12, 14, 7, 5]
Env powergym Task 123Bus Algo happo Exp test updates 132/333 episodes, total num timesteps 9504/24000, FPS 0.
Average step reward is -1.5490201711654663.
Some episodes done, average episode reward is -37.176483570361604.

Evaluation average episode reward is -23.772462846342354.

fixed_agent_order_reversed========================== [8, 6, 9, 11, 10, 14, 5, 4, 7, 13, 12, 1, 3, 2, 0]
Env powergym Task 123Bus Algo happo Exp test updates 133/333 episodes, total num timesteps 9576/24000, FPS 0.
Average step reward is -1.6100491285324097.
Some episodes done, average episode reward is -38.64117982821063.

Evaluation average episode reward is -24.14622125390812.

fixed_agent_order_reversed========================== [8, 3, 6, 2, 0, 11, 9, 1, 4, 13, 10, 7, 5, 14, 12]
Env powergym Task 123Bus Algo happo Exp test updates 134/333 episodes, total num timesteps 9648/24000, FPS 0.
Average step reward is -1.574269413948059.
Some episodes done, average episode reward is -37.782465721925355.

Evaluation average episode reward is -24.060366924636266.

fixed_agent_order_reversed========================== [8, 0, 10, 2, 6, 3, 11, 1, 9, 4, 7, 13, 5, 12, 14]
Env powergym Task 123Bus Algo happo Exp test updates 135/333 episodes, total num timesteps 9720/24000, FPS 0.
Average step reward is -1.5566086769104004.
Some episodes done, average episode reward is -37.35860668152483.

Evaluation average episode reward is -23.73633304343433.

fixed_agent_order_reversed========================== [2, 6, 11, 10, 3, 4, 8, 5, 9, 14, 1, 0, 12, 13, 7]
Env powergym Task 123Bus Algo happo Exp test updates 136/333 episodes, total num timesteps 9792/24000, FPS 0.
Average step reward is -1.3855050802230835.
Some episodes done, average episode reward is -33.25212344338833.

Evaluation average episode reward is -23.619322400440865.

fixed_agent_order_reversed========================== [9, 8, 13, 4, 7, 3, 2, 10, 1, 5, 12, 11, 0, 6, 14]
Env powergym Task 123Bus Algo happo Exp test updates 137/333 episodes, total num timesteps 9864/24000, FPS 0.
Average step reward is -1.4811241626739502.
Some episodes done, average episode reward is -35.54698026485777.

Evaluation average episode reward is -23.032018137054603.

fixed_agent_order_reversed========================== [7, 14, 12, 13, 1, 0, 5, 6, 8, 11, 10, 4, 9, 2, 3]
Env powergym Task 123Bus Algo happo Exp test updates 138/333 episodes, total num timesteps 9936/24000, FPS 0.
Average step reward is -1.4231314659118652.
Some episodes done, average episode reward is -34.15515811283125.

Evaluation average episode reward is -23.840568213523937.

fixed_agent_order_reversed========================== [9, 7, 0, 13, 10, 1, 12, 8, 4, 3, 2, 5, 11, 14, 6]
Env powergym Task 123Bus Algo happo Exp test updates 139/333 episodes, total num timesteps 10008/24000, FPS 0.
Average step reward is -1.4803900718688965.
Some episodes done, average episode reward is -35.52936449510052.

Evaluation average episode reward is -23.83048248346221.

fixed_agent_order_reversed========================== [6, 11, 5, 8, 7, 10, 12, 9, 13, 1, 2, 4, 14, 0, 3]
Env powergym Task 123Bus Algo happo Exp test updates 140/333 episodes, total num timesteps 10080/24000, FPS 0.
Average step reward is -1.5193164348602295.
Some episodes done, average episode reward is -36.463595517218906.

Evaluation average episode reward is -23.83048248346221.

fixed_agent_order_reversed========================== [9, 10, 4, 11, 14, 6, 5, 8, 13, 7, 3, 12, 2, 1, 0]
Env powergym Task 123Bus Algo happo Exp test updates 141/333 episodes, total num timesteps 10152/24000, FPS 0.
Average step reward is -1.4072909355163574.
Some episodes done, average episode reward is -33.77498325393386.

Evaluation average episode reward is -23.770669330241844.

fixed_agent_order_reversed========================== [7, 14, 13, 12, 5, 0, 1, 9, 10, 4, 3, 11, 2, 6, 8]
Env powergym Task 123Bus Algo happo Exp test updates 142/333 episodes, total num timesteps 10224/24000, FPS 0.
Average step reward is -1.515604853630066.
Some episodes done, average episode reward is -36.37451702841573.

Evaluation average episode reward is -23.242997252965107.

fixed_agent_order_reversed========================== [6, 8, 5, 11, 7, 10, 9, 13, 14, 12, 1, 4, 3, 2, 0]
Env powergym Task 123Bus Algo happo Exp test updates 143/333 episodes, total num timesteps 10296/24000, FPS 0.
Average step reward is -1.2826887369155884.
Some episodes done, average episode reward is -30.784532778246973.

Evaluation average episode reward is -20.049476336346807.

fixed_agent_order_reversed========================== [7, 6, 0, 5, 1, 13, 11, 12, 8, 2, 10, 3, 9, 4, 14]
Env powergym Task 123Bus Algo happo Exp test updates 144/333 episodes, total num timesteps 10368/24000, FPS 0.
Average step reward is -1.5040696859359741.
Some episodes done, average episode reward is -36.09767298471286.

Evaluation average episode reward is -20.025514923756642.

fixed_agent_order_reversed========================== [7, 6, 5, 12, 11, 8, 1, 13, 9, 4, 10, 14, 2, 3, 0]
Env powergym Task 123Bus Algo happo Exp test updates 145/333 episodes, total num timesteps 10440/24000, FPS 0.
Average step reward is -1.2997370958328247.
Some episodes done, average episode reward is -31.193689820628475.

Evaluation average episode reward is -20.05338777172847.

fixed_agent_order_reversed========================== [6, 11, 0, 7, 8, 5, 9, 10, 1, 2, 3, 13, 4, 14, 12]
Env powergym Task 123Bus Algo happo Exp test updates 146/333 episodes, total num timesteps 10512/24000, FPS 0.
Average step reward is -1.3418890237808228.
Some episodes done, average episode reward is -32.20533528215758.

Evaluation average episode reward is -20.111850691052584.

fixed_agent_order_reversed========================== [7, 5, 10, 6, 0, 14, 1, 2, 11, 13, 12, 4, 8, 3, 9]
Env powergym Task 123Bus Algo happo Exp test updates 147/333 episodes, total num timesteps 10584/24000, FPS 0.
Average step reward is -1.241548776626587.
Some episodes done, average episode reward is -29.797172282508683.

Evaluation average episode reward is -20.111850691052584.

fixed_agent_order_reversed========================== [14, 12, 13, 9, 10, 7, 4, 5, 1, 2, 3, 11, 0, 6, 8]
Env powergym Task 123Bus Algo happo Exp test updates 148/333 episodes, total num timesteps 10656/24000, FPS 0.
Average step reward is -1.189455509185791.
Some episodes done, average episode reward is -28.546932386134348.

Evaluation average episode reward is -20.111850691052584.

fixed_agent_order_reversed========================== [0, 2, 3, 1, 9, 7, 4, 13, 8, 10, 5, 11, 6, 12, 14]
Env powergym Task 123Bus Algo happo Exp test updates 149/333 episodes, total num timesteps 10728/24000, FPS 0.
Average step reward is -1.1966955661773682.
Some episodes done, average episode reward is -28.7206930133796.

Evaluation average episode reward is -20.58523514961089.

fixed_agent_order_reversed========================== [14, 12, 7, 13, 5, 1, 4, 11, 6, 9, 10, 8, 3, 2, 0]
Env powergym Task 123Bus Algo happo Exp test updates 150/333 episodes, total num timesteps 10800/24000, FPS 0.
Average step reward is -1.26829993724823.
Some episodes done, average episode reward is -30.439200049946695.

Evaluation average episode reward is -18.946003671509555.

fixed_agent_order_reversed========================== [7, 14, 12, 13, 0, 6, 5, 11, 1, 8, 4, 10, 2, 3, 9]
Env powergym Task 123Bus Algo happo Exp test updates 151/333 episodes, total num timesteps 10872/24000, FPS 0.
Average step reward is -1.1472091674804688.
Some episodes done, average episode reward is -27.533017868414106.

Evaluation average episode reward is -15.139735424255463.

fixed_agent_order_reversed========================== [7, 6, 5, 12, 0, 13, 11, 14, 1, 2, 10, 3, 4, 9, 8]
Env powergym Task 123Bus Algo happo Exp test updates 152/333 episodes, total num timesteps 10944/24000, FPS 0.
Average step reward is -1.2034341096878052.
Some episodes done, average episode reward is -28.88241790690246.

Evaluation average episode reward is -14.307198490029299.

fixed_agent_order_reversed========================== [9, 4, 13, 12, 3, 10, 2, 14, 1, 7, 8, 11, 0, 5, 6]
Env powergym Task 123Bus Algo happo Exp test updates 153/333 episodes, total num timesteps 11016/24000, FPS 0.
Average step reward is -1.2570737600326538.
Some episodes done, average episode reward is -30.169769538866575.

Evaluation average episode reward is -13.629045363364028.

fixed_agent_order_reversed========================== [9, 7, 0, 13, 1, 12, 10, 3, 4, 2, 8, 5, 14, 11, 6]
Env powergym Task 123Bus Algo happo Exp test updates 154/333 episodes, total num timesteps 11088/24000, FPS 0.
Average step reward is -1.0169024467468262.
Some episodes done, average episode reward is -24.40566215739891.

Evaluation average episode reward is -14.678234666135412.

fixed_agent_order_reversed========================== [7, 0, 5, 6, 1, 12, 13, 8, 11, 2, 3, 10, 14, 4, 9]
Env powergym Task 123Bus Algo happo Exp test updates 155/333 episodes, total num timesteps 11160/24000, FPS 0.
Average step reward is -1.0296674966812134.
Some episodes done, average episode reward is -24.712021300243013.

Evaluation average episode reward is -13.993421201141636.

fixed_agent_order_reversed========================== [7, 0, 9, 10, 5, 1, 6, 3, 11, 2, 4, 13, 8, 14, 12]
Env powergym Task 123Bus Algo happo Exp test updates 156/333 episodes, total num timesteps 11232/24000, FPS 0.
Average step reward is -1.0765879154205322.
Some episodes done, average episode reward is -25.83811056984028.

Evaluation average episode reward is -13.564894568677353.

fixed_agent_order_reversed========================== [14, 12, 7, 10, 4, 13, 2, 9, 3, 1, 0, 5, 11, 6, 8]
Env powergym Task 123Bus Algo happo Exp test updates 157/333 episodes, total num timesteps 11304/24000, FPS 0.
Average step reward is -0.9972164630889893.
Some episodes done, average episode reward is -23.933195685089753.

Evaluation average episode reward is -13.564894568677353.

fixed_agent_order_reversed========================== [5, 7, 12, 6, 14, 13, 11, 9, 3, 4, 1, 10, 0, 2, 8]
Env powergym Task 123Bus Algo happo Exp test updates 158/333 episodes, total num timesteps 11376/24000, FPS 0.
Average step reward is -1.0776582956314087.
Some episodes done, average episode reward is -25.863801976224348.

Evaluation average episode reward is -13.564894568677353.

fixed_agent_order_reversed========================== [7, 5, 6, 14, 12, 13, 11, 8, 10, 1, 9, 4, 0, 3, 2]
Env powergym Task 123Bus Algo happo Exp test updates 159/333 episodes, total num timesteps 11448/24000, FPS 0.
Average step reward is -0.9932297468185425.
Some episodes done, average episode reward is -23.837513085349425.

Evaluation average episode reward is -13.322459801985575.

fixed_agent_order_reversed========================== [8, 2, 3, 0, 4, 9, 10, 1, 13, 11, 12, 7, 5, 6, 14]
Env powergym Task 123Bus Algo happo Exp test updates 160/333 episodes, total num timesteps 11520/24000, FPS 0.
Average step reward is -1.1777435541152954.
Some episodes done, average episode reward is -28.265847237167083.

Evaluation average episode reward is -13.33441987755986.

fixed_agent_order_reversed========================== [7, 6, 5, 14, 13, 11, 1, 10, 4, 8, 12, 3, 0, 2, 9]
Env powergym Task 123Bus Algo happo Exp test updates 161/333 episodes, total num timesteps 11592/24000, FPS 0.
Average step reward is -1.0393574237823486.
Some episodes done, average episode reward is -24.944578172062403.

Evaluation average episode reward is -13.33441987755986.

fixed_agent_order_reversed========================== [8, 7, 6, 5, 11, 9, 13, 12, 14, 10, 4, 1, 0, 3, 2]
Env powergym Task 123Bus Algo happo Exp test updates 162/333 episodes, total num timesteps 11664/24000, FPS 0.
Average step reward is -0.9405600428581238.
Some episodes done, average episode reward is -22.57344175409759.

Evaluation average episode reward is -13.33441987755986.

fixed_agent_order_reversed========================== [2, 5, 4, 13, 0, 11, 6, 10, 1, 3, 14, 12, 7, 8, 9]
Env powergym Task 123Bus Algo happo Exp test updates 163/333 episodes, total num timesteps 11736/24000, FPS 0.
Average step reward is -0.9926190972328186.
Some episodes done, average episode reward is -23.822858602140244.

Evaluation average episode reward is -14.012872150092749.

fixed_agent_order_reversed========================== [6, 11, 5, 8, 0, 7, 9, 3, 2, 13, 1, 4, 10, 12, 14]
Env powergym Task 123Bus Algo happo Exp test updates 164/333 episodes, total num timesteps 11808/24000, FPS 0.
Average step reward is -1.0489935874938965.
Some episodes done, average episode reward is -25.175844990427688.

Evaluation average episode reward is -13.968910693267146.

fixed_agent_order_reversed========================== [12, 4, 14, 10, 3, 13, 5, 9, 7, 2, 11, 8, 1, 6, 0]
Env powergym Task 123Bus Algo happo Exp test updates 165/333 episodes, total num timesteps 11880/24000, FPS 0.
Average step reward is -1.0024538040161133.
Some episodes done, average episode reward is -24.058891306378303.

Evaluation average episode reward is -12.531124052878111.

fixed_agent_order_reversed========================== [8, 7, 13, 9, 4, 10, 1, 11, 2, 3, 5, 6, 12, 0, 14]
Env powergym Task 123Bus Algo happo Exp test updates 166/333 episodes, total num timesteps 11952/24000, FPS 0.
Average step reward is -1.0168249607086182.
Some episodes done, average episode reward is -24.40379737445093.

Evaluation average episode reward is -12.574648068003816.

fixed_agent_order_reversed========================== [14, 7, 13, 6, 1, 11, 12, 8, 4, 5, 10, 3, 2, 0, 9]
Env powergym Task 123Bus Algo happo Exp test updates 167/333 episodes, total num timesteps 12024/24000, FPS 0.
Average step reward is -1.1349830627441406.
Some episodes done, average episode reward is -27.23959174186551.

Evaluation average episode reward is -12.320150408209495.

fixed_agent_order_reversed========================== [6, 8, 11, 5, 0, 13, 2, 7, 1, 3, 10, 4, 12, 9, 14]
Env powergym Task 123Bus Algo happo Exp test updates 168/333 episodes, total num timesteps 12096/24000, FPS 0.
Average step reward is -1.079766035079956.
Some episodes done, average episode reward is -25.914386688748365.

Evaluation average episode reward is -12.320150408209495.

fixed_agent_order_reversed========================== [6, 8, 11, 5, 12, 2, 4, 7, 1, 10, 14, 13, 3, 9, 0]
Env powergym Task 123Bus Algo happo Exp test updates 169/333 episodes, total num timesteps 12168/24000, FPS 0.
Average step reward is -0.9181996583938599.
Some episodes done, average episode reward is -22.036790697114128.

Evaluation average episode reward is -12.295136646390496.

fixed_agent_order_reversed========================== [6, 5, 7, 11, 10, 14, 13, 9, 4, 8, 12, 1, 3, 2, 0]
Env powergym Task 123Bus Algo happo Exp test updates 170/333 episodes, total num timesteps 12240/24000, FPS 0.
Average step reward is -0.916367769241333.
Some episodes done, average episode reward is -21.992826701175417.

Evaluation average episode reward is -12.320156766729681.

fixed_agent_order_reversed========================== [0, 7, 6, 2, 1, 5, 13, 3, 11, 10, 9, 4, 12, 8, 14]
Env powergym Task 123Bus Algo happo Exp test updates 171/333 episodes, total num timesteps 12312/24000, FPS 0.
Average step reward is -0.9404144287109375.
Some episodes done, average episode reward is -22.569947498339868.

Evaluation average episode reward is -11.48646555326175.

fixed_agent_order_reversed========================== [2, 6, 3, 11, 4, 8, 0, 5, 1, 10, 13, 14, 12, 9, 7]
Env powergym Task 123Bus Algo happo Exp test updates 172/333 episodes, total num timesteps 12384/24000, FPS 0.
Average step reward is -0.8444276452064514.
Some episodes done, average episode reward is -20.26626355676606.

Evaluation average episode reward is -11.261115292910958.

fixed_agent_order_reversed========================== [2, 3, 0, 4, 1, 10, 9, 13, 11, 12, 7, 5, 8, 6, 14]
Env powergym Task 123Bus Algo happo Exp test updates 173/333 episodes, total num timesteps 12456/24000, FPS 0.
Average step reward is -0.868279755115509.
Some episodes done, average episode reward is -20.838715151245605.

Evaluation average episode reward is -11.12703050118582.

fixed_agent_order_reversed========================== [6, 11, 8, 5, 4, 14, 2, 10, 3, 13, 12, 1, 9, 7, 0]
Env powergym Task 123Bus Algo happo Exp test updates 174/333 episodes, total num timesteps 12528/24000, FPS 0.
Average step reward is -0.8322906494140625.
Some episodes done, average episode reward is -19.974977075235785.

Evaluation average episode reward is -11.895365280593348.

fixed_agent_order_reversed========================== [6, 5, 2, 11, 8, 3, 9, 4, 10, 0, 13, 7, 1, 12, 14]
Env powergym Task 123Bus Algo happo Exp test updates 175/333 episodes, total num timesteps 12600/24000, FPS 0.
Average step reward is -0.7826268672943115.
Some episodes done, average episode reward is -18.783044383714895.

Evaluation average episode reward is -11.93113954195456.

fixed_agent_order_reversed========================== [6, 14, 11, 5, 7, 8, 12, 10, 9, 13, 4, 2, 1, 0, 3]
Env powergym Task 123Bus Algo happo Exp test updates 176/333 episodes, total num timesteps 12672/24000, FPS 0.
Average step reward is -0.8317335247993469.
Some episodes done, average episode reward is -19.961604336926836.

Evaluation average episode reward is -11.93113954195456.

fixed_agent_order_reversed========================== [9, 10, 8, 4, 14, 11, 5, 12, 6, 13, 3, 7, 2, 1, 0]
Env powergym Task 123Bus Algo happo Exp test updates 177/333 episodes, total num timesteps 12744/24000, FPS 0.
Average step reward is -0.8437261581420898.
Some episodes done, average episode reward is -20.24942895849414.

Evaluation average episode reward is -11.93113954195456.

fixed_agent_order_reversed========================== [6, 8, 11, 5, 9, 7, 10, 13, 4, 12, 1, 2, 3, 14, 0]
Env powergym Task 123Bus Algo happo Exp test updates 178/333 episodes, total num timesteps 12816/24000, FPS 0.
Average step reward is -0.7761150598526001.
Some episodes done, average episode reward is -18.62676127248956.

Evaluation average episode reward is -11.93113954195456.

fixed_agent_order_reversed========================== [9, 4, 3, 8, 2, 11, 10, 13, 12, 6, 14, 1, 5, 0, 7]
Env powergym Task 123Bus Algo happo Exp test updates 179/333 episodes, total num timesteps 12888/24000, FPS 0.
Average step reward is -0.8824658393859863.
Some episodes done, average episode reward is -21.179179623261508.

Evaluation average episode reward is -11.956087196868465.

fixed_agent_order_reversed========================== [8, 0, 13, 7, 2, 6, 5, 1, 4, 3, 14, 12, 11, 9, 10]
Env powergym Task 123Bus Algo happo Exp test updates 180/333 episodes, total num timesteps 12960/24000, FPS 0.
Average step reward is -0.9196106791496277.
Some episodes done, average episode reward is -22.070655817919505.

Evaluation average episode reward is -12.040120327680953.

fixed_agent_order_reversed========================== [14, 12, 7, 13, 9, 4, 5, 10, 8, 1, 11, 6, 3, 2, 0]
Env powergym Task 123Bus Algo happo Exp test updates 181/333 episodes, total num timesteps 13032/24000, FPS 0.
Average step reward is -0.7878208756446838.
Some episodes done, average episode reward is -18.9077013926939.

Evaluation average episode reward is -11.956087196868465.

fixed_agent_order_reversed========================== [0, 3, 2, 10, 13, 4, 9, 1, 11, 5, 12, 7, 6, 8, 14]
Env powergym Task 123Bus Algo happo Exp test updates 182/333 episodes, total num timesteps 13104/24000, FPS 0.
Average step reward is -0.7616774439811707.
Some episodes done, average episode reward is -18.280257632666515.

Evaluation average episode reward is -13.070724248252008.

fixed_agent_order_reversed========================== [14, 9, 4, 10, 12, 8, 3, 11, 13, 2, 5, 6, 1, 7, 0]
Env powergym Task 123Bus Algo happo Exp test updates 183/333 episodes, total num timesteps 13176/24000, FPS 0.
Average step reward is -0.7467655539512634.
Some episodes done, average episode reward is -17.922374373823782.

Evaluation average episode reward is -13.073330761052475.

fixed_agent_order_reversed========================== [14, 0, 3, 12, 2, 13, 4, 1, 10, 9, 7, 5, 11, 8, 6]
Env powergym Task 123Bus Algo happo Exp test updates 184/333 episodes, total num timesteps 13248/24000, FPS 0.
Average step reward is -0.7633793950080872.
Some episodes done, average episode reward is -18.32110692049054.

Evaluation average episode reward is -13.429704018564122.

fixed_agent_order_reversed========================== [6, 8, 9, 11, 10, 5, 13, 4, 7, 2, 3, 12, 1, 0, 14]
Env powergym Task 123Bus Algo happo Exp test updates 185/333 episodes, total num timesteps 13320/24000, FPS 0.
Average step reward is -0.7449169158935547.
Some episodes done, average episode reward is -17.87800576237749.

Evaluation average episode reward is -13.429704018564122.

fixed_agent_order_reversed========================== [3, 4, 2, 9, 14, 10, 13, 0, 1, 12, 7, 5, 11, 8, 6]
Env powergym Task 123Bus Algo happo Exp test updates 186/333 episodes, total num timesteps 13392/24000, FPS 0.
Average step reward is -0.7478373646736145.
Some episodes done, average episode reward is -17.948097365360994.

Evaluation average episode reward is -13.46638277304554.

fixed_agent_order_reversed========================== [9, 4, 10, 8, 3, 2, 13, 11, 12, 1, 14, 5, 6, 7, 0]
Env powergym Task 123Bus Algo happo Exp test updates 187/333 episodes, total num timesteps 13464/24000, FPS 0.
Average step reward is -0.8212268352508545.
Some episodes done, average episode reward is -19.709443104887868.

Evaluation average episode reward is -13.861806145518992.

fixed_agent_order_reversed========================== [9, 10, 6, 11, 13, 4, 14, 8, 7, 5, 3, 12, 2, 1, 0]
Env powergym Task 123Bus Algo happo Exp test updates 188/333 episodes, total num timesteps 13536/24000, FPS 0.
Average step reward is -0.8387961387634277.
Some episodes done, average episode reward is -20.13110731182413.

Evaluation average episode reward is -13.591605775231223.

fixed_agent_order_reversed========================== [14, 9, 7, 12, 10, 13, 4, 5, 11, 8, 1, 6, 3, 2, 0]
Env powergym Task 123Bus Algo happo Exp test updates 189/333 episodes, total num timesteps 13608/24000, FPS 0.
Average step reward is -0.7845631241798401.
Some episodes done, average episode reward is -18.829515407314975.

Evaluation average episode reward is -13.58250745680084.

fixed_agent_order_reversed========================== [3, 2, 4, 9, 10, 14, 0, 13, 1, 5, 7, 12, 11, 6, 8]
Env powergym Task 123Bus Algo happo Exp test updates 190/333 episodes, total num timesteps 13680/24000, FPS 0.
Average step reward is -0.7300125360488892.
Some episodes done, average episode reward is -17.520300571605016.

Evaluation average episode reward is -13.517001558132685.

fixed_agent_order_reversed========================== [6, 7, 11, 5, 8, 0, 9, 1, 13, 10, 3, 4, 2, 14, 12]
Env powergym Task 123Bus Algo happo Exp test updates 191/333 episodes, total num timesteps 13752/24000, FPS 0.
Average step reward is -0.7800097465515137.
Some episodes done, average episode reward is -18.72023471723163.

Evaluation average episode reward is -13.461224715116849.

fixed_agent_order_reversed========================== [6, 8, 11, 9, 2, 3, 0, 1, 10, 4, 5, 13, 7, 14, 12]
Env powergym Task 123Bus Algo happo Exp test updates 192/333 episodes, total num timesteps 13824/24000, FPS 0.
Average step reward is -0.7038726806640625.
Some episodes done, average episode reward is -16.892944754307138.

Evaluation average episode reward is -13.427786978112467.

fixed_agent_order_reversed========================== [9, 8, 10, 4, 11, 6, 5, 13, 3, 7, 1, 12, 2, 14, 0]
Env powergym Task 123Bus Algo happo Exp test updates 193/333 episodes, total num timesteps 13896/24000, FPS 0.
Average step reward is -0.7231180667877197.
Some episodes done, average episode reward is -17.354832399712787.

Evaluation average episode reward is -13.439744630500842.

fixed_agent_order_reversed========================== [14, 7, 12, 13, 5, 6, 10, 11, 9, 4, 1, 8, 3, 2, 0]
Env powergym Task 123Bus Algo happo Exp test updates 194/333 episodes, total num timesteps 13968/24000, FPS 0.
Average step reward is -0.732981264591217.
Some episodes done, average episode reward is -17.591549810852612.

Evaluation average episode reward is -13.461224715116849.

fixed_agent_order_reversed========================== [14, 12, 7, 13, 1, 0, 4, 3, 9, 5, 2, 10, 11, 8, 6]
Env powergym Task 123Bus Algo happo Exp test updates 195/333 episodes, total num timesteps 14040/24000, FPS 0.
Average step reward is -0.6532612442970276.
Some episodes done, average episode reward is -15.678268791050526.

Evaluation average episode reward is -13.719463736248526.

fixed_agent_order_reversed========================== [14, 4, 12, 11, 13, 9, 2, 5, 10, 3, 6, 1, 8, 7, 0]
Env powergym Task 123Bus Algo happo Exp test updates 196/333 episodes, total num timesteps 14112/24000, FPS 0.
Average step reward is -0.6266927719116211.
Some episodes done, average episode reward is -15.040626471613031.

Evaluation average episode reward is -13.332886742494244.

fixed_agent_order_reversed========================== [9, 0, 3, 10, 7, 4, 1, 2, 13, 8, 12, 5, 11, 6, 14]
Env powergym Task 123Bus Algo happo Exp test updates 197/333 episodes, total num timesteps 14184/24000, FPS 0.
Average step reward is -0.613688051700592.
Some episodes done, average episode reward is -14.728513441931932.

Evaluation average episode reward is -13.498137624168224.

fixed_agent_order_reversed========================== [9, 3, 10, 2, 4, 11, 5, 6, 1, 0, 8, 13, 7, 12, 14]
Env powergym Task 123Bus Algo happo Exp test updates 198/333 episodes, total num timesteps 14256/24000, FPS 0.
Average step reward is -0.6333770751953125.
Some episodes done, average episode reward is -15.201049560202378.

Evaluation average episode reward is -13.426333487719488.

fixed_agent_order_reversed========================== [6, 9, 11, 7, 13, 8, 10, 5, 1, 0, 4, 3, 14, 2, 12]
Env powergym Task 123Bus Algo happo Exp test updates 199/333 episodes, total num timesteps 14328/24000, FPS 0.
Average step reward is -0.6486581563949585.
Some episodes done, average episode reward is -15.567796441782024.

Evaluation average episode reward is -13.413945016348222.

fixed_agent_order_reversed========================== [14, 9, 10, 13, 3, 4, 2, 12, 5, 11, 0, 1, 7, 6, 8]
Env powergym Task 123Bus Algo happo Exp test updates 200/333 episodes, total num timesteps 14400/24000, FPS 0.
Average step reward is -0.6029660105705261.
Some episodes done, average episode reward is -14.471183097345625.

Evaluation average episode reward is -13.413945016348222.

fixed_agent_order_reversed========================== [9, 14, 12, 13, 4, 10, 7, 3, 2, 1, 8, 5, 11, 0, 6]
Env powergym Task 123Bus Algo happo Exp test updates 201/333 episodes, total num timesteps 14472/24000, FPS 0.
Average step reward is -0.6262983083724976.
Some episodes done, average episode reward is -15.03115904939517.

Evaluation average episode reward is -13.413945016348222.

fixed_agent_order_reversed========================== [9, 4, 13, 10, 7, 2, 3, 11, 1, 5, 14, 8, 6, 0, 12]
Env powergym Task 123Bus Algo happo Exp test updates 202/333 episodes, total num timesteps 14544/24000, FPS 0.
Average step reward is -0.6553151607513428.
Some episodes done, average episode reward is -15.727564361573926.

Evaluation average episode reward is -11.167524743929263.

fixed_agent_order_reversed========================== [0, 2, 3, 4, 1, 13, 10, 9, 14, 7, 12, 5, 11, 8, 6]
Env powergym Task 123Bus Algo happo Exp test updates 203/333 episodes, total num timesteps 14616/24000, FPS 0.
Average step reward is -0.6342509388923645.
Some episodes done, average episode reward is -15.222023713180905.

Evaluation average episode reward is -11.069158338476564.

fixed_agent_order_reversed========================== [14, 0, 3, 13, 10, 12, 1, 2, 4, 9, 8, 7, 5, 11, 6]
Env powergym Task 123Bus Algo happo Exp test updates 204/333 episodes, total num timesteps 14688/24000, FPS 0.
Average step reward is -0.6370116472244263.
Some episodes done, average episode reward is -15.288277441476117.

Evaluation average episode reward is -11.069158338476564.

fixed_agent_order_reversed========================== [9, 0, 3, 2, 6, 10, 7, 11, 1, 4, 8, 13, 5, 12, 14]
Env powergym Task 123Bus Algo happo Exp test updates 205/333 episodes, total num timesteps 14760/24000, FPS 0.
Average step reward is -0.6076724529266357.
Some episodes done, average episode reward is -14.584140177601276.

Evaluation average episode reward is -11.091714178361448.

fixed_agent_order_reversed========================== [4, 14, 2, 3, 10, 13, 8, 1, 5, 11, 12, 0, 9, 7, 6]
Env powergym Task 123Bus Algo happo Exp test updates 206/333 episodes, total num timesteps 14832/24000, FPS 0.
Average step reward is -0.6600704193115234.
Some episodes done, average episode reward is -15.841689637910397.

Evaluation average episode reward is -11.14540119231262.

fixed_agent_order_reversed========================== [6, 11, 12, 9, 3, 5, 4, 13, 10, 14, 2, 1, 8, 7, 0]
Env powergym Task 123Bus Algo happo Exp test updates 207/333 episodes, total num timesteps 14904/24000, FPS 0.
Average step reward is -0.6852982044219971.
Some episodes done, average episode reward is -16.44715709361093.

Evaluation average episode reward is -11.14540119231262.

fixed_agent_order_reversed========================== [14, 5, 7, 10, 12, 4, 13, 8, 2, 1, 3, 11, 0, 6, 9]
Env powergym Task 123Bus Algo happo Exp test updates 208/333 episodes, total num timesteps 14976/24000, FPS 0.
Average step reward is -0.557927131652832.
Some episodes done, average episode reward is -13.390250957877077.

Evaluation average episode reward is -11.140229220054394.

fixed_agent_order_reversed========================== [3, 2, 9, 4, 10, 0, 13, 1, 12, 14, 5, 11, 7, 8, 6]
Env powergym Task 123Bus Algo happo Exp test updates 209/333 episodes, total num timesteps 15048/24000, FPS 0.
Average step reward is -0.5626895427703857.
Some episodes done, average episode reward is -13.504548662928778.

Evaluation average episode reward is -10.944251345762979.

fixed_agent_order_reversed========================== [3, 10, 6, 5, 11, 4, 1, 7, 12, 0, 8, 2, 14, 13, 9]
Env powergym Task 123Bus Algo happo Exp test updates 210/333 episodes, total num timesteps 15120/24000, FPS 0.
Average step reward is -0.628783106803894.
Some episodes done, average episode reward is -15.090794918113067.

Evaluation average episode reward is -10.944251345762979.

fixed_agent_order_reversed========================== [14, 9, 10, 4, 8, 11, 5, 6, 12, 3, 13, 2, 1, 7, 0]
Env powergym Task 123Bus Algo happo Exp test updates 211/333 episodes, total num timesteps 15192/24000, FPS 0.
Average step reward is -0.5325214862823486.
Some episodes done, average episode reward is -12.780515298777678.

Evaluation average episode reward is -10.733483525687275.

fixed_agent_order_reversed========================== [9, 8, 4, 5, 11, 10, 6, 2, 12, 3, 7, 13, 1, 14, 0]
Env powergym Task 123Bus Algo happo Exp test updates 212/333 episodes, total num timesteps 15264/24000, FPS 0.
Average step reward is -0.5249879360198975.
Some episodes done, average episode reward is -12.599710397808893.

Evaluation average episode reward is -10.733483525687275.

fixed_agent_order_reversed========================== [6, 11, 5, 0, 2, 3, 8, 10, 9, 4, 1, 7, 13, 12, 14]
Env powergym Task 123Bus Algo happo Exp test updates 213/333 episodes, total num timesteps 15336/24000, FPS 0.
Average step reward is -0.5385358333587646.
Some episodes done, average episode reward is -12.92486043121871.

Evaluation average episode reward is -10.733483525687275.

fixed_agent_order_reversed========================== [7, 9, 13, 0, 12, 1, 10, 5, 14, 4, 3, 11, 8, 2, 6]
Env powergym Task 123Bus Algo happo Exp test updates 214/333 episodes, total num timesteps 15408/24000, FPS 0.
Average step reward is -0.555917501449585.
Some episodes done, average episode reward is -13.342020888583443.

Evaluation average episode reward is -10.733483525687275.

fixed_agent_order_reversed========================== [4, 14, 2, 13, 10, 3, 12, 0, 7, 9, 1, 5, 8, 11, 6]
Env powergym Task 123Bus Algo happo Exp test updates 215/333 episodes, total num timesteps 15480/24000, FPS 0.
Average step reward is -0.5323992967605591.
Some episodes done, average episode reward is -12.777582575840341.

Evaluation average episode reward is -10.750202011974318.

fixed_agent_order_reversed========================== [9, 8, 10, 7, 13, 3, 4, 0, 1, 11, 2, 6, 12, 5, 14]
Env powergym Task 123Bus Algo happo Exp test updates 216/333 episodes, total num timesteps 15552/24000, FPS 0.
Average step reward is -0.5202453136444092.
Some episodes done, average episode reward is -12.485887926756504.

Evaluation average episode reward is -10.750202011974318.

fixed_agent_order_reversed========================== [9, 10, 4, 5, 14, 12, 13, 7, 3, 2, 11, 1, 8, 6, 0]
Env powergym Task 123Bus Algo happo Exp test updates 217/333 episodes, total num timesteps 15624/24000, FPS 0.
Average step reward is -0.5965559482574463.
Some episodes done, average episode reward is -14.317341173379994.

Evaluation average episode reward is -11.072868048416334.

fixed_agent_order_reversed========================== [3, 4, 2, 8, 9, 10, 13, 11, 1, 5, 12, 0, 14, 6, 7]
Env powergym Task 123Bus Algo happo Exp test updates 218/333 episodes, total num timesteps 15696/24000, FPS 0.
Average step reward is -0.5417128205299377.
Some episodes done, average episode reward is -13.001107346661504.

Evaluation average episode reward is -11.156947708996185.

fixed_agent_order_reversed========================== [5, 10, 4, 2, 7, 8, 3, 11, 9, 13, 1, 14, 0, 6, 12]
Env powergym Task 123Bus Algo happo Exp test updates 219/333 episodes, total num timesteps 15768/24000, FPS 0.
Average step reward is -0.5456898212432861.
Some episodes done, average episode reward is -13.096555933808588.

Evaluation average episode reward is -11.156947708996185.

fixed_agent_order_reversed========================== [0, 2, 3, 6, 5, 11, 4, 8, 10, 1, 9, 13, 7, 12, 14]
Env powergym Task 123Bus Algo happo Exp test updates 220/333 episodes, total num timesteps 15840/24000, FPS 0.
Average step reward is -0.4956376850605011.
Some episodes done, average episode reward is -11.895303485224682.

Evaluation average episode reward is -11.101823175335738.

fixed_agent_order_reversed========================== [6, 11, 8, 5, 2, 3, 10, 0, 4, 9, 1, 13, 7, 12, 14]
Env powergym Task 123Bus Algo happo Exp test updates 221/333 episodes, total num timesteps 15912/24000, FPS 0.
Average step reward is -0.49863168597221375.
Some episodes done, average episode reward is -11.967160381675171.

Evaluation average episode reward is -11.101823175335738.

fixed_agent_order_reversed========================== [2, 3, 4, 5, 11, 6, 10, 12, 14, 8, 0, 9, 1, 13, 7]
Env powergym Task 123Bus Algo happo Exp test updates 222/333 episodes, total num timesteps 15984/24000, FPS 0.
Average step reward is -0.5151306986808777.
Some episodes done, average episode reward is -12.363135091335039.

Evaluation average episode reward is -11.101823175335738.

fixed_agent_order_reversed========================== [2, 12, 4, 9, 3, 10, 8, 13, 5, 14, 11, 1, 0, 7, 6]
Env powergym Task 123Bus Algo happo Exp test updates 223/333 episodes, total num timesteps 16056/24000, FPS 0.
Average step reward is -0.5537693500518799.
Some episodes done, average episode reward is -13.290463022459656.

Evaluation average episode reward is -11.017417227853988.

fixed_agent_order_reversed========================== [10, 9, 4, 13, 11, 12, 7, 6, 3, 2, 5, 14, 1, 8, 0]
Env powergym Task 123Bus Algo happo Exp test updates 224/333 episodes, total num timesteps 16128/24000, FPS 0.
Average step reward is -0.4934232532978058.
Some episodes done, average episode reward is -11.842158958999761.

Evaluation average episode reward is -11.017417227853988.

fixed_agent_order_reversed========================== [7, 9, 13, 12, 8, 1, 10, 0, 4, 3, 2, 14, 5, 11, 6]
Env powergym Task 123Bus Algo happo Exp test updates 225/333 episodes, total num timesteps 16200/24000, FPS 0.
Average step reward is -0.5121488571166992.
Some episodes done, average episode reward is -12.291571806981132.

Evaluation average episode reward is -11.017417227853988.

fixed_agent_order_reversed========================== [3, 4, 2, 9, 10, 12, 13, 5, 0, 1, 8, 11, 7, 14, 6]
Env powergym Task 123Bus Algo happo Exp test updates 226/333 episodes, total num timesteps 16272/24000, FPS 0.
Average step reward is -0.5217718482017517.
Some episodes done, average episode reward is -12.522523310597919.

Evaluation average episode reward is -11.017417227853988.

fixed_agent_order_reversed========================== [7, 5, 12, 13, 14, 9, 1, 0, 4, 10, 3, 11, 2, 8, 6]
Env powergym Task 123Bus Algo happo Exp test updates 227/333 episodes, total num timesteps 16344/24000, FPS 0.
Average step reward is -0.48176226019859314.
Some episodes done, average episode reward is -11.562294007475879.

Evaluation average episode reward is -11.017417227853988.

fixed_agent_order_reversed========================== [14, 0, 13, 7, 12, 9, 2, 1, 5, 3, 10, 6, 11, 4, 8]
Env powergym Task 123Bus Algo happo Exp test updates 228/333 episodes, total num timesteps 16416/24000, FPS 0.
Average step reward is -0.4726126492023468.
Some episodes done, average episode reward is -11.342703706032088.

Evaluation average episode reward is -11.072868048416334.

fixed_agent_order_reversed========================== [7, 5, 6, 0, 12, 11, 14, 2, 1, 13, 10, 8, 4, 3, 9]
Env powergym Task 123Bus Algo happo Exp test updates 229/333 episodes, total num timesteps 16488/24000, FPS 0.
Average step reward is -0.5052595138549805.
Some episodes done, average episode reward is -12.126226376829328.

Evaluation average episode reward is -10.733483525687275.

fixed_agent_order_reversed========================== [9, 10, 4, 7, 13, 3, 5, 12, 2, 1, 14, 11, 8, 0, 6]
Env powergym Task 123Bus Algo happo Exp test updates 230/333 episodes, total num timesteps 16560/24000, FPS 0.
Average step reward is -0.4771076738834381.
Some episodes done, average episode reward is -11.45058323890624.

Evaluation average episode reward is -10.750202011974318.

fixed_agent_order_reversed========================== [0, 7, 6, 1, 13, 5, 11, 12, 8, 9, 2, 3, 10, 14, 4]
Env powergym Task 123Bus Algo happo Exp test updates 231/333 episodes, total num timesteps 16632/24000, FPS 0.
Average step reward is -0.48200246691703796.
Some episodes done, average episode reward is -11.568059487124225.

Evaluation average episode reward is -10.750202011974318.

fixed_agent_order_reversed========================== [8, 6, 14, 12, 10, 11, 4, 13, 5, 9, 3, 7, 1, 2, 0]
Env powergym Task 123Bus Algo happo Exp test updates 232/333 episodes, total num timesteps 16704/24000, FPS 0.
Average step reward is -0.5105149745941162.
Some episodes done, average episode reward is -12.25235894299214.

Evaluation average episode reward is -10.750202011974318.

fixed_agent_order_reversed========================== [8, 6, 5, 11, 9, 7, 10, 4, 2, 1, 3, 13, 0, 12, 14]
Env powergym Task 123Bus Algo happo Exp test updates 233/333 episodes, total num timesteps 16776/24000, FPS 0.
Average step reward is -0.5112435817718506.
Some episodes done, average episode reward is -12.269845444539243.

Evaluation average episode reward is -10.750202011974318.

fixed_agent_order_reversed========================== [6, 8, 11, 9, 4, 3, 10, 5, 13, 2, 12, 1, 0, 7, 14]
Env powergym Task 123Bus Algo happo Exp test updates 234/333 episodes, total num timesteps 16848/24000, FPS 0.
Average step reward is -0.4999848008155823.
Some episodes done, average episode reward is -11.99963563879867.

Evaluation average episode reward is -10.750202011974318.

fixed_agent_order_reversed========================== [9, 4, 13, 3, 8, 10, 2, 12, 1, 7, 14, 0, 11, 5, 6]
Env powergym Task 123Bus Algo happo Exp test updates 235/333 episodes, total num timesteps 16920/24000, FPS 0.
Average step reward is -0.49491098523139954.
Some episodes done, average episode reward is -11.87786447953431.

Evaluation average episode reward is -10.750202011974318.

fixed_agent_order_reversed========================== [7, 13, 14, 6, 12, 1, 0, 11, 5, 9, 4, 10, 2, 3, 8]
Env powergym Task 123Bus Algo happo Exp test updates 236/333 episodes, total num timesteps 16992/24000, FPS 0.
Average step reward is -0.4928569793701172.
Some episodes done, average episode reward is -11.828567320860492.

Evaluation average episode reward is -10.709464152358665.

fixed_agent_order_reversed========================== [7, 0, 6, 1, 5, 11, 13, 8, 12, 3, 2, 10, 4, 9, 14]
Env powergym Task 123Bus Algo happo Exp test updates 237/333 episodes, total num timesteps 17064/24000, FPS 0.
Average step reward is -0.46678242087364197.
Some episodes done, average episode reward is -11.202778132755933.

Evaluation average episode reward is -10.709464152358665.

fixed_agent_order_reversed========================== [7, 0, 5, 14, 6, 1, 12, 13, 11, 10, 2, 4, 3, 8, 9]
Env powergym Task 123Bus Algo happo Exp test updates 238/333 episodes, total num timesteps 17136/24000, FPS 0.
Average step reward is -0.5136680006980896.
Some episodes done, average episode reward is -12.328032446777224.

Evaluation average episode reward is -10.709464152358665.

fixed_agent_order_reversed========================== [7, 6, 0, 14, 5, 13, 11, 12, 1, 10, 9, 8, 3, 4, 2]
Env powergym Task 123Bus Algo happo Exp test updates 239/333 episodes, total num timesteps 17208/24000, FPS 0.
Average step reward is -0.5052666068077087.
Some episodes done, average episode reward is -12.126397932951091.

Evaluation average episode reward is -10.709464152358665.

fixed_agent_order_reversed========================== [8, 9, 4, 12, 13, 10, 11, 6, 14, 1, 2, 7, 3, 5, 0]
Env powergym Task 123Bus Algo happo Exp test updates 240/333 episodes, total num timesteps 17280/24000, FPS 0.
Average step reward is -0.49202075600624084.
Some episodes done, average episode reward is -11.80849833895338.

Evaluation average episode reward is -10.709464152358665.

fixed_agent_order_reversed========================== [2, 3, 0, 8, 9, 4, 10, 1, 11, 6, 5, 13, 7, 12, 14]
Env powergym Task 123Bus Algo happo Exp test updates 241/333 episodes, total num timesteps 17352/24000, FPS 0.
Average step reward is -0.474759578704834.
Some episodes done, average episode reward is -11.394230137494246.

Evaluation average episode reward is -10.709464152358665.

fixed_agent_order_reversed========================== [9, 8, 4, 10, 3, 2, 11, 12, 6, 13, 1, 5, 14, 0, 7]
Env powergym Task 123Bus Algo happo Exp test updates 242/333 episodes, total num timesteps 17424/24000, FPS 0.
Average step reward is -0.5169627666473389.
Some episodes done, average episode reward is -12.407105618503929.

Evaluation average episode reward is -10.709464152358665.

fixed_agent_order_reversed========================== [14, 13, 7, 12, 9, 10, 1, 4, 11, 3, 8, 6, 2, 0, 5]
Env powergym Task 123Bus Algo happo Exp test updates 243/333 episodes, total num timesteps 17496/24000, FPS 0.
Average step reward is -0.4873139560222626.
Some episodes done, average episode reward is -11.695535603724457.

Evaluation average episode reward is -10.709464152358665.

fixed_agent_order_reversed========================== [14, 9, 10, 12, 4, 13, 8, 5, 7, 11, 3, 1, 2, 6, 0]
Env powergym Task 123Bus Algo happo Exp test updates 244/333 episodes, total num timesteps 17568/24000, FPS 0.
Average step reward is -0.5014971494674683.
Some episodes done, average episode reward is -12.035931847672083.

Evaluation average episode reward is -10.756233970214517.

fixed_agent_order_reversed========================== [0, 9, 7, 10, 1, 13, 2, 3, 4, 12, 5, 11, 6, 14, 8]
Env powergym Task 123Bus Algo happo Exp test updates 245/333 episodes, total num timesteps 17640/24000, FPS 0.
Average step reward is -0.4836559295654297.
Some episodes done, average episode reward is -11.607742201880761.

Evaluation average episode reward is -10.891854208883268.

fixed_agent_order_reversed========================== [9, 8, 4, 10, 3, 13, 12, 5, 11, 2, 14, 6, 7, 1, 0]
Env powergym Task 123Bus Algo happo Exp test updates 246/333 episodes, total num timesteps 17712/24000, FPS 0.
Average step reward is -0.47465336322784424.
Some episodes done, average episode reward is -11.391680383311884.

Evaluation average episode reward is -10.931439572835453.

fixed_agent_order_reversed========================== [9, 14, 13, 4, 7, 12, 8, 10, 1, 3, 5, 2, 11, 0, 6]
Env powergym Task 123Bus Algo happo Exp test updates 247/333 episodes, total num timesteps 17784/24000, FPS 0.
Average step reward is -0.5030830502510071.
Some episodes done, average episode reward is -12.073993852445914.

Evaluation average episode reward is -10.881142571052644.

fixed_agent_order_reversed========================== [9, 7, 13, 10, 8, 1, 4, 3, 0, 2, 5, 12, 11, 14, 6]
Env powergym Task 123Bus Algo happo Exp test updates 248/333 episodes, total num timesteps 17856/24000, FPS 0.
Average step reward is -0.4767804741859436.
Some episodes done, average episode reward is -11.442732400620876.

Evaluation average episode reward is -10.844851476960422.

fixed_agent_order_reversed========================== [9, 13, 3, 0, 1, 7, 4, 2, 10, 12, 8, 5, 11, 14, 6]
Env powergym Task 123Bus Algo happo Exp test updates 249/333 episodes, total num timesteps 17928/24000, FPS 0.
Average step reward is -0.49818214774131775.
Some episodes done, average episode reward is -11.956371454509261.

Evaluation average episode reward is -10.844851476960422.

fixed_agent_order_reversed========================== [6, 8, 11, 5, 2, 3, 4, 10, 0, 1, 7, 9, 13, 12, 14]
Env powergym Task 123Bus Algo happo Exp test updates 250/333 episodes, total num timesteps 18000/24000, FPS 0.
Average step reward is -0.475655198097229.
Some episodes done, average episode reward is -11.415723979555366.

Evaluation average episode reward is -10.844851476960422.

fixed_agent_order_reversed========================== [14, 9, 12, 13, 10, 7, 4, 8, 1, 3, 11, 5, 2, 6, 0]
Env powergym Task 123Bus Algo happo Exp test updates 251/333 episodes, total num timesteps 18072/24000, FPS 0.
Average step reward is -0.4927741289138794.
Some episodes done, average episode reward is -11.82657991849716.

Evaluation average episode reward is -10.844851476960422.

fixed_agent_order_reversed========================== [6, 5, 11, 0, 7, 2, 8, 10, 1, 3, 4, 13, 12, 14, 9]
Env powergym Task 123Bus Algo happo Exp test updates 252/333 episodes, total num timesteps 18144/24000, FPS 0.
Average step reward is -0.4755387306213379.
Some episodes done, average episode reward is -11.412929157224335.

Evaluation average episode reward is -10.709464152358665.

fixed_agent_order_reversed========================== [9, 7, 12, 14, 13, 10, 5, 4, 1, 11, 3, 8, 6, 2, 0]
Env powergym Task 123Bus Algo happo Exp test updates 253/333 episodes, total num timesteps 18216/24000, FPS 0.
Average step reward is -0.48577868938446045.
Some episodes done, average episode reward is -11.65868914096707.

Evaluation average episode reward is -10.85228048894969.

fixed_agent_order_reversed========================== [9, 3, 4, 2, 10, 13, 12, 8, 1, 14, 0, 11, 5, 7, 6]
Env powergym Task 123Bus Algo happo Exp test updates 254/333 episodes, total num timesteps 18288/24000, FPS 0.
Average step reward is -0.4751499891281128.
Some episodes done, average episode reward is -11.40360031584889.

Evaluation average episode reward is -10.711941030267335.

fixed_agent_order_reversed========================== [7, 0, 5, 6, 1, 14, 13, 12, 11, 2, 8, 10, 4, 3, 9]
Env powergym Task 123Bus Algo happo Exp test updates 255/333 episodes, total num timesteps 18360/24000, FPS 0.
Average step reward is -0.5088599324226379.
Some episodes done, average episode reward is -12.21263870329417.

Evaluation average episode reward is -10.709464152358665.

fixed_agent_order_reversed========================== [2, 3, 4, 0, 9, 8, 12, 13, 1, 10, 5, 7, 11, 14, 6]
Env powergym Task 123Bus Algo happo Exp test updates 256/333 episodes, total num timesteps 18432/24000, FPS 0.
Average step reward is -0.5201568603515625.
Some episodes done, average episode reward is -12.483764385307168.

Evaluation average episode reward is -10.709464152358665.

fixed_agent_order_reversed========================== [0, 2, 3, 1, 8, 6, 11, 10, 4, 9, 13, 7, 5, 12, 14]
Env powergym Task 123Bus Algo happo Exp test updates 257/333 episodes, total num timesteps 18504/24000, FPS 0.
Average step reward is -0.48452430963516235.
Some episodes done, average episode reward is -11.628583613568848.

Evaluation average episode reward is -10.709464152358665.

fixed_agent_order_reversed========================== [8, 9, 2, 4, 3, 13, 1, 0, 7, 10, 12, 11, 5, 6, 14]
Env powergym Task 123Bus Algo happo Exp test updates 258/333 episodes, total num timesteps 18576/24000, FPS 0.
Average step reward is -0.48924848437309265.
Some episodes done, average episode reward is -11.741964079700118.

Evaluation average episode reward is -10.709464152358665.

fixed_agent_order_reversed========================== [6, 11, 5, 8, 10, 9, 4, 3, 12, 1, 13, 0, 7, 2, 14]
Env powergym Task 123Bus Algo happo Exp test updates 259/333 episodes, total num timesteps 18648/24000, FPS 0.
Average step reward is -0.49302735924720764.
Some episodes done, average episode reward is -11.832656166078593.

Evaluation average episode reward is -10.709464152358665.

fixed_agent_order_reversed========================== [6, 5, 9, 7, 11, 13, 14, 10, 2, 12, 0, 4, 1, 8, 3]
Env powergym Task 123Bus Algo happo Exp test updates 260/333 episodes, total num timesteps 18720/24000, FPS 0.
Average step reward is -0.4947645366191864.
Some episodes done, average episode reward is -11.874348561778069.

Evaluation average episode reward is -10.709464152358665.

fixed_agent_order_reversed========================== [6, 2, 14, 11, 0, 8, 5, 3, 1, 4, 12, 13, 10, 7, 9]
Env powergym Task 123Bus Algo happo Exp test updates 261/333 episodes, total num timesteps 18792/24000, FPS 0.
Average step reward is -0.47168561816215515.
Some episodes done, average episode reward is -11.320454194098412.

Evaluation average episode reward is -10.709464152358665.

fixed_agent_order_reversed========================== [6, 11, 0, 5, 7, 1, 8, 13, 14, 12, 10, 2, 4, 3, 9]
Env powergym Task 123Bus Algo happo Exp test updates 262/333 episodes, total num timesteps 18864/24000, FPS 0.
Average step reward is -0.4859294891357422.
Some episodes done, average episode reward is -11.662306691649745.

Evaluation average episode reward is -10.707989870795922.

fixed_agent_order_reversed========================== [6, 5, 1, 7, 11, 14, 0, 13, 4, 8, 10, 3, 9, 2, 12]
Env powergym Task 123Bus Algo happo Exp test updates 263/333 episodes, total num timesteps 18936/24000, FPS 0.
Average step reward is -0.4849708378314972.
Some episodes done, average episode reward is -11.639298838416826.

Evaluation average episode reward is -10.701489656278588.

fixed_agent_order_reversed========================== [9, 4, 3, 2, 8, 10, 11, 13, 1, 5, 6, 12, 0, 14, 7]
Env powergym Task 123Bus Algo happo Exp test updates 264/333 episodes, total num timesteps 19008/24000, FPS 0.
Average step reward is -0.4897077977657318.
Some episodes done, average episode reward is -11.752987410475441.

Evaluation average episode reward is -10.709464152358665.

fixed_agent_order_reversed========================== [9, 3, 4, 10, 2, 13, 5, 1, 14, 11, 0, 12, 7, 8, 6]
Env powergym Task 123Bus Algo happo Exp test updates 265/333 episodes, total num timesteps 19080/24000, FPS 0.
Average step reward is -0.49687010049819946.
Some episodes done, average episode reward is -11.924882316681469.

Evaluation average episode reward is -10.709464152358665.

fixed_agent_order_reversed========================== [3, 4, 2, 13, 14, 0, 10, 9, 1, 12, 5, 7, 11, 8, 6]
Env powergym Task 123Bus Algo happo Exp test updates 266/333 episodes, total num timesteps 19152/24000, FPS 0.
Average step reward is -0.4828938841819763.
Some episodes done, average episode reward is -11.589453760222307.

Evaluation average episode reward is -10.709464152358665.

fixed_agent_order_reversed========================== [6, 5, 7, 11, 8, 14, 9, 10, 13, 12, 4, 1, 3, 2, 0]
Env powergym Task 123Bus Algo happo Exp test updates 267/333 episodes, total num timesteps 19224/24000, FPS 0.
Average step reward is -0.4824446439743042.
Some episodes done, average episode reward is -11.578671807535654.

Evaluation average episode reward is -10.709464152358665.

fixed_agent_order_reversed========================== [6, 11, 5, 12, 14, 7, 8, 9, 13, 10, 4, 1, 3, 0, 2]
Env powergym Task 123Bus Algo happo Exp test updates 268/333 episodes, total num timesteps 19296/24000, FPS 0.
Average step reward is -0.4786977767944336.
Some episodes done, average episode reward is -11.48874670980374.

Evaluation average episode reward is -10.709464152358665.

fixed_agent_order_reversed========================== [8, 9, 7, 13, 12, 4, 10, 3, 1, 2, 11, 5, 0, 6, 14]
Env powergym Task 123Bus Algo happo Exp test updates 269/333 episodes, total num timesteps 19368/24000, FPS 0.
Average step reward is -0.481188029050827.
Some episodes done, average episode reward is -11.548512356076662.

Evaluation average episode reward is -10.709464152358665.

fixed_agent_order_reversed========================== [9, 10, 7, 12, 5, 8, 4, 14, 13, 1, 3, 2, 11, 0, 6]
Env powergym Task 123Bus Algo happo Exp test updates 270/333 episodes, total num timesteps 19440/24000, FPS 0.
Average step reward is -0.4662929177284241.
Some episodes done, average episode reward is -11.191028891160371.

Evaluation average episode reward is -10.709464152358665.

fixed_agent_order_reversed========================== [2, 3, 0, 9, 4, 10, 13, 1, 12, 5, 14, 7, 11, 8, 6]
Env powergym Task 123Bus Algo happo Exp test updates 271/333 episodes, total num timesteps 19512/24000, FPS 0.
Average step reward is -0.46216249465942383.
Some episodes done, average episode reward is -11.091900197676145.

Evaluation average episode reward is -10.709464152358665.

fixed_agent_order_reversed========================== [14, 7, 5, 12, 13, 1, 0, 2, 4, 3, 11, 6, 10, 8, 9]
Env powergym Task 123Bus Algo happo Exp test updates 272/333 episodes, total num timesteps 19584/24000, FPS 0.
Average step reward is -0.4602987468242645.
Some episodes done, average episode reward is -11.047169727778503.

Evaluation average episode reward is -10.709464152358665.

fixed_agent_order_reversed========================== [14, 7, 12, 13, 1, 5, 4, 11, 10, 9, 6, 3, 0, 2, 8]
Env powergym Task 123Bus Algo happo Exp test updates 273/333 episodes, total num timesteps 19656/24000, FPS 0.
Average step reward is -0.49724280834198.
Some episodes done, average episode reward is -11.933828218751165.

Evaluation average episode reward is -10.709464152358665.

fixed_agent_order_reversed========================== [9, 4, 2, 3, 10, 8, 13, 1, 12, 14, 0, 5, 11, 7, 6]
Env powergym Task 123Bus Algo happo Exp test updates 274/333 episodes, total num timesteps 19728/24000, FPS 0.
Average step reward is -0.4688355028629303.
Some episodes done, average episode reward is -11.252053446449084.

Evaluation average episode reward is -10.709464152358665.

fixed_agent_order_reversed========================== [9, 3, 10, 2, 4, 5, 11, 8, 6, 7, 13, 0, 1, 12, 14]
Env powergym Task 123Bus Algo happo Exp test updates 275/333 episodes, total num timesteps 19800/24000, FPS 0.
Average step reward is -0.4745762050151825.
Some episodes done, average episode reward is -11.389828221659586.

Evaluation average episode reward is -10.709464152358665.

fixed_agent_order_reversed========================== [3, 2, 4, 14, 9, 13, 10, 5, 12, 11, 0, 1, 7, 6, 8]
Env powergym Task 123Bus Algo happo Exp test updates 276/333 episodes, total num timesteps 19872/24000, FPS 0.
Average step reward is -0.46027910709381104.
Some episodes done, average episode reward is -11.046698384607652.

Evaluation average episode reward is -10.709464152358665.

fixed_agent_order_reversed========================== [3, 2, 4, 8, 9, 13, 10, 1, 11, 12, 5, 14, 0, 6, 7]
Env powergym Task 123Bus Algo happo Exp test updates 277/333 episodes, total num timesteps 19944/24000, FPS 0.
Average step reward is -0.48573145270347595.
Some episodes done, average episode reward is -11.657554615296695.

Evaluation average episode reward is -10.709464152358665.

fixed_agent_order_reversed========================== [14, 9, 4, 12, 10, 13, 8, 3, 7, 5, 2, 1, 11, 6, 0]
Env powergym Task 123Bus Algo happo Exp test updates 278/333 episodes, total num timesteps 20016/24000, FPS 0.
Average step reward is -0.4609058201313019.
Some episodes done, average episode reward is -11.061738657720744.

Evaluation average episode reward is -10.709464152358665.

fixed_agent_order_reversed========================== [7, 0, 13, 1, 12, 3, 2, 4, 14, 5, 10, 8, 9, 11, 6]
Env powergym Task 123Bus Algo happo Exp test updates 279/333 episodes, total num timesteps 20088/24000, FPS 0.
Average step reward is -0.46051502227783203.
Some episodes done, average episode reward is -11.05236070108341.

Evaluation average episode reward is -10.709464152358665.

fixed_agent_order_reversed========================== [9, 8, 10, 4, 3, 13, 2, 11, 7, 1, 12, 5, 6, 14, 0]
Env powergym Task 123Bus Algo happo Exp test updates 280/333 episodes, total num timesteps 20160/24000, FPS 0.
Average step reward is -0.45747989416122437.
Some episodes done, average episode reward is -10.979516723544519.

Evaluation average episode reward is -10.709464152358665.

fixed_agent_order_reversed========================== [6, 11, 5, 8, 7, 9, 10, 13, 14, 0, 12, 4, 1, 3, 2]
Env powergym Task 123Bus Algo happo Exp test updates 281/333 episodes, total num timesteps 20232/24000, FPS 0.
Average step reward is -0.45355871319770813.
Some episodes done, average episode reward is -10.885408945839359.

Evaluation average episode reward is -10.709464152358665.

fixed_agent_order_reversed========================== [8, 9, 4, 2, 3, 11, 10, 6, 5, 1, 13, 7, 0, 12, 14]
Env powergym Task 123Bus Algo happo Exp test updates 282/333 episodes, total num timesteps 20304/24000, FPS 0.
Average step reward is -0.46916893124580383.
Some episodes done, average episode reward is -11.260053729637058.

Evaluation average episode reward is -10.709464152358665.

fixed_agent_order_reversed========================== [14, 7, 12, 5, 13, 1, 0, 6, 11, 4, 2, 3, 8, 10, 9]
Env powergym Task 123Bus Algo happo Exp test updates 283/333 episodes, total num timesteps 20376/24000, FPS 0.
Average step reward is -0.45281803607940674.
Some episodes done, average episode reward is -10.867632733254743.

Evaluation average episode reward is -10.709464152358665.

fixed_agent_order_reversed========================== [9, 2, 3, 10, 4, 8, 11, 5, 6, 0, 1, 13, 12, 7, 14]
Env powergym Task 123Bus Algo happo Exp test updates 284/333 episodes, total num timesteps 20448/24000, FPS 0.
Average step reward is -0.48306381702423096.
Some episodes done, average episode reward is -11.593530393080053.

Evaluation average episode reward is -10.709464152358665.

fixed_agent_order_reversed========================== [6, 14, 7, 5, 11, 12, 10, 8, 13, 9, 4, 1, 3, 2, 0]
Env powergym Task 123Bus Algo happo Exp test updates 285/333 episodes, total num timesteps 20520/24000, FPS 0.
Average step reward is -0.46359047293663025.
Some episodes done, average episode reward is -11.126170675793043.

Evaluation average episode reward is -10.709464152358665.

fixed_agent_order_reversed========================== [6, 11, 7, 0, 5, 1, 8, 13, 12, 14, 10, 2, 4, 3, 9]
Env powergym Task 123Bus Algo happo Exp test updates 286/333 episodes, total num timesteps 20592/24000, FPS 0.
Average step reward is -0.46579477190971375.
Some episodes done, average episode reward is -11.17907519111447.

Evaluation average episode reward is -10.709464152358665.

fixed_agent_order_reversed========================== [14, 9, 12, 8, 7, 5, 10, 4, 13, 11, 6, 3, 1, 2, 0]
Env powergym Task 123Bus Algo happo Exp test updates 287/333 episodes, total num timesteps 20664/24000, FPS 0.
Average step reward is -0.470159649848938.
Some episodes done, average episode reward is -11.283832241429103.

Evaluation average episode reward is -10.709464152358665.

fixed_agent_order_reversed========================== [7, 13, 14, 12, 9, 1, 4, 10, 5, 3, 2, 11, 0, 8, 6]
Env powergym Task 123Bus Algo happo Exp test updates 288/333 episodes, total num timesteps 20736/24000, FPS 0.
Average step reward is -0.48004812002182007.
Some episodes done, average episode reward is -11.52115590323974.

Evaluation average episode reward is -10.709464152358665.

fixed_agent_order_reversed========================== [9, 8, 4, 14, 13, 12, 10, 11, 5, 3, 2, 7, 6, 1, 0]
Env powergym Task 123Bus Algo happo Exp test updates 289/333 episodes, total num timesteps 20808/24000, FPS 0.
Average step reward is -0.46415650844573975.
Some episodes done, average episode reward is -11.139756477892199.

Evaluation average episode reward is -10.709464152358665.

fixed_agent_order_reversed========================== [0, 6, 7, 5, 11, 2, 1, 3, 8, 13, 4, 10, 12, 9, 14]
Env powergym Task 123Bus Algo happo Exp test updates 290/333 episodes, total num timesteps 20880/24000, FPS 0.
Average step reward is -0.45980358123779297.
Some episodes done, average episode reward is -11.035285663361883.

Evaluation average episode reward is -10.709464152358665.

fixed_agent_order_reversed========================== [8, 6, 11, 9, 4, 10, 2, 3, 5, 13, 1, 12, 14, 0, 7]
Env powergym Task 123Bus Algo happo Exp test updates 291/333 episodes, total num timesteps 20952/24000, FPS 0.
Average step reward is -0.4618317186832428.
Some episodes done, average episode reward is -11.083960633522572.

Evaluation average episode reward is -10.709464152358665.

fixed_agent_order_reversed========================== [6, 11, 7, 5, 0, 8, 1, 14, 13, 12, 10, 2, 4, 9, 3]
Env powergym Task 123Bus Algo happo Exp test updates 292/333 episodes, total num timesteps 21024/24000, FPS 0.
Average step reward is -0.44916173815727234.
Some episodes done, average episode reward is -10.779882979158936.

Evaluation average episode reward is -10.709464152358665.

fixed_agent_order_reversed========================== [0, 2, 3, 1, 4, 9, 10, 8, 11, 5, 13, 6, 7, 12, 14]
Env powergym Task 123Bus Algo happo Exp test updates 293/333 episodes, total num timesteps 21096/24000, FPS 0.
Average step reward is -0.45675864815711975.
Some episodes done, average episode reward is -10.962207665548696.

Evaluation average episode reward is -10.709464152358665.

fixed_agent_order_reversed========================== [0, 7, 1, 13, 2, 12, 5, 14, 3, 10, 4, 9, 8, 11, 6]
Env powergym Task 123Bus Algo happo Exp test updates 294/333 episodes, total num timesteps 21168/24000, FPS 0.
Average step reward is -0.45398828387260437.
Some episodes done, average episode reward is -10.895718567288121.

Evaluation average episode reward is -10.709464152358665.

fixed_agent_order_reversed========================== [9, 4, 10, 3, 2, 12, 13, 8, 5, 7, 1, 14, 0, 11, 6]
Env powergym Task 123Bus Algo happo Exp test updates 295/333 episodes, total num timesteps 21240/24000, FPS 0.
Average step reward is -0.4603308439254761.
Some episodes done, average episode reward is -11.047940628714526.

Evaluation average episode reward is -10.709464152358665.

fixed_agent_order_reversed========================== [6, 11, 5, 10, 4, 8, 9, 2, 13, 14, 3, 7, 12, 1, 0]
Env powergym Task 123Bus Algo happo Exp test updates 296/333 episodes, total num timesteps 21312/24000, FPS 0.
Average step reward is -0.44807136058807373.
Some episodes done, average episode reward is -10.753712545498459.

Evaluation average episode reward is -10.709464152358665.

fixed_agent_order_reversed========================== [9, 4, 8, 12, 10, 3, 13, 14, 2, 11, 1, 7, 5, 6, 0]
Env powergym Task 123Bus Algo happo Exp test updates 297/333 episodes, total num timesteps 21384/24000, FPS 0.
Average step reward is -0.46410423517227173.
Some episodes done, average episode reward is -11.138502375657422.

Evaluation average episode reward is -10.709464152358665.

fixed_agent_order_reversed========================== [9, 8, 10, 4, 13, 3, 7, 12, 1, 2, 5, 11, 14, 0, 6]
Env powergym Task 123Bus Algo happo Exp test updates 298/333 episodes, total num timesteps 21456/24000, FPS 0.
Average step reward is -0.44974204897880554.
Some episodes done, average episode reward is -10.793809111629242.

Evaluation average episode reward is -10.709464152358665.

fixed_agent_order_reversed========================== [0, 2, 3, 4, 1, 13, 12, 9, 10, 7, 5, 8, 14, 11, 6]
Env powergym Task 123Bus Algo happo Exp test updates 299/333 episodes, total num timesteps 21528/24000, FPS 0.
Average step reward is -0.4532093405723572.
Some episodes done, average episode reward is -10.877024869751708.

Evaluation average episode reward is -10.709464152358665.

fixed_agent_order_reversed========================== [6, 14, 11, 7, 5, 9, 8, 13, 12, 10, 4, 1, 3, 2, 0]
Env powergym Task 123Bus Algo happo Exp test updates 300/333 episodes, total num timesteps 21600/24000, FPS 0.
Average step reward is -0.4520590007305145.
Some episodes done, average episode reward is -10.84941589114038.

Evaluation average episode reward is -10.709464152358665.

fixed_agent_order_reversed========================== [9, 4, 10, 3, 2, 8, 14, 11, 5, 12, 13, 6, 1, 7, 0]
Env powergym Task 123Bus Algo happo Exp test updates 301/333 episodes, total num timesteps 21672/24000, FPS 0.
Average step reward is -0.45212438702583313.
Some episodes done, average episode reward is -10.850984541482205.

Evaluation average episode reward is -10.709464152358665.

fixed_agent_order_reversed========================== [7, 14, 5, 12, 6, 13, 0, 1, 11, 10, 4, 2, 8, 3, 9]
Env powergym Task 123Bus Algo happo Exp test updates 302/333 episodes, total num timesteps 21744/24000, FPS 0.
Average step reward is -0.45348235964775085.
Some episodes done, average episode reward is -10.883577186579188.

Evaluation average episode reward is -10.709464152358665.

fixed_agent_order_reversed========================== [14, 2, 5, 1, 4, 3, 7, 13, 0, 12, 10, 11, 6, 8, 9]
Env powergym Task 123Bus Algo happo Exp test updates 303/333 episodes, total num timesteps 21816/24000, FPS 0.
Average step reward is -0.4533449411392212.
Some episodes done, average episode reward is -10.880278121057366.

Evaluation average episode reward is -10.703948806593255.

fixed_agent_order_reversed========================== [9, 10, 7, 13, 4, 5, 12, 3, 2, 1, 11, 14, 0, 8, 6]
Env powergym Task 123Bus Algo happo Exp test updates 304/333 episodes, total num timesteps 21888/24000, FPS 0.
Average step reward is -0.4549142122268677.
Some episodes done, average episode reward is -10.917940753599977.

Evaluation average episode reward is -10.703948806593255.

fixed_agent_order_reversed========================== [14, 3, 9, 10, 13, 4, 2, 12, 0, 1, 5, 7, 11, 6, 8]
Env powergym Task 123Bus Algo happo Exp test updates 305/333 episodes, total num timesteps 21960/24000, FPS 0.
Average step reward is -0.46839258074760437.
Some episodes done, average episode reward is -11.241421785602052.

Evaluation average episode reward is -10.703948806593255.

fixed_agent_order_reversed========================== [8, 9, 10, 2, 3, 4, 6, 11, 5, 0, 13, 1, 12, 7, 14]
Env powergym Task 123Bus Algo happo Exp test updates 306/333 episodes, total num timesteps 22032/24000, FPS 0.
Average step reward is -0.44958576560020447.
Some episodes done, average episode reward is -10.79005798231465.

Evaluation average episode reward is -10.701342274767974.

fixed_agent_order_reversed========================== [14, 9, 4, 12, 13, 3, 2, 10, 1, 8, 7, 5, 0, 11, 6]
Env powergym Task 123Bus Algo happo Exp test updates 307/333 episodes, total num timesteps 22104/24000, FPS 0.
Average step reward is -0.45262908935546875.
Some episodes done, average episode reward is -10.863097713498535.

Evaluation average episode reward is -10.701342274767974.

fixed_agent_order_reversed========================== [9, 10, 4, 3, 2, 11, 6, 8, 5, 14, 12, 13, 1, 0, 7]
Env powergym Task 123Bus Algo happo Exp test updates 308/333 episodes, total num timesteps 22176/24000, FPS 0.
Average step reward is -0.44947201013565063.
Some episodes done, average episode reward is -10.78732850299908.

Evaluation average episode reward is -10.701342274767974.

fixed_agent_order_reversed========================== [6, 5, 7, 14, 11, 8, 12, 10, 13, 4, 9, 1, 2, 3, 0]
Env powergym Task 123Bus Algo happo Exp test updates 309/333 episodes, total num timesteps 22248/24000, FPS 0.
Average step reward is -0.450846791267395.
Some episodes done, average episode reward is -10.820323531401584.

Evaluation average episode reward is -10.701342274767974.

fixed_agent_order_reversed========================== [4, 9, 8, 10, 3, 2, 11, 5, 13, 12, 6, 14, 1, 7, 0]
Env powergym Task 123Bus Algo happo Exp test updates 310/333 episodes, total num timesteps 22320/24000, FPS 0.
Average step reward is -0.4532429277896881.
Some episodes done, average episode reward is -10.877830382894851.

Evaluation average episode reward is -10.701342274767974.

fixed_agent_order_reversed========================== [14, 4, 9, 10, 3, 2, 11, 12, 13, 5, 6, 1, 8, 7, 0]
Env powergym Task 123Bus Algo happo Exp test updates 311/333 episodes, total num timesteps 22392/24000, FPS 0.
Average step reward is -0.4569501280784607.
Some episodes done, average episode reward is -10.966802918616287.

Evaluation average episode reward is -10.701342274767974.

fixed_agent_order_reversed========================== [10, 9, 7, 8, 13, 4, 12, 11, 2, 6, 5, 1, 3, 14, 0]
Env powergym Task 123Bus Algo happo Exp test updates 312/333 episodes, total num timesteps 22464/24000, FPS 0.
Average step reward is -0.4616737961769104.
Some episodes done, average episode reward is -11.080171119680237.

Evaluation average episode reward is -10.701342274767974.

fixed_agent_order_reversed========================== [9, 4, 8, 3, 10, 2, 13, 14, 12, 1, 5, 11, 7, 0, 6]
Env powergym Task 123Bus Algo happo Exp test updates 313/333 episodes, total num timesteps 22536/24000, FPS 0.
Average step reward is -0.4579322636127472.
Some episodes done, average episode reward is -10.990374136257843.

Evaluation average episode reward is -10.701342274767974.

fixed_agent_order_reversed========================== [6, 7, 5, 0, 11, 12, 14, 13, 10, 1, 2, 3, 9, 4, 8]
Env powergym Task 123Bus Algo happo Exp test updates 314/333 episodes, total num timesteps 22608/24000, FPS 0.
Average step reward is -0.45918115973472595.
Some episodes done, average episode reward is -11.0203470537167.

Evaluation average episode reward is -10.701342274767974.

fixed_agent_order_reversed========================== [9, 10, 8, 4, 3, 2, 11, 13, 5, 6, 1, 12, 14, 7, 0]
Env powergym Task 123Bus Algo happo Exp test updates 315/333 episodes, total num timesteps 22680/24000, FPS 0.
Average step reward is -0.46487775444984436.
Some episodes done, average episode reward is -11.157066195072803.

Evaluation average episode reward is -10.701342274767974.

fixed_agent_order_reversed========================== [7, 5, 6, 14, 11, 12, 13, 10, 0, 1, 4, 8, 2, 9, 3]
Env powergym Task 123Bus Algo happo Exp test updates 316/333 episodes, total num timesteps 22752/24000, FPS 0.
Average step reward is -0.45571571588516235.
Some episodes done, average episode reward is -10.937177572391748.

Evaluation average episode reward is -10.701342274767974.

fixed_agent_order_reversed========================== [9, 4, 3, 10, 13, 2, 8, 1, 12, 7, 0, 11, 5, 14, 6]
Env powergym Task 123Bus Algo happo Exp test updates 317/333 episodes, total num timesteps 22824/24000, FPS 0.
Average step reward is -0.47755560278892517.
Some episodes done, average episode reward is -11.461334925119617.

Evaluation average episode reward is -10.701342274767974.

fixed_agent_order_reversed========================== [0, 2, 3, 1, 9, 4, 8, 10, 13, 5, 11, 7, 12, 6, 14]
There are 134 edges and 131 unique edges. Overlapping transformer edges
Closing the environment
There are 134 edges and 131 unique edges. Overlapping transformer edges
Closing the environment
There are 134 edges and 131 unique edges. Overlapping transformer edges
Closing the environment
There are 134 edges and 131 unique edges. Overlapping transformer edges
Closing the environment
There are 134 edges and 131 unique edges. Overlapping transformer edges
Closing the environment
There are 134 edges and 131 unique edges. Overlapping transformer edges
Closing the environment
/home/yushixuan/anaconda3/envs/harl/lib/python3.8/site-packages/numpy/core/shape_base.py:420: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arrays = [asanyarray(arr) for arr in arrays]
Env powergym Task 123Bus Algo happo Exp test updates 318/333 episodes, total num timesteps 22896/24000, FPS 0.
Average step reward is -0.461322158575058.
Some episodes done, average episode reward is -11.07173136240575.

Evaluation average episode reward is -10.701342274767974.

fixed_agent_order_reversed========================== [4, 9, 3, 10, 2, 8, 14, 13, 12, 1, 5, 11, 7, 0, 6]
Env powergym Task 123Bus Algo happo Exp test updates 319/333 episodes, total num timesteps 22968/24000, FPS 0.
Average step reward is -0.5067439675331116.
Some episodes done, average episode reward is -12.161855061524683.

Evaluation average episode reward is -10.701342274767974.

fixed_agent_order_reversed========================== [8, 6, 11, 9, 14, 7, 13, 12, 4, 5, 10, 3, 1, 2, 0]
Env powergym Task 123Bus Algo happo Exp test updates 320/333 episodes, total num timesteps 23040/24000, FPS 0.
Average step reward is -0.47110897302627563.
Some episodes done, average episode reward is -11.306615988180125.

Evaluation average episode reward is -10.701342274767974.

fixed_agent_order_reversed========================== [9, 10, 4, 3, 14, 12, 13, 8, 7, 2, 5, 11, 1, 6, 0]
Env powergym Task 123Bus Algo happo Exp test updates 321/333 episodes, total num timesteps 23112/24000, FPS 0.
Average step reward is -0.46093830466270447.
Some episodes done, average episode reward is -11.062518292011793.

Evaluation average episode reward is -10.701342274767974.

fixed_agent_order_reversed========================== [9, 4, 3, 13, 2, 0, 10, 1, 12, 7, 8, 5, 11, 14, 6]
Env powergym Task 123Bus Algo happo Exp test updates 322/333 episodes, total num timesteps 23184/24000, FPS 0.
Average step reward is -0.4626396596431732.
Some episodes done, average episode reward is -11.103350915815305.

Evaluation average episode reward is -10.701342274767974.

fixed_agent_order_reversed========================== [8, 6, 11, 5, 4, 10, 2, 9, 13, 7, 3, 1, 0, 12, 14]
Env powergym Task 123Bus Algo happo Exp test updates 323/333 episodes, total num timesteps 23256/24000, FPS 0.
Average step reward is -0.46295368671417236.
Some episodes done, average episode reward is -11.11088810553896.

Evaluation average episode reward is -10.701342274767974.

fixed_agent_order_reversed========================== [9, 8, 4, 10, 3, 2, 13, 11, 5, 12, 14, 1, 6, 7, 0]
Env powergym Task 123Bus Algo happo Exp test updates 324/333 episodes, total num timesteps 23328/24000, FPS 0.
Average step reward is -0.4590491056442261.
Some episodes done, average episode reward is -11.017179780224838.

Evaluation average episode reward is -10.701342274767974.

fixed_agent_order_reversed========================== [6, 7, 0, 5, 14, 11, 1, 12, 13, 10, 2, 8, 3, 4, 9]
Env powergym Task 123Bus Algo happo Exp test updates 325/333 episodes, total num timesteps 23400/24000, FPS 0.
Average step reward is -0.48091617226600647.
Some episodes done, average episode reward is -11.541988715650433.

Evaluation average episode reward is -10.701342274767974.

fixed_agent_order_reversed========================== [14, 10, 9, 6, 8, 11, 4, 5, 12, 13, 3, 2, 1, 7, 0]
Env powergym Task 123Bus Algo happo Exp test updates 326/333 episodes, total num timesteps 23472/24000, FPS 0.
Average step reward is -0.46386972069740295.
Some episodes done, average episode reward is -11.132874149828199.

Evaluation average episode reward is -10.701342274767974.

fixed_agent_order_reversed========================== [6, 11, 13, 14, 5, 10, 7, 12, 8, 0, 1, 9, 3, 4, 2]
Env powergym Task 123Bus Algo happo Exp test updates 327/333 episodes, total num timesteps 23544/24000, FPS 0.
Average step reward is -0.47564247250556946.
Some episodes done, average episode reward is -11.415419484668007.

Evaluation average episode reward is -10.701342274767974.

fixed_agent_order_reversed========================== [14, 7, 13, 12, 5, 4, 10, 1, 11, 9, 6, 3, 2, 8, 0]
Env powergym Task 123Bus Algo happo Exp test updates 328/333 episodes, total num timesteps 23616/24000, FPS 0.
Average step reward is -0.4602779746055603.
Some episodes done, average episode reward is -11.046671760025069.

Evaluation average episode reward is -10.701342274767974.

fixed_agent_order_reversed========================== [9, 4, 10, 3, 8, 2, 12, 13, 14, 5, 1, 11, 7, 0, 6]
Env powergym Task 123Bus Algo happo Exp test updates 329/333 episodes, total num timesteps 23688/24000, FPS 0.
Average step reward is -0.46360480785369873.
Some episodes done, average episode reward is -11.126515337239496.

Evaluation average episode reward is -10.701342274767974.

fixed_agent_order_reversed========================== [13, 7, 14, 9, 12, 10, 0, 4, 3, 1, 2, 5, 11, 8, 6]
Env powergym Task 123Bus Algo happo Exp test updates 330/333 episodes, total num timesteps 23760/24000, FPS 0.
Average step reward is -0.4654167890548706.
Some episodes done, average episode reward is -11.170003608181498.

Evaluation average episode reward is -10.701342274767974.

fixed_agent_order_reversed========================== [6, 7, 5, 11, 12, 8, 0, 13, 1, 9, 4, 10, 3, 2, 14]
Env powergym Task 123Bus Algo happo Exp test updates 331/333 episodes, total num timesteps 23832/24000, FPS 0.
Average step reward is -0.46515369415283203.
Some episodes done, average episode reward is -11.163687656509628.

Evaluation average episode reward is -10.701342274767974.

fixed_agent_order_reversed========================== [9, 3, 2, 4, 8, 5, 10, 11, 6, 13, 0, 12, 1, 7, 14]
Env powergym Task 123Bus Algo happo Exp test updates 332/333 episodes, total num timesteps 23904/24000, FPS 0.
Average step reward is -0.4690077006816864.
Some episodes done, average episode reward is -11.256184579250567.

Evaluation average episode reward is -10.703948806593255.

fixed_agent_order_reversed========================== [14, 12, 7, 13, 5, 4, 1, 9, 10, 3, 2, 8, 11, 0, 6]
Env powergym Task 123Bus Algo happo Exp test updates 333/333 episodes, total num timesteps 23976/24000, FPS 0.
Average step reward is -0.4534856379032135.
Some episodes done, average episode reward is -10.883655406998246.

Evaluation average episode reward is -10.703948806593255.

choose to use gpu...
3 {'env_name': '123Bus', 'seed': 123456, 'num_steps': 1000, 'num_workers': 'None', 'use_plot': False, 'do_testing': False, 'mode': 'single', 'useS': True, 'big2small': False}
make_train_env= 3
There are 134 edges and 131 unique edges. Overlapping transformer edges
There are 134 edges and 131 unique edges. Overlapping transformer edges
There are 134 edges and 131 unique edges. Overlapping transformer edges
make_eval_env= 3
There are 134 edges and 131 unique edges. Overlapping transformer edges
There are 134 edges and 131 unique edges. Overlapping transformer edges
share_observation_space:  [Box([ 0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.   0.
  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -1.   0.  -1.   0.
 -1.   0.  -1. ], [ 1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.   1.
  1.   1.  33.  33.  33.  33.  33.  33.  33.   1.   1.   1.   1.   1.
  1.   1.   1. ], (297,), float32), Box([ 0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.   0.
  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -1.   0.  -1.   0.
 -1.   0.  -1. ], [ 1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.   1.
  1.   1.  33.  33.  33.  33.  33.  33.  33.   1.   1.   1.   1.   1.
  1.   1.   1. ], (297,), float32), Box([ 0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.   0.
  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -1.   0.  -1.   0.
 -1.   0.  -1. ], [ 1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.   1.
  1.   1.  33.  33.  33.  33.  33.  33.  33.   1.   1.   1.   1.   1.
  1.   1.   1. ], (297,), float32), Box([ 0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.   0.
  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -1.   0.  -1.   0.
 -1.   0.  -1. ], [ 1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.   1.
  1.   1.  33.  33.  33.  33.  33.  33.  33.   1.   1.   1.   1.   1.
  1.   1.   1. ], (297,), float32), Box([ 0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.   0.
  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -1.   0.  -1.   0.
 -1.   0.  -1. ], [ 1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.   1.
  1.   1.  33.  33.  33.  33.  33.  33.  33.   1.   1.   1.   1.   1.
  1.   1.   1. ], (297,), float32), Box([ 0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.   0.
  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -1.   0.  -1.   0.
 -1.   0.  -1. ], [ 1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.   1.
  1.   1.  33.  33.  33.  33.  33.  33.  33.   1.   1.   1.   1.   1.
  1.   1.   1. ], (297,), float32), Box([ 0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.   0.
  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -1.   0.  -1.   0.
 -1.   0.  -1. ], [ 1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.   1.
  1.   1.  33.  33.  33.  33.  33.  33.  33.   1.   1.   1.   1.   1.
  1.   1.   1. ], (297,), float32), Box([ 0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.   0.
  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -1.   0.  -1.   0.
 -1.   0.  -1. ], [ 1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.   1.
  1.   1.  33.  33.  33.  33.  33.  33.  33.   1.   1.   1.   1.   1.
  1.   1.   1. ], (297,), float32), Box([ 0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.   0.
  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -1.   0.  -1.   0.
 -1.   0.  -1. ], [ 1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.   1.
  1.   1.  33.  33.  33.  33.  33.  33.  33.   1.   1.   1.   1.   1.
  1.   1.   1. ], (297,), float32), Box([ 0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.   0.
  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -1.   0.  -1.   0.
 -1.   0.  -1. ], [ 1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.   1.
  1.   1.  33.  33.  33.  33.  33.  33.  33.   1.   1.   1.   1.   1.
  1.   1.   1. ], (297,), float32), Box([ 0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.   0.
  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -1.   0.  -1.   0.
 -1.   0.  -1. ], [ 1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.   1.
  1.   1.  33.  33.  33.  33.  33.  33.  33.   1.   1.   1.   1.   1.
  1.   1.   1. ], (297,), float32), Box([ 0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.   0.
  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -1.   0.  -1.   0.
 -1.   0.  -1. ], [ 1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.   1.
  1.   1.  33.  33.  33.  33.  33.  33.  33.   1.   1.   1.   1.   1.
  1.   1.   1. ], (297,), float32), Box([ 0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.   0.
  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -1.   0.  -1.   0.
 -1.   0.  -1. ], [ 1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.   1.
  1.   1.  33.  33.  33.  33.  33.  33.  33.   1.   1.   1.   1.   1.
  1.   1.   1. ], (297,), float32), Box([ 0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.   0.
  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -1.   0.  -1.   0.
 -1.   0.  -1. ], [ 1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.   1.
  1.   1.  33.  33.  33.  33.  33.  33.  33.   1.   1.   1.   1.   1.
  1.   1.   1. ], (297,), float32), Box([ 0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.   0.
  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -1.   0.  -1.   0.
 -1.   0.  -1. ], [ 1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.   1.
  1.   1.  33.  33.  33.  33.  33.  33.  33.   1.   1.   1.   1.   1.
  1.   1.   1. ], (297,), float32)]
observation_space:  [Box([ 0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.   0.
  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -1.   0.  -1.   0.
 -1.   0.  -1. ], [ 1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.   1.
  1.   1.  33.  33.  33.  33.  33.  33.  33.   1.   1.   1.   1.   1.
  1.   1.   1. ], (297,), float32), Box([ 0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.   0.
  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -1.   0.  -1.   0.
 -1.   0.  -1. ], [ 1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.   1.
  1.   1.  33.  33.  33.  33.  33.  33.  33.   1.   1.   1.   1.   1.
  1.   1.   1. ], (297,), float32), Box([ 0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.   0.
  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -1.   0.  -1.   0.
 -1.   0.  -1. ], [ 1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.   1.
  1.   1.  33.  33.  33.  33.  33.  33.  33.   1.   1.   1.   1.   1.
  1.   1.   1. ], (297,), float32), Box([ 0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.   0.
  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -1.   0.  -1.   0.
 -1.   0.  -1. ], [ 1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.   1.
  1.   1.  33.  33.  33.  33.  33.  33.  33.   1.   1.   1.   1.   1.
  1.   1.   1. ], (297,), float32), Box([ 0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.   0.
  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -1.   0.  -1.   0.
 -1.   0.  -1. ], [ 1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.   1.
  1.   1.  33.  33.  33.  33.  33.  33.  33.   1.   1.   1.   1.   1.
  1.   1.   1. ], (297,), float32), Box([ 0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.   0.
  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -1.   0.  -1.   0.
 -1.   0.  -1. ], [ 1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.   1.
  1.   1.  33.  33.  33.  33.  33.  33.  33.   1.   1.   1.   1.   1.
  1.   1.   1. ], (297,), float32), Box([ 0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.   0.
  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -1.   0.  -1.   0.
 -1.   0.  -1. ], [ 1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.   1.
  1.   1.  33.  33.  33.  33.  33.  33.  33.   1.   1.   1.   1.   1.
  1.   1.   1. ], (297,), float32), Box([ 0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.   0.
  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -1.   0.  -1.   0.
 -1.   0.  -1. ], [ 1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.   1.
  1.   1.  33.  33.  33.  33.  33.  33.  33.   1.   1.   1.   1.   1.
  1.   1.   1. ], (297,), float32), Box([ 0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.   0.
  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -1.   0.  -1.   0.
 -1.   0.  -1. ], [ 1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.   1.
  1.   1.  33.  33.  33.  33.  33.  33.  33.   1.   1.   1.   1.   1.
  1.   1.   1. ], (297,), float32), Box([ 0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.   0.
  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -1.   0.  -1.   0.
 -1.   0.  -1. ], [ 1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.   1.
  1.   1.  33.  33.  33.  33.  33.  33.  33.   1.   1.   1.   1.   1.
  1.   1.   1. ], (297,), float32), Box([ 0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.   0.
  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -1.   0.  -1.   0.
 -1.   0.  -1. ], [ 1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.   1.
  1.   1.  33.  33.  33.  33.  33.  33.  33.   1.   1.   1.   1.   1.
  1.   1.   1. ], (297,), float32), Box([ 0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.   0.
  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -1.   0.  -1.   0.
 -1.   0.  -1. ], [ 1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.   1.
  1.   1.  33.  33.  33.  33.  33.  33.  33.   1.   1.   1.   1.   1.
  1.   1.   1. ], (297,), float32), Box([ 0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.   0.
  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -1.   0.  -1.   0.
 -1.   0.  -1. ], [ 1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.   1.
  1.   1.  33.  33.  33.  33.  33.  33.  33.   1.   1.   1.   1.   1.
  1.   1.   1. ], (297,), float32), Box([ 0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.   0.
  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -1.   0.  -1.   0.
 -1.   0.  -1. ], [ 1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.   1.
  1.   1.  33.  33.  33.  33.  33.  33.  33.   1.   1.   1.   1.   1.
  1.   1.   1. ], (297,), float32), Box([ 0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8
  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.8  0.   0.
  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  -1.   0.  -1.   0.
 -1.   0.  -1. ], [ 1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2
  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.2  1.   1.
  1.   1.  33.  33.  33.  33.  33.  33.  33.   1.   1.   1.   1.   1.
  1.   1.   1. ], (297,), float32)]
action_space:  [Discrete(2), Discrete(2), Discrete(2), Discrete(2), Discrete(33), Discrete(33), Discrete(33), Discrete(33), Discrete(33), Discrete(33), Discrete(33), Discrete(33), Discrete(33), Discrete(33), Discrete(33)]
There are 134 edges and 131 unique edges. Overlapping transformer edges
self.state_type= EP
start running
/home/yushixuan/anaconda3/envs/harl_cupy/lib/python3.9/site-packages/numpy/core/shape_base.py:420: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  arrays = [asanyarray(arr) for arr in arrays]
fixed_agent_order_reversed========================== [7, 6, 0, 9, 13, 8, 1, 12, 11, 5, 3, 14, 4, 10, 2]
Env powergym Task 123Bus Algo happo Exp test updates 1/333 episodes, total num timesteps 72/24000, FPS 3.
Average step reward is -7.179079532623291.
Some episodes done, average episode reward is -172.2979106837668.

Evaluation average episode reward is -121.09424459294212.

fixed_agent_order_reversed========================== [4, 14, 2, 12, 13, 10, 1, 9, 3, 5, 8, 7, 0, 11, 6]
Env powergym Task 123Bus Algo happo Exp test updates 2/333 episodes, total num timesteps 144/24000, FPS 1.
Average step reward is -6.862171649932861.
Some episodes done, average episode reward is -164.69212190351962.

Evaluation average episode reward is -75.2754647750807.

fixed_agent_order_reversed========================== [0, 2, 3, 6, 1, 7, 5, 11, 8, 4, 10, 9, 13, 12, 14]
Env powergym Task 123Bus Algo happo Exp test updates 3/333 episodes, total num timesteps 216/24000, FPS 1.
Average step reward is -7.421372413635254.
Some episodes done, average episode reward is -178.11294331884338.

Evaluation average episode reward is -108.58003041567069.

fixed_agent_order_reversed========================== [14, 12, 13, 4, 3, 2, 9, 10, 7, 5, 11, 0, 1, 8, 6]
Env powergym Task 123Bus Algo happo Exp test updates 4/333 episodes, total num timesteps 288/24000, FPS 1.
Average step reward is -6.950217247009277.
Some episodes done, average episode reward is -166.80522125150378.

Evaluation average episode reward is -121.15916330745587.

fixed_agent_order_reversed========================== [7, 0, 6, 1, 14, 5, 13, 11, 12, 10, 2, 9, 4, 3, 8]
Env powergym Task 123Bus Algo happo Exp test updates 5/333 episodes, total num timesteps 360/24000, FPS 1.
Average step reward is -7.162516117095947.
Some episodes done, average episode reward is -171.9004021648993.

Evaluation average episode reward is -112.26458489931564.

fixed_agent_order_reversed========================== [5, 7, 10, 12, 6, 14, 1, 2, 13, 0, 11, 4, 9, 3, 8]
Env powergym Task 123Bus Algo happo Exp test updates 6/333 episodes, total num timesteps 432/24000, FPS 1.
Average step reward is -6.887625217437744.
Some episodes done, average episode reward is -165.3030084416209.

Evaluation average episode reward is -92.51873888547273.

fixed_agent_order_reversed========================== [14, 13, 4, 8, 6, 11, 12, 2, 3, 1, 10, 5, 7, 0, 9]
Env powergym Task 123Bus Algo happo Exp test updates 7/333 episodes, total num timesteps 504/24000, FPS 1.
Average step reward is -6.590215682983398.
Some episodes done, average episode reward is -158.16517591735786.

Evaluation average episode reward is -51.10922714120314.

fixed_agent_order_reversed========================== [3, 10, 2, 13, 0, 4, 14, 8, 11, 5, 6, 9, 1, 7, 12]
Env powergym Task 123Bus Algo happo Exp test updates 8/333 episodes, total num timesteps 576/24000, FPS 1.
Average step reward is -7.225923538208008.
Some episodes done, average episode reward is -173.42216482016863.

Evaluation average episode reward is -84.23617359606644.

fixed_agent_order_reversed========================== [6, 11, 8, 0, 2, 5, 3, 9, 1, 4, 10, 13, 7, 12, 14]
Env powergym Task 123Bus Algo happo Exp test updates 9/333 episodes, total num timesteps 648/24000, FPS 1.
Average step reward is -6.571106433868408.
Some episodes done, average episode reward is -157.70656202896248.

Evaluation average episode reward is -68.53914176168509.

fixed_agent_order_reversed========================== [9, 3, 10, 4, 5, 2, 8, 11, 7, 1, 0, 13, 6, 12, 14]
Env powergym Task 123Bus Algo happo Exp test updates 10/333 episodes, total num timesteps 720/24000, FPS 1.
Average step reward is -6.3958420753479.
Some episodes done, average episode reward is -153.50020913232433.

Evaluation average episode reward is -66.67940697566137.

fixed_agent_order_reversed========================== [7, 14, 9, 12, 5, 13, 6, 8, 11, 1, 4, 10, 3, 0, 2]
Env powergym Task 123Bus Algo happo Exp test updates 11/333 episodes, total num timesteps 792/24000, FPS 1.
Average step reward is -6.674184322357178.
Some episodes done, average episode reward is -160.18041878641884.

Evaluation average episode reward is -152.45467882780258.

fixed_agent_order_reversed========================== [5, 10, 12, 7, 4, 14, 13, 9, 11, 6, 8, 3, 2, 1, 0]
Env powergym Task 123Bus Algo happo Exp test updates 12/333 episodes, total num timesteps 864/24000, FPS 1.
Average step reward is -5.855714797973633.
Some episodes done, average episode reward is -140.53715266120219.

Evaluation average episode reward is -91.47640435482442.

fixed_agent_order_reversed========================== [6, 11, 7, 8, 5, 4, 3, 1, 10, 12, 2, 0, 9, 13, 14]
Env powergym Task 123Bus Algo happo Exp test updates 13/333 episodes, total num timesteps 936/24000, FPS 1.
Average step reward is -6.7468085289001465.
Some episodes done, average episode reward is -161.92339906109552.

Evaluation average episode reward is -62.13786675717912.

fixed_agent_order_reversed========================== [9, 7, 6, 13, 2, 5, 11, 12, 0, 4, 1, 3, 10, 8, 14]
Env powergym Task 123Bus Algo happo Exp test updates 14/333 episodes, total num timesteps 1008/24000, FPS 1.
Average step reward is -5.887174606323242.
Some episodes done, average episode reward is -141.29218972922817.

Evaluation average episode reward is -83.27475302324169.

fixed_agent_order_reversed========================== [6, 7, 12, 5, 0, 1, 11, 13, 14, 2, 4, 3, 10, 9, 8]
Env powergym Task 123Bus Algo happo Exp test updates 15/333 episodes, total num timesteps 1080/24000, FPS 1.
Average step reward is -5.67264986038208.
Some episodes done, average episode reward is -136.14359452837957.

Evaluation average episode reward is -79.71821493867719.

fixed_agent_order_reversed========================== [14, 12, 5, 4, 2, 10, 13, 11, 8, 3, 7, 1, 6, 9, 0]
Env powergym Task 123Bus Algo happo Exp test updates 16/333 episodes, total num timesteps 1152/24000, FPS 1.
Average step reward is -5.22043514251709.
Some episodes done, average episode reward is -125.29045314971933.

Evaluation average episode reward is -89.9552640694679.

fixed_agent_order_reversed========================== [12, 2, 7, 1, 13, 0, 4, 8, 3, 11, 5, 10, 9, 6, 14]
Env powergym Task 123Bus Algo happo Exp test updates 17/333 episodes, total num timesteps 1224/24000, FPS 1.
Average step reward is -6.11722993850708.
Some episodes done, average episode reward is -146.81352633067823.

Evaluation average episode reward is -51.47723424187109.

fixed_agent_order_reversed========================== [14, 13, 0, 9, 7, 12, 2, 1, 10, 3, 5, 4, 8, 11, 6]
Env powergym Task 123Bus Algo happo Exp test updates 18/333 episodes, total num timesteps 1296/24000, FPS 1.
Average step reward is -5.327512741088867.
Some episodes done, average episode reward is -127.86031160305602.

Evaluation average episode reward is -37.39079952036091.

fixed_agent_order_reversed========================== [5, 0, 6, 7, 11, 10, 3, 1, 2, 8, 9, 13, 4, 12, 14]
Env powergym Task 123Bus Algo happo Exp test updates 19/333 episodes, total num timesteps 1368/24000, FPS 1.
Average step reward is -5.262969970703125.
Some episodes done, average episode reward is -126.31128134867906.

Evaluation average episode reward is -73.77287745117054.

fixed_agent_order_reversed========================== [12, 14, 7, 10, 5, 13, 4, 11, 1, 6, 9, 3, 8, 2, 0]
Env powergym Task 123Bus Algo happo Exp test updates 20/333 episodes, total num timesteps 1440/24000, FPS 1.
Average step reward is -4.644067764282227.
Some episodes done, average episode reward is -111.45762434299498.

Evaluation average episode reward is -49.3374794704581.

fixed_agent_order_reversed========================== [2, 6, 11, 0, 1, 3, 9, 8, 4, 10, 13, 5, 7, 12, 14]
Env powergym Task 123Bus Algo happo Exp test updates 21/333 episodes, total num timesteps 1512/24000, FPS 1.
Average step reward is -3.967543363571167.
Some episodes done, average episode reward is -95.22103590649408.

Evaluation average episode reward is -30.507700114074755.

fixed_agent_order_reversed========================== [10, 9, 14, 4, 13, 12, 11, 3, 6, 2, 1, 7, 5, 0, 8]
Env powergym Task 123Bus Algo happo Exp test updates 22/333 episodes, total num timesteps 1584/24000, FPS 1.
Average step reward is -4.352775573730469.
Some episodes done, average episode reward is -104.4666175773592.

Evaluation average episode reward is -30.301093129751848.

fixed_agent_order_reversed========================== [6, 5, 7, 11, 8, 4, 10, 3, 1, 12, 9, 13, 2, 0, 14]
Env powergym Task 123Bus Algo happo Exp test updates 23/333 episodes, total num timesteps 1656/24000, FPS 1.
Average step reward is -3.904895782470703.
Some episodes done, average episode reward is -93.71750070411348.

Evaluation average episode reward is -45.00070064476091.

fixed_agent_order_reversed========================== [9, 6, 11, 3, 7, 5, 10, 8, 4, 13, 12, 14, 1, 0, 2]
Env powergym Task 123Bus Algo happo Exp test updates 24/333 episodes, total num timesteps 1728/24000, FPS 1.
Average step reward is -3.8794572353363037.
Some episodes done, average episode reward is -93.10697040660101.

Evaluation average episode reward is -52.761227499304674.

fixed_agent_order_reversed========================== [10, 9, 4, 3, 5, 8, 14, 7, 11, 12, 13, 1, 2, 6, 0]
Env powergym Task 123Bus Algo happo Exp test updates 25/333 episodes, total num timesteps 1800/24000, FPS 1.
Average step reward is -3.922989845275879.
Some episodes done, average episode reward is -94.15175866566756.

Evaluation average episode reward is -63.82495353245321.

fixed_agent_order_reversed========================== [9, 4, 8, 12, 2, 3, 10, 14, 11, 13, 1, 5, 6, 0, 7]
Env powergym Task 123Bus Algo happo Exp test updates 26/333 episodes, total num timesteps 1872/24000, FPS 1.
Average step reward is -3.513932228088379.
Some episodes done, average episode reward is -84.33437000093473.

Evaluation average episode reward is -73.97947892288842.

fixed_agent_order_reversed========================== [8, 3, 2, 11, 4, 6, 1, 12, 0, 13, 5, 10, 14, 9, 7]
Env powergym Task 123Bus Algo happo Exp test updates 27/333 episodes, total num timesteps 1944/24000, FPS 1.
Average step reward is -3.7480647563934326.
Some episodes done, average episode reward is -89.95354862661833.

Evaluation average episode reward is -74.81954040502764.

fixed_agent_order_reversed========================== [7, 6, 5, 0, 11, 1, 10, 13, 8, 12, 14, 4, 2, 9, 3]
Env powergym Task 123Bus Algo happo Exp test updates 28/333 episodes, total num timesteps 2016/24000, FPS 1.
Average step reward is -3.544799566268921.
Some episodes done, average episode reward is -85.075189567692.

Evaluation average episode reward is -70.21679660709658.

fixed_agent_order_reversed========================== [0, 6, 3, 2, 5, 11, 4, 8, 1, 10, 13, 9, 7, 12, 14]
Env powergym Task 123Bus Algo happo Exp test updates 29/333 episodes, total num timesteps 2088/24000, FPS 1.
Average step reward is -3.572279691696167.
Some episodes done, average episode reward is -85.73470994347899.

Evaluation average episode reward is -47.815994423532885.

fixed_agent_order_reversed========================== [3, 5, 14, 4, 11, 2, 6, 1, 0, 13, 12, 10, 7, 8, 9]
Env powergym Task 123Bus Algo happo Exp test updates 30/333 episodes, total num timesteps 2160/24000, FPS 1.
Average step reward is -3.98453688621521.
Some episodes done, average episode reward is -95.62888505464059.

Evaluation average episode reward is -44.838693366602.

fixed_agent_order_reversed========================== [10, 9, 14, 6, 13, 11, 12, 7, 4, 5, 1, 3, 2, 8, 0]
Env powergym Task 123Bus Algo happo Exp test updates 31/333 episodes, total num timesteps 2232/24000, FPS 1.
Average step reward is -3.618475914001465.
Some episodes done, average episode reward is -86.84341596933496.

Evaluation average episode reward is -52.83185430355811.

fixed_agent_order_reversed========================== [14, 12, 7, 13, 1, 4, 9, 0, 2, 3, 10, 11, 5, 8, 6]
Env powergym Task 123Bus Algo happo Exp test updates 32/333 episodes, total num timesteps 2304/24000, FPS 1.
Average step reward is -3.944524049758911.
Some episodes done, average episode reward is -94.66857891851869.

Evaluation average episode reward is -42.996416638936886.

fixed_agent_order_reversed========================== [7, 12, 5, 0, 13, 1, 14, 10, 4, 6, 9, 2, 11, 3, 8]
Env powergym Task 123Bus Algo happo Exp test updates 33/333 episodes, total num timesteps 2376/24000, FPS 1.
Average step reward is -3.6784451007843018.
Some episodes done, average episode reward is -88.28268042135282.

Evaluation average episode reward is -41.72431710918257.

fixed_agent_order_reversed========================== [2, 4, 11, 8, 3, 1, 9, 6, 0, 10, 12, 5, 13, 14, 7]
Env powergym Task 123Bus Algo happo Exp test updates 34/333 episodes, total num timesteps 2448/24000, FPS 1.
Average step reward is -3.8264966011047363.
Some episodes done, average episode reward is -91.8359250300263.

Evaluation average episode reward is -47.42325509940347.

fixed_agent_order_reversed========================== [6, 5, 11, 8, 7, 0, 1, 10, 9, 4, 3, 2, 13, 12, 14]
Env powergym Task 123Bus Algo happo Exp test updates 35/333 episodes, total num timesteps 2520/24000, FPS 1.
Average step reward is -3.4387617111206055.
Some episodes done, average episode reward is -82.53028330260669.

Evaluation average episode reward is -52.71388272397129.

fixed_agent_order_reversed========================== [14, 13, 9, 12, 10, 7, 0, 3, 4, 2, 1, 5, 8, 11, 6]
Env powergym Task 123Bus Algo happo Exp test updates 36/333 episodes, total num timesteps 2592/24000, FPS 1.
Average step reward is -3.4621634483337402.
Some episodes done, average episode reward is -83.09192176226261.

Evaluation average episode reward is -47.0181451157927.

fixed_agent_order_reversed========================== [14, 12, 7, 13, 0, 8, 9, 1, 5, 2, 4, 10, 3, 11, 6]
Env powergym Task 123Bus Algo happo Exp test updates 37/333 episodes, total num timesteps 2664/24000, FPS 1.
Average step reward is -2.931056261062622.
Some episodes done, average episode reward is -70.34534968715519.

Evaluation average episode reward is -39.560886830201305.

fixed_agent_order_reversed========================== [2, 9, 12, 14, 7, 13, 4, 5, 8, 11, 10, 6, 1, 0, 3]
Env powergym Task 123Bus Algo happo Exp test updates 38/333 episodes, total num timesteps 2736/24000, FPS 1.
Average step reward is -3.2206969261169434.
Some episodes done, average episode reward is -77.2967247413283.

Evaluation average episode reward is -49.512467428481216.

fixed_agent_order_reversed========================== [14, 12, 13, 8, 7, 5, 10, 6, 4, 11, 1, 2, 3, 9, 0]
Env powergym Task 123Bus Algo happo Exp test updates 39/333 episodes, total num timesteps 2808/24000, FPS 1.
Average step reward is -3.065138101577759.
Some episodes done, average episode reward is -73.56330910161934.

Evaluation average episode reward is -48.25595445199394.

fixed_agent_order_reversed========================== [14, 13, 0, 9, 2, 8, 10, 1, 12, 3, 7, 4, 11, 6, 5]
Env powergym Task 123Bus Algo happo Exp test updates 40/333 episodes, total num timesteps 2880/24000, FPS 1.
Average step reward is -2.821492910385132.
Some episodes done, average episode reward is -67.71583637460121.

Evaluation average episode reward is -33.48717088480129.

fixed_agent_order_reversed========================== [6, 9, 8, 11, 5, 4, 7, 10, 13, 3, 2, 1, 12, 14, 0]
Env powergym Task 123Bus Algo happo Exp test updates 41/333 episodes, total num timesteps 2952/24000, FPS 1.
Average step reward is -2.804213523864746.
Some episodes done, average episode reward is -67.30112835445117.

Evaluation average episode reward is -37.43697500913989.

fixed_agent_order_reversed========================== [7, 14, 12, 13, 5, 6, 10, 9, 8, 11, 1, 4, 3, 0, 2]
Env powergym Task 123Bus Algo happo Exp test updates 42/333 episodes, total num timesteps 3024/24000, FPS 1.
Average step reward is -2.7995176315307617.
Some episodes done, average episode reward is -67.18842107150199.

Evaluation average episode reward is -40.99766755507151.

fixed_agent_order_reversed========================== [2, 13, 7, 14, 9, 0, 1, 12, 8, 4, 11, 6, 5, 10, 3]
Env powergym Task 123Bus Algo happo Exp test updates 43/333 episodes, total num timesteps 3096/24000, FPS 1.
Average step reward is -2.814744234085083.
Some episodes done, average episode reward is -67.55386162169437.

Evaluation average episode reward is -38.15186600622863.

fixed_agent_order_reversed========================== [7, 6, 5, 8, 11, 1, 0, 12, 4, 3, 2, 9, 10, 13, 14]
Env powergym Task 123Bus Algo happo Exp test updates 44/333 episodes, total num timesteps 3168/24000, FPS 1.
Average step reward is -2.4337878227233887.
Some episodes done, average episode reward is -58.41090657402183.

Evaluation average episode reward is -38.58824021856972.

fixed_agent_order_reversed========================== [7, 6, 12, 5, 14, 8, 13, 11, 1, 10, 9, 4, 0, 2, 3]
Env powergym Task 123Bus Algo happo Exp test updates 45/333 episodes, total num timesteps 3240/24000, FPS 1.
Average step reward is -2.4135541915893555.
Some episodes done, average episode reward is -57.92530563952903.

Evaluation average episode reward is -37.74444770750458.

fixed_agent_order_reversed========================== [9, 3, 2, 8, 4, 11, 6, 10, 5, 0, 1, 13, 7, 12, 14]
Env powergym Task 123Bus Algo happo Exp test updates 46/333 episodes, total num timesteps 3312/24000, FPS 1.
Average step reward is -2.6070094108581543.
Some episodes done, average episode reward is -62.56822711142212.

Evaluation average episode reward is -28.41452025471008.

fixed_agent_order_reversed========================== [10, 9, 2, 4, 3, 11, 5, 6, 8, 12, 1, 14, 13, 0, 7]
Env powergym Task 123Bus Algo happo Exp test updates 47/333 episodes, total num timesteps 3384/24000, FPS 1.
Average step reward is -2.5764102935791016.
Some episodes done, average episode reward is -61.833847040495435.

Evaluation average episode reward is -31.666905043553317.

fixed_agent_order_reversed========================== [6, 0, 9, 11, 8, 3, 1, 7, 2, 10, 5, 4, 13, 14, 12]
Env powergym Task 123Bus Algo happo Exp test updates 48/333 episodes, total num timesteps 3456/24000, FPS 1.
Average step reward is -2.541771411895752.
Some episodes done, average episode reward is -61.00251208800452.

Evaluation average episode reward is -32.78571166250452.

fixed_agent_order_reversed========================== [7, 5, 6, 1, 0, 11, 10, 4, 13, 8, 3, 12, 14, 9, 2]
Env powergym Task 123Bus Algo happo Exp test updates 49/333 episodes, total num timesteps 3528/24000, FPS 1.
Average step reward is -2.356517791748047.
Some episodes done, average episode reward is -56.55642930634789.

Evaluation average episode reward is -33.71475834319315.

fixed_agent_order_reversed========================== [6, 7, 11, 5, 8, 10, 4, 1, 9, 13, 12, 3, 14, 2, 0]
Env powergym Task 123Bus Algo happo Exp test updates 50/333 episodes, total num timesteps 3600/24000, FPS 1.
Average step reward is -2.360602855682373.
Some episodes done, average episode reward is -56.65446790806751.

Evaluation average episode reward is -34.010481803482726.

fixed_agent_order_reversed========================== [14, 12, 13, 4, 1, 7, 8, 10, 2, 11, 3, 5, 6, 0, 9]
Env powergym Task 123Bus Algo happo Exp test updates 51/333 episodes, total num timesteps 3672/24000, FPS 1.
Average step reward is -2.455977439880371.
Some episodes done, average episode reward is -58.94345395435224.

Evaluation average episode reward is -33.68003078543805.

fixed_agent_order_reversed========================== [9, 10, 4, 11, 6, 3, 13, 7, 5, 1, 8, 2, 12, 0, 14]
Env powergym Task 123Bus Algo happo Exp test updates 52/333 episodes, total num timesteps 3744/24000, FPS 1.
Average step reward is -2.338937997817993.
Some episodes done, average episode reward is -56.1345137243548.

Evaluation average episode reward is -29.082415968152333.

fixed_agent_order_reversed========================== [6, 11, 10, 7, 8, 5, 4, 13, 9, 12, 14, 1, 2, 3, 0]
Env powergym Task 123Bus Algo happo Exp test updates 53/333 episodes, total num timesteps 3816/24000, FPS 1.
Average step reward is -2.084611415863037.
Some episodes done, average episode reward is -50.03067270707902.

Evaluation average episode reward is -33.19695598447294.

fixed_agent_order_reversed========================== [6, 5, 11, 10, 2, 3, 0, 4, 9, 8, 7, 13, 1, 12, 14]
Env powergym Task 123Bus Algo happo Exp test updates 54/333 episodes, total num timesteps 3888/24000, FPS 1.
Average step reward is -2.111726760864258.
Some episodes done, average episode reward is -50.681446536782154.

Evaluation average episode reward is -25.571271312296883.

fixed_agent_order_reversed========================== [7, 6, 5, 11, 12, 1, 14, 0, 10, 13, 4, 3, 2, 8, 9]
Env powergym Task 123Bus Algo happo Exp test updates 55/333 episodes, total num timesteps 3960/24000, FPS 1.
Average step reward is -2.1017203330993652.
Some episodes done, average episode reward is -50.44128681819291.

Evaluation average episode reward is -34.06098715437916.

fixed_agent_order_reversed========================== [6, 7, 5, 11, 13, 0, 1, 14, 8, 12, 10, 2, 4, 9, 3]
Env powergym Task 123Bus Algo happo Exp test updates 56/333 episodes, total num timesteps 4032/24000, FPS 1.
Average step reward is -2.0520918369293213.
Some episodes done, average episode reward is -49.250208861280235.

Evaluation average episode reward is -30.183440189404028.

fixed_agent_order_reversed========================== [0, 10, 14, 3, 7, 9, 13, 1, 4, 12, 2, 5, 11, 6, 8]
Env powergym Task 123Bus Algo happo Exp test updates 57/333 episodes, total num timesteps 4104/24000, FPS 1.
Average step reward is -2.133894920349121.
Some episodes done, average episode reward is -51.21347404647954.

Evaluation average episode reward is -26.23327056052695.

fixed_agent_order_reversed========================== [7, 13, 14, 10, 12, 9, 1, 4, 2, 11, 0, 8, 6, 3, 5]
Env powergym Task 123Bus Algo happo Exp test updates 58/333 episodes, total num timesteps 4176/24000, FPS 1.
Average step reward is -1.9199587106704712.
Some episodes done, average episode reward is -46.07901363508429.

Evaluation average episode reward is -26.102889380461097.

fixed_agent_order_reversed========================== [7, 6, 5, 11, 4, 10, 8, 1, 9, 2, 12, 3, 13, 0, 14]
Env powergym Task 123Bus Algo happo Exp test updates 59/333 episodes, total num timesteps 4248/24000, FPS 1.
Average step reward is -1.8498671054840088.
Some episodes done, average episode reward is -44.39680794791229.

Evaluation average episode reward is -23.265618183955294.

fixed_agent_order_reversed========================== [0, 7, 1, 13, 12, 2, 3, 14, 4, 5, 11, 6, 10, 9, 8]
Env powergym Task 123Bus Algo happo Exp test updates 60/333 episodes, total num timesteps 4320/24000, FPS 1.
Average step reward is -1.9843279123306274.
Some episodes done, average episode reward is -47.62386855991565.

Evaluation average episode reward is -23.265618183955294.

fixed_agent_order_reversed========================== [10, 9, 7, 4, 12, 5, 11, 14, 8, 13, 6, 3, 1, 2, 0]
Env powergym Task 123Bus Algo happo Exp test updates 61/333 episodes, total num timesteps 4392/24000, FPS 1.
Average step reward is -2.02170467376709.
Some episodes done, average episode reward is -48.520909268074035.

Evaluation average episode reward is -26.963201883392006.

fixed_agent_order_reversed========================== [6, 8, 9, 11, 10, 4, 5, 3, 2, 7, 13, 1, 0, 12, 14]
Env powergym Task 123Bus Algo happo Exp test updates 62/333 episodes, total num timesteps 4464/24000, FPS 1.
Average step reward is -1.7054171562194824.
Some episodes done, average episode reward is -40.930008025155466.

Evaluation average episode reward is -34.06296643282614.

fixed_agent_order_reversed========================== [12, 14, 4, 13, 10, 7, 2, 9, 5, 3, 1, 8, 11, 0, 6]
Env powergym Task 123Bus Algo happo Exp test updates 63/333 episodes, total num timesteps 4536/24000, FPS 1.
Average step reward is -2.115595817565918.
Some episodes done, average episode reward is -50.77429741124613.

Evaluation average episode reward is -23.75328929540906.

fixed_agent_order_reversed========================== [7, 1, 5, 4, 2, 11, 0, 3, 12, 9, 6, 8, 13, 10, 14]
Env powergym Task 123Bus Algo happo Exp test updates 64/333 episodes, total num timesteps 4608/24000, FPS 1.
Average step reward is -1.750731348991394.
Some episodes done, average episode reward is -42.01755217248222.

Evaluation average episode reward is -23.315136675885185.

fixed_agent_order_reversed========================== [6, 7, 5, 11, 0, 10, 1, 13, 2, 8, 4, 9, 14, 3, 12]
Env powergym Task 123Bus Algo happo Exp test updates 65/333 episodes, total num timesteps 4680/24000, FPS 1.
Average step reward is -1.719594120979309.
Some episodes done, average episode reward is -41.27025853682239.

Evaluation average episode reward is -23.948603629905623.

fixed_agent_order_reversed========================== [3, 0, 6, 11, 2, 5, 4, 10, 9, 1, 7, 13, 12, 14, 8]
Env powergym Task 123Bus Algo happo Exp test updates 66/333 episodes, total num timesteps 4752/24000, FPS 1.
Average step reward is -1.4831719398498535.
Some episodes done, average episode reward is -35.59612337470849.

Evaluation average episode reward is -27.207981805576463.

fixed_agent_order_reversed========================== [7, 5, 14, 12, 13, 1, 9, 10, 0, 4, 11, 6, 2, 3, 8]
Env powergym Task 123Bus Algo happo Exp test updates 67/333 episodes, total num timesteps 4824/24000, FPS 1.
Average step reward is -1.580959677696228.
Some episodes done, average episode reward is -37.94302971771913.

Evaluation average episode reward is -26.254898660962567.

fixed_agent_order_reversed========================== [9, 3, 4, 11, 2, 6, 5, 10, 8, 1, 13, 0, 7, 12, 14]
Env powergym Task 123Bus Algo happo Exp test updates 68/333 episodes, total num timesteps 4896/24000, FPS 1.
Average step reward is -1.8222593069076538.
Some episodes done, average episode reward is -43.73422456064046.

Evaluation average episode reward is -25.193695039187848.

fixed_agent_order_reversed========================== [6, 5, 14, 11, 8, 9, 7, 10, 4, 12, 13, 2, 1, 3, 0]
Env powergym Task 123Bus Algo happo Exp test updates 69/333 episodes, total num timesteps 4968/24000, FPS 1.
Average step reward is -1.8521859645843506.
Some episodes done, average episode reward is -44.45246125807075.

Evaluation average episode reward is -27.969646789322315.

fixed_agent_order_reversed========================== [12, 14, 13, 7, 4, 9, 10, 5, 1, 8, 2, 3, 11, 0, 6]
Env powergym Task 123Bus Algo happo Exp test updates 70/333 episodes, total num timesteps 5040/24000, FPS 1.
Average step reward is -1.7243531942367554.
Some episodes done, average episode reward is -41.3844764761186.

Evaluation average episode reward is -26.724244266202856.

fixed_agent_order_reversed========================== [6, 7, 5, 11, 0, 8, 14, 13, 12, 1, 2, 10, 4, 3, 9]
Env powergym Task 123Bus Algo happo Exp test updates 71/333 episodes, total num timesteps 5112/24000, FPS 1.
Average step reward is -1.6441993713378906.
Some episodes done, average episode reward is -39.46078284825446.

Evaluation average episode reward is -26.315588571657628.

fixed_agent_order_reversed========================== [12, 14, 5, 2, 8, 4, 13, 7, 10, 1, 11, 3, 9, 0, 6]
Env powergym Task 123Bus Algo happo Exp test updates 72/333 episodes, total num timesteps 5184/24000, FPS 1.
Average step reward is -1.5223751068115234.
Some episodes done, average episode reward is -36.53700298683254.

Evaluation average episode reward is -26.847271497316502.

fixed_agent_order_reversed========================== [6, 8, 11, 9, 5, 10, 3, 7, 4, 1, 0, 13, 2, 12, 14]
Env powergym Task 123Bus Algo happo Exp test updates 73/333 episodes, total num timesteps 5256/24000, FPS 1.
Average step reward is -1.353848934173584.
Some episodes done, average episode reward is -32.49237564088857.

Evaluation average episode reward is -29.363090659305616.

fixed_agent_order_reversed========================== [14, 12, 7, 0, 5, 13, 1, 6, 11, 2, 4, 8, 3, 10, 9]
Env powergym Task 123Bus Algo happo Exp test updates 74/333 episodes, total num timesteps 5328/24000, FPS 1.
Average step reward is -1.3258470296859741.
Some episodes done, average episode reward is -31.820326478971197.

Evaluation average episode reward is -24.14997777465136.

fixed_agent_order_reversed========================== [3, 0, 5, 10, 9, 2, 8, 4, 11, 6, 1, 14, 13, 7, 12]
Env powergym Task 123Bus Algo happo Exp test updates 75/333 episodes, total num timesteps 5400/24000, FPS 1.
Average step reward is -1.5036240816116333.
Some episodes done, average episode reward is -36.08697883539566.

Evaluation average episode reward is -24.70992734231343.

fixed_agent_order_reversed========================== [12, 14, 5, 7, 6, 11, 1, 10, 8, 13, 4, 0, 3, 2, 9]
Env powergym Task 123Bus Algo happo Exp test updates 76/333 episodes, total num timesteps 5472/24000, FPS 1.
Average step reward is -1.3844330310821533.
Some episodes done, average episode reward is -33.226391691030315.

Evaluation average episode reward is -23.740156768868847.

fixed_agent_order_reversed========================== [6, 5, 11, 2, 4, 10, 9, 3, 1, 7, 0, 13, 12, 14, 8]
Env powergym Task 123Bus Algo happo Exp test updates 77/333 episodes, total num timesteps 5544/24000, FPS 1.
Average step reward is -1.2799791097640991.
Some episodes done, average episode reward is -30.71949795027319.

Evaluation average episode reward is -23.31190784531675.

fixed_agent_order_reversed========================== [6, 11, 8, 5, 9, 4, 3, 10, 2, 1, 7, 0, 13, 12, 14]
Env powergym Task 123Bus Algo happo Exp test updates 78/333 episodes, total num timesteps 5616/24000, FPS 1.
Average step reward is -1.3197567462921143.
Some episodes done, average episode reward is -31.67416136905277.

Evaluation average episode reward is -21.94914221499616.

fixed_agent_order_reversed========================== [6, 11, 0, 8, 3, 5, 2, 1, 4, 10, 7, 9, 13, 12, 14]
Env powergym Task 123Bus Algo happo Exp test updates 79/333 episodes, total num timesteps 5688/24000, FPS 1.
Average step reward is -1.2189518213272095.
Some episodes done, average episode reward is -29.254844221224655.

Evaluation average episode reward is -22.44837818332998.

fixed_agent_order_reversed========================== [6, 9, 11, 10, 5, 7, 4, 8, 2, 3, 1, 13, 0, 12, 14]
Env powergym Task 123Bus Algo happo Exp test updates 80/333 episodes, total num timesteps 5760/24000, FPS 1.
Average step reward is -1.29305100440979.
Some episodes done, average episode reward is -31.03322245286725.

Evaluation average episode reward is -22.73548887666244.

fixed_agent_order_reversed========================== [13, 7, 14, 0, 6, 9, 1, 12, 11, 2, 5, 10, 8, 4, 3]
Env powergym Task 123Bus Algo happo Exp test updates 81/333 episodes, total num timesteps 5832/24000, FPS 1.
Average step reward is -1.266653060913086.
Some episodes done, average episode reward is -30.399671926249002.

Evaluation average episode reward is -22.882840427288215.

fixed_agent_order_reversed========================== [10, 3, 6, 11, 8, 4, 5, 9, 2, 14, 1, 12, 13, 7, 0]
Env powergym Task 123Bus Algo happo Exp test updates 82/333 episodes, total num timesteps 5904/24000, FPS 1.
Average step reward is -1.1132380962371826.
Some episodes done, average episode reward is -26.717715232011255.

Evaluation average episode reward is -22.13065462364879.

fixed_agent_order_reversed========================== [10, 9, 4, 3, 12, 5, 14, 2, 11, 13, 8, 1, 7, 6, 0]
Env powergym Task 123Bus Algo happo Exp test updates 83/333 episodes, total num timesteps 5976/24000, FPS 1.
Average step reward is -1.1358981132507324.
Some episodes done, average episode reward is -27.261557871660944.

Evaluation average episode reward is -24.07940840611957.

fixed_agent_order_reversed========================== [3, 10, 5, 4, 11, 0, 6, 1, 9, 7, 2, 13, 12, 14, 8]
Env powergym Task 123Bus Algo happo Exp test updates 84/333 episodes, total num timesteps 6048/24000, FPS 1.
Average step reward is -1.1705453395843506.
Some episodes done, average episode reward is -28.093090563342685.

Evaluation average episode reward is -23.873915050089483.

fixed_agent_order_reversed========================== [7, 10, 9, 6, 5, 11, 3, 13, 4, 1, 0, 12, 14, 2, 8]
Env powergym Task 123Bus Algo happo Exp test updates 85/333 episodes, total num timesteps 6120/24000, FPS 1.
Average step reward is -1.0940405130386353.
Some episodes done, average episode reward is -26.256974906770427.

Evaluation average episode reward is -22.016611829462885.

fixed_agent_order_reversed========================== [7, 12, 13, 9, 14, 1, 4, 10, 3, 2, 11, 0, 8, 6, 5]
Env powergym Task 123Bus Algo happo Exp test updates 86/333 episodes, total num timesteps 6192/24000, FPS 1.
Average step reward is -1.165993571281433.
Some episodes done, average episode reward is -27.9838473272916.

Evaluation average episode reward is -22.515835022086836.

fixed_agent_order_reversed========================== [9, 4, 10, 2, 8, 13, 3, 11, 1, 12, 7, 14, 6, 5, 0]
Env powergym Task 123Bus Algo happo Exp test updates 87/333 episodes, total num timesteps 6264/24000, FPS 1.
Average step reward is -1.0748534202575684.
Some episodes done, average episode reward is -25.796482391926798.

Evaluation average episode reward is -23.086092228758805.

fixed_agent_order_reversed========================== [6, 11, 8, 9, 4, 5, 10, 3, 2, 7, 1, 13, 0, 12, 14]
Env powergym Task 123Bus Algo happo Exp test updates 88/333 episodes, total num timesteps 6336/24000, FPS 1.
Average step reward is -1.054166316986084.
Some episodes done, average episode reward is -25.299990152569695.

Evaluation average episode reward is -23.088276674775102.

fixed_agent_order_reversed========================== [6, 0, 7, 8, 11, 1, 5, 9, 3, 10, 13, 2, 4, 14, 12]
Env powergym Task 123Bus Algo happo Exp test updates 89/333 episodes, total num timesteps 6408/24000, FPS 1.
Average step reward is -1.1437371969223022.
Some episodes done, average episode reward is -27.44969263750819.

Evaluation average episode reward is -19.98212308037152.

fixed_agent_order_reversed========================== [8, 10, 6, 9, 11, 13, 4, 7, 5, 2, 14, 3, 12, 1, 0]
Env powergym Task 123Bus Algo happo Exp test updates 90/333 episodes, total num timesteps 6480/24000, FPS 1.
Average step reward is -1.1853612661361694.
Some episodes done, average episode reward is -28.448669368206684.

Evaluation average episode reward is -22.774948139806956.

fixed_agent_order_reversed========================== [6, 11, 8, 5, 4, 2, 3, 9, 1, 10, 0, 7, 13, 12, 14]
Env powergym Task 123Bus Algo happo Exp test updates 91/333 episodes, total num timesteps 6552/24000, FPS 1.
Average step reward is -1.0133395195007324.
Some episodes done, average episode reward is -24.320149461790567.

Evaluation average episode reward is -21.161129317391616.

fixed_agent_order_reversed========================== [14, 12, 7, 13, 5, 1, 10, 4, 0, 2, 11, 8, 3, 9, 6]
Env powergym Task 123Bus Algo happo Exp test updates 92/333 episodes, total num timesteps 6624/24000, FPS 1.
Average step reward is -1.0931596755981445.
Some episodes done, average episode reward is -26.235833874509144.

Evaluation average episode reward is -19.6247703778167.

fixed_agent_order_reversed========================== [5, 13, 7, 14, 0, 2, 1, 12, 4, 9, 11, 6, 3, 10, 8]
Env powergym Task 123Bus Algo happo Exp test updates 93/333 episodes, total num timesteps 6696/24000, FPS 1.
Average step reward is -0.9719769954681396.
Some episodes done, average episode reward is -23.327448689448953.

Evaluation average episode reward is -19.777957467419096.

fixed_agent_order_reversed========================== [6, 5, 11, 7, 10, 8, 13, 14, 9, 12, 4, 1, 2, 0, 3]
Env powergym Task 123Bus Algo happo Exp test updates 94/333 episodes, total num timesteps 6768/24000, FPS 1.
Average step reward is -1.0079312324523926.
Some episodes done, average episode reward is -24.190348574820632.

Evaluation average episode reward is -19.838566048660685.

fixed_agent_order_reversed========================== [14, 10, 0, 7, 13, 5, 3, 9, 12, 1, 4, 2, 11, 6, 8]
Env powergym Task 123Bus Algo happo Exp test updates 95/333 episodes, total num timesteps 6840/24000, FPS 1.
Average step reward is -0.9880425333976746.
Some episodes done, average episode reward is -23.71301932668077.

Evaluation average episode reward is -19.838566048660685.

fixed_agent_order_reversed========================== [6, 11, 9, 10, 4, 8, 5, 3, 7, 2, 1, 13, 0, 12, 14]
Env powergym Task 123Bus Algo happo Exp test updates 96/333 episodes, total num timesteps 6912/24000, FPS 1.
Average step reward is -1.0113765001296997.
Some episodes done, average episode reward is -24.27303685184381.

Evaluation average episode reward is -20.541440877161886.

fixed_agent_order_reversed========================== [9, 6, 11, 10, 7, 2, 5, 3, 4, 0, 1, 8, 13, 12, 14]
Env powergym Task 123Bus Algo happo Exp test updates 97/333 episodes, total num timesteps 6984/24000, FPS 1.
Average step reward is -1.1127376556396484.
Some episodes done, average episode reward is -26.70570520420979.

Evaluation average episode reward is -20.541440877161886.

fixed_agent_order_reversed========================== [9, 8, 7, 4, 13, 10, 1, 3, 2, 12, 11, 14, 0, 6, 5]
Env powergym Task 123Bus Algo happo Exp test updates 98/333 episodes, total num timesteps 7056/24000, FPS 1.
Average step reward is -1.004687786102295.
Some episodes done, average episode reward is -24.112506614511577.

Evaluation average episode reward is -20.605680628392346.

fixed_agent_order_reversed========================== [7, 8, 9, 4, 13, 11, 10, 6, 1, 5, 12, 2, 14, 3, 0]
Env powergym Task 123Bus Algo happo Exp test updates 99/333 episodes, total num timesteps 7128/24000, FPS 1.
Average step reward is -1.0737196207046509.
Some episodes done, average episode reward is -25.769271453912108.

Evaluation average episode reward is -20.541924209433468.

fixed_agent_order_reversed========================== [14, 9, 4, 10, 8, 12, 11, 2, 3, 13, 6, 5, 1, 7, 0]
Env powergym Task 123Bus Algo happo Exp test updates 100/333 episodes, total num timesteps 7200/24000, FPS 1.
Average step reward is -0.9676253795623779.
Some episodes done, average episode reward is -23.223008640933717.

Evaluation average episode reward is -19.988848336859316.

fixed_agent_order_reversed========================== [6, 5, 8, 11, 10, 7, 9, 14, 4, 3, 2, 13, 1, 0, 12]
Env powergym Task 123Bus Algo happo Exp test updates 101/333 episodes, total num timesteps 7272/24000, FPS 1.
Average step reward is -1.0031077861785889.
Some episodes done, average episode reward is -24.074588081831667.

Evaluation average episode reward is -19.988848336859316.

fixed_agent_order_reversed========================== [6, 11, 0, 5, 2, 1, 3, 8, 7, 4, 10, 13, 9, 12, 14]
Env powergym Task 123Bus Algo happo Exp test updates 102/333 episodes, total num timesteps 7344/24000, FPS 1.
Average step reward is -0.9162939786911011.
Some episodes done, average episode reward is -21.991056835517227.

Evaluation average episode reward is -19.988876052462267.

fixed_agent_order_reversed========================== [7, 8, 10, 6, 9, 5, 11, 4, 1, 13, 2, 12, 3, 0, 14]
Env powergym Task 123Bus Algo happo Exp test updates 103/333 episodes, total num timesteps 7416/24000, FPS 1.
Average step reward is -0.9142964482307434.
Some episodes done, average episode reward is -21.943114847561958.

Evaluation average episode reward is -19.98888996668607.

fixed_agent_order_reversed========================== [12, 7, 14, 5, 1, 13, 4, 2, 10, 0, 11, 9, 3, 6, 8]
Env powergym Task 123Bus Algo happo Exp test updates 104/333 episodes, total num timesteps 7488/24000, FPS 1.
Average step reward is -0.9304618835449219.
Some episodes done, average episode reward is -22.331087265045905.

Evaluation average episode reward is -19.988848336859316.

fixed_agent_order_reversed========================== [0, 3, 10, 9, 2, 1, 4, 14, 7, 13, 5, 12, 11, 8, 6]
Env powergym Task 123Bus Algo happo Exp test updates 105/333 episodes, total num timesteps 7560/24000, FPS 1.
Average step reward is -1.0057435035705566.
Some episodes done, average episode reward is -24.137844547135543.

Evaluation average episode reward is -19.988848336859316.

fixed_agent_order_reversed========================== [8, 10, 9, 6, 11, 0, 2, 3, 4, 1, 7, 13, 5, 14, 12]
Env powergym Task 123Bus Algo happo Exp test updates 106/333 episodes, total num timesteps 7632/24000, FPS 1.
Average step reward is -0.9742896556854248.
Some episodes done, average episode reward is -23.382952255368238.

Evaluation average episode reward is -19.988848336859316.

fixed_agent_order_reversed========================== [8, 9, 4, 12, 2, 11, 13, 10, 6, 1, 5, 7, 14, 3, 0]
Env powergym Task 123Bus Algo happo Exp test updates 107/333 episodes, total num timesteps 7704/24000, FPS 1.
Average step reward is -0.9411089420318604.
Some episodes done, average episode reward is -22.586616280312796.

Evaluation average episode reward is -20.17308895153693.

fixed_agent_order_reversed========================== [8, 10, 9, 11, 6, 4, 2, 3, 5, 1, 0, 12, 13, 7, 14]
Env powergym Task 123Bus Algo happo Exp test updates 108/333 episodes, total num timesteps 7776/24000, FPS 1.
Average step reward is -0.9176710844039917.
Some episodes done, average episode reward is -22.024105559980672.

Evaluation average episode reward is -19.988848336859316.

fixed_agent_order_reversed========================== [7, 6, 0, 1, 11, 5, 8, 13, 10, 9, 3, 4, 2, 14, 12]
Env powergym Task 123Bus Algo happo Exp test updates 109/333 episodes, total num timesteps 7848/24000, FPS 1.
Average step reward is -0.9471129179000854.
Some episodes done, average episode reward is -22.73071064547967.

Evaluation average episode reward is -20.113541663491258.

fixed_agent_order_reversed========================== [0, 6, 9, 5, 11, 3, 2, 7, 13, 4, 1, 10, 8, 14, 12]
Env powergym Task 123Bus Algo happo Exp test updates 110/333 episodes, total num timesteps 7920/24000, FPS 1.
Average step reward is -1.0211279392242432.
Some episodes done, average episode reward is -24.507070053549302.

Evaluation average episode reward is -20.17308895153693.

fixed_agent_order_reversed========================== [6, 5, 11, 10, 7, 3, 4, 8, 1, 12, 0, 2, 14, 9, 13]
Env powergym Task 123Bus Algo happo Exp test updates 111/333 episodes, total num timesteps 7992/24000, FPS 1.
Average step reward is -0.9341504573822021.
Some episodes done, average episode reward is -22.419611889967445.

Evaluation average episode reward is -20.17308895153693.

fixed_agent_order_reversed========================== [0, 7, 1, 6, 13, 8, 11, 10, 14, 3, 9, 2, 4, 5, 12]
Env powergym Task 123Bus Algo happo Exp test updates 112/333 episodes, total num timesteps 8064/24000, FPS 1.
Average step reward is -0.9740416407585144.
Some episodes done, average episode reward is -23.37700005958506.

Evaluation average episode reward is -20.17308895153693.

fixed_agent_order_reversed========================== [9, 8, 7, 6, 11, 5, 4, 2, 13, 1, 10, 12, 3, 0, 14]
Env powergym Task 123Bus Algo happo Exp test updates 113/333 episodes, total num timesteps 8136/24000, FPS 1.
Average step reward is -0.959818422794342.
Some episodes done, average episode reward is -23.035643351956598.

Evaluation average episode reward is -20.1731678313563.

fixed_agent_order_reversed========================== [6, 0, 5, 11, 7, 3, 1, 2, 4, 8, 10, 9, 13, 12, 14]
Env powergym Task 123Bus Algo happo Exp test updates 114/333 episodes, total num timesteps 8208/24000, FPS 1.
Average step reward is -0.9762909412384033.
Some episodes done, average episode reward is -23.43098171009578.

Evaluation average episode reward is -20.725563523151454.

fixed_agent_order_reversed========================== [7, 5, 6, 12, 14, 11, 10, 1, 13, 4, 0, 8, 2, 9, 3]
Env powergym Task 123Bus Algo happo Exp test updates 115/333 episodes, total num timesteps 8280/24000, FPS 1.
Average step reward is -0.8909883499145508.
Some episodes done, average episode reward is -21.38372148712789.

Evaluation average episode reward is -20.725563523151454.

fixed_agent_order_reversed========================== [6, 11, 8, 9, 3, 2, 0, 4, 10, 5, 1, 13, 7, 12, 14]
Env powergym Task 123Bus Algo happo Exp test updates 116/333 episodes, total num timesteps 8352/24000, FPS 1.
Average step reward is -0.894532322883606.
Some episodes done, average episode reward is -21.46877579863008.

Evaluation average episode reward is -20.725563523151454.

fixed_agent_order_reversed========================== [12, 14, 4, 8, 5, 11, 6, 9, 10, 13, 2, 3, 7, 1, 0]
Env powergym Task 123Bus Algo happo Exp test updates 117/333 episodes, total num timesteps 8424/24000, FPS 1.
Average step reward is -0.9663679599761963.
Some episodes done, average episode reward is -23.192829700523532.

Evaluation average episode reward is -20.022195461145653.

fixed_agent_order_reversed========================== [6, 10, 11, 7, 5, 8, 9, 3, 4, 1, 0, 13, 2, 14, 12]
Env powergym Task 123Bus Algo happo Exp test updates 118/333 episodes, total num timesteps 8496/24000, FPS 1.
Average step reward is -0.9451792240142822.
Some episodes done, average episode reward is -22.684300987477595.

Evaluation average episode reward is -20.725563523151454.

fixed_agent_order_reversed========================== [7, 6, 5, 11, 1, 8, 10, 0, 13, 12, 14, 4, 3, 9, 2]
Env powergym Task 123Bus Algo happo Exp test updates 119/333 episodes, total num timesteps 8568/24000, FPS 1.
Average step reward is -0.8934847712516785.
Some episodes done, average episode reward is -21.44363501212925.

Evaluation average episode reward is -20.725563523151454.

fixed_agent_order_reversed========================== [8, 7, 6, 9, 5, 11, 4, 10, 13, 2, 12, 1, 14, 3, 0]
Env powergym Task 123Bus Algo happo Exp test updates 120/333 episodes, total num timesteps 8640/24000, FPS 1.
Average step reward is -0.9642791748046875.
Some episodes done, average episode reward is -23.14270052931431.

Evaluation average episode reward is -20.725563523151454.

fixed_agent_order_reversed========================== [5, 4, 12, 8, 9, 11, 7, 6, 10, 1, 2, 3, 14, 13, 0]
Env powergym Task 123Bus Algo happo Exp test updates 121/333 episodes, total num timesteps 8712/24000, FPS 1.
Average step reward is -0.9715527296066284.
Some episodes done, average episode reward is -23.317266529294802.

Evaluation average episode reward is -20.725563523151454.

fixed_agent_order_reversed========================== [6, 11, 4, 5, 12, 10, 2, 3, 1, 9, 13, 14, 7, 0, 8]
Env powergym Task 123Bus Algo happo Exp test updates 122/333 episodes, total num timesteps 8784/24000, FPS 1.
Average step reward is -0.9451499581336975.
Some episodes done, average episode reward is -22.683600183023458.

Evaluation average episode reward is -20.55167387113422.

fixed_agent_order_reversed========================== [9, 3, 10, 8, 4, 2, 11, 5, 0, 1, 13, 6, 7, 12, 14]
Env powergym Task 123Bus Algo happo Exp test updates 123/333 episodes, total num timesteps 8856/24000, FPS 1.
Average step reward is -0.9108405709266663.
Some episodes done, average episode reward is -21.860173821907477.

Evaluation average episode reward is -20.653741317030576.

fixed_agent_order_reversed========================== [3, 0, 2, 1, 8, 4, 7, 10, 11, 9, 5, 12, 6, 14, 13]
Env powergym Task 123Bus Algo happo Exp test updates 124/333 episodes, total num timesteps 8928/24000, FPS 1.
Average step reward is -0.9496567845344543.
Some episodes done, average episode reward is -22.791762528226897.

Evaluation average episode reward is -20.564739834421562.

fixed_agent_order_reversed========================== [9, 3, 2, 4, 11, 6, 8, 10, 13, 5, 1, 0, 12, 14, 7]
Env powergym Task 123Bus Algo happo Exp test updates 125/333 episodes, total num timesteps 9000/24000, FPS 1.
Average step reward is -1.043222427368164.
Some episodes done, average episode reward is -25.03733792638086.

Evaluation average episode reward is -20.113880283989403.

fixed_agent_order_reversed========================== [6, 7, 5, 0, 11, 1, 13, 10, 2, 14, 4, 8, 3, 12, 9]
Env powergym Task 123Bus Algo happo Exp test updates 126/333 episodes, total num timesteps 9072/24000, FPS 1.
Average step reward is -0.9516410827636719.
Some episodes done, average episode reward is -22.839385840644308.

Evaluation average episode reward is -21.038806333467836.

fixed_agent_order_reversed========================== [5, 6, 7, 10, 11, 4, 8, 3, 9, 1, 13, 2, 12, 0, 14]
Env powergym Task 123Bus Algo happo Exp test updates 127/333 episodes, total num timesteps 9144/24000, FPS 1.
Average step reward is -0.9429590702056885.
Some episodes done, average episode reward is -22.631018651742185.

Evaluation average episode reward is -19.75120411582731.

fixed_agent_order_reversed========================== [7, 6, 5, 11, 9, 13, 10, 0, 1, 2, 4, 8, 3, 14, 12]
Env powergym Task 123Bus Algo happo Exp test updates 128/333 episodes, total num timesteps 9216/24000, FPS 1.
Average step reward is -1.046576738357544.
Some episodes done, average episode reward is -25.11783992307517.

Evaluation average episode reward is -20.8255740034201.

fixed_agent_order_reversed========================== [9, 4, 2, 8, 11, 13, 3, 10, 7, 1, 6, 0, 5, 12, 14]
Env powergym Task 123Bus Algo happo Exp test updates 129/333 episodes, total num timesteps 9288/24000, FPS 1.
Average step reward is -0.9353904724121094.
Some episodes done, average episode reward is -22.449373608199057.

Evaluation average episode reward is -19.624722516425198.

fixed_agent_order_reversed========================== [6, 11, 9, 10, 8, 3, 4, 5, 2, 1, 7, 13, 0, 12, 14]
Env powergym Task 123Bus Algo happo Exp test updates 130/333 episodes, total num timesteps 9360/24000, FPS 1.
Average step reward is -1.0224347114562988.
Some episodes done, average episode reward is -24.53843442812725.

Evaluation average episode reward is -19.624722516425198.

fixed_agent_order_reversed========================== [8, 6, 10, 9, 11, 4, 5, 2, 13, 3, 1, 14, 12, 7, 0]
Env powergym Task 123Bus Algo happo Exp test updates 131/333 episodes, total num timesteps 9432/24000, FPS 1.
Average step reward is -0.8872781991958618.
Some episodes done, average episode reward is -21.29467589611315.

Evaluation average episode reward is -19.624722516425198.

fixed_agent_order_reversed========================== [6, 11, 5, 4, 14, 9, 10, 12, 7, 13, 8, 1, 3, 2, 0]
Env powergym Task 123Bus Algo happo Exp test updates 132/333 episodes, total num timesteps 9504/24000, FPS 1.
Average step reward is -0.8915449976921082.
Some episodes done, average episode reward is -21.397078979592937.

Evaluation average episode reward is -19.625860748428945.

fixed_agent_order_reversed========================== [6, 7, 11, 8, 0, 1, 5, 13, 3, 10, 4, 12, 2, 14, 9]
Env powergym Task 123Bus Algo happo Exp test updates 133/333 episodes, total num timesteps 9576/24000, FPS 1.
Average step reward is -0.9356407523155212.
Some episodes done, average episode reward is -22.455380076007174.

Evaluation average episode reward is -19.626393589115292.

fixed_agent_order_reversed========================== [2, 3, 5, 4, 12, 9, 1, 0, 10, 11, 13, 14, 7, 6, 8]
Env powergym Task 123Bus Algo happo Exp test updates 134/333 episodes, total num timesteps 9648/24000, FPS 1.
Average step reward is -0.8826403021812439.
Some episodes done, average episode reward is -21.1833662707356.

Evaluation average episode reward is -19.626393589115292.

fixed_agent_order_reversed========================== [6, 8, 5, 11, 10, 9, 4, 3, 7, 2, 0, 1, 13, 12, 14]
Env powergym Task 123Bus Algo happo Exp test updates 135/333 episodes, total num timesteps 9720/24000, FPS 1.
Average step reward is -0.8971350193023682.
Some episodes done, average episode reward is -21.531239988790798.

Evaluation average episode reward is -19.626393589115292.

fixed_agent_order_reversed========================== [6, 8, 5, 11, 7, 9, 4, 2, 10, 1, 13, 3, 12, 0, 14]
Env powergym Task 123Bus Algo happo Exp test updates 136/333 episodes, total num timesteps 9792/24000, FPS 1.
Average step reward is -0.9052222371101379.
Some episodes done, average episode reward is -21.725334324630534.

Evaluation average episode reward is -19.635073479080678.

fixed_agent_order_reversed========================== [6, 8, 9, 11, 3, 4, 2, 5, 10, 1, 13, 0, 7, 12, 14]
Env powergym Task 123Bus Algo happo Exp test updates 137/333 episodes, total num timesteps 9864/24000, FPS 1.
Average step reward is -0.9045388698577881.
Some episodes done, average episode reward is -21.70893317992726.

Evaluation average episode reward is -19.714762271940625.

fixed_agent_order_reversed========================== [0, 6, 3, 11, 1, 5, 7, 2, 4, 8, 10, 9, 13, 12, 14]
Env powergym Task 123Bus Algo happo Exp test updates 138/333 episodes, total num timesteps 9936/24000, FPS 1.
Average step reward is -0.9370191097259521.
Some episodes done, average episode reward is -22.488459884188728.

Evaluation average episode reward is -19.635073479080678.

fixed_agent_order_reversed========================== [8, 9, 2, 13, 0, 6, 11, 7, 4, 10, 3, 1, 5, 14, 12]
Env powergym Task 123Bus Algo happo Exp test updates 139/333 episodes, total num timesteps 10008/24000, FPS 1.
Average step reward is -0.9359787702560425.
Some episodes done, average episode reward is -22.463489498883135.

Evaluation average episode reward is -19.635073479080678.

fixed_agent_order_reversed========================== [7, 13, 6, 1, 12, 8, 11, 14, 9, 10, 5, 2, 0, 4, 3]
Env powergym Task 123Bus Algo happo Exp test updates 140/333 episodes, total num timesteps 10080/24000, FPS 1.
Average step reward is -0.9478700160980225.
Some episodes done, average episode reward is -22.748881519503072.

Evaluation average episode reward is -19.723050593811042.

fixed_agent_order_reversed========================== [0, 7, 1, 9, 2, 8, 10, 3, 4, 13, 11, 6, 5, 12, 14]
Env powergym Task 123Bus Algo happo Exp test updates 141/333 episodes, total num timesteps 10152/24000, FPS 1.
Average step reward is -1.0228025913238525.
Some episodes done, average episode reward is -24.547260990465947.

Evaluation average episode reward is -20.049741613422096.

fixed_agent_order_reversed========================== [6, 7, 5, 8, 11, 4, 12, 9, 13, 10, 1, 2, 14, 3, 0]
Env powergym Task 123Bus Algo happo Exp test updates 142/333 episodes, total num timesteps 10224/24000, FPS 1.
Average step reward is -0.9471943378448486.
Some episodes done, average episode reward is -22.73266242554232.

Evaluation average episode reward is -19.635073479080678.

fixed_agent_order_reversed========================== [9, 8, 4, 3, 11, 2, 10, 6, 13, 1, 12, 0, 5, 14, 7]
Env powergym Task 123Bus Algo happo Exp test updates 143/333 episodes, total num timesteps 10296/24000, FPS 1.
Average step reward is -0.9597575068473816.
Some episodes done, average episode reward is -23.034179838444985.

Evaluation average episode reward is -21.040920007647326.

fixed_agent_order_reversed========================== [12, 14, 5, 4, 6, 11, 13, 7, 10, 2, 1, 9, 3, 8, 0]
Env powergym Task 123Bus Algo happo Exp test updates 144/333 episodes, total num timesteps 10368/24000, FPS 1.
Average step reward is -1.0046367645263672.
Some episodes done, average episode reward is -24.111282668741666.

Evaluation average episode reward is -20.306560577419365.

fixed_agent_order_reversed========================== [8, 9, 4, 2, 11, 10, 13, 6, 3, 1, 5, 7, 12, 0, 14]
Env powergym Task 123Bus Algo happo Exp test updates 145/333 episodes, total num timesteps 10440/24000, FPS 1.
Average step reward is -0.9433485865592957.
Some episodes done, average episode reward is -22.640364003489868.

Evaluation average episode reward is -19.752875188513087.

fixed_agent_order_reversed========================== [8, 4, 11, 9, 6, 2, 10, 5, 3, 1, 7, 13, 0, 12, 14]
Env powergym Task 123Bus Algo happo Exp test updates 146/333 episodes, total num timesteps 10512/24000, FPS 1.
Average step reward is -0.9459851384162903.
Some episodes done, average episode reward is -22.703644156895248.

Evaluation average episode reward is -19.635073479080678.

fixed_agent_order_reversed========================== [9, 4, 10, 11, 2, 8, 3, 6, 5, 7, 13, 1, 12, 0, 14]
Env powergym Task 123Bus Algo happo Exp test updates 147/333 episodes, total num timesteps 10584/24000, FPS 1.
Average step reward is -0.9524008631706238.
Some episodes done, average episode reward is -22.857620389628124.

Evaluation average episode reward is -20.049741613422096.

fixed_agent_order_reversed========================== [6, 11, 8, 5, 7, 10, 9, 4, 1, 2, 13, 3, 0, 12, 14]
Env powergym Task 123Bus Algo happo Exp test updates 148/333 episodes, total num timesteps 10656/24000, FPS 1.
Average step reward is -0.9759474992752075.
Some episodes done, average episode reward is -23.422738802735555.

Evaluation average episode reward is -20.152481702718514.

fixed_agent_order_reversed========================== [9, 3, 4, 2, 13, 0, 1, 10, 11, 5, 8, 7, 6, 12, 14]
Env powergym Task 123Bus Algo happo Exp test updates 149/333 episodes, total num timesteps 10728/24000, FPS 1.
Average step reward is -0.9443643093109131.
Some episodes done, average episode reward is -22.664742406867735.

Evaluation average episode reward is -19.635073479080678.

fixed_agent_order_reversed========================== [6, 11, 9, 2, 0, 4, 8, 10, 3, 1, 5, 7, 13, 12, 14]
Env powergym Task 123Bus Algo happo Exp test updates 150/333 episodes, total num timesteps 10800/24000, FPS 1.
Average step reward is -0.9373152256011963.
Some episodes done, average episode reward is -22.4955659458772.

Evaluation average episode reward is -19.635073479080678.

fixed_agent_order_reversed========================== [0, 5, 9, 2, 3, 6, 7, 11, 10, 13, 1, 4, 8, 14, 12]
Env powergym Task 123Bus Algo happo Exp test updates 151/333 episodes, total num timesteps 10872/24000, FPS 1.
Average step reward is -0.9218632578849792.
Some episodes done, average episode reward is -22.124717759384595.

Evaluation average episode reward is -20.26425137515258.

fixed_agent_order_reversed========================== [8, 5, 10, 4, 11, 7, 6, 3, 9, 12, 2, 1, 13, 14, 0]
Env powergym Task 123Bus Algo happo Exp test updates 152/333 episodes, total num timesteps 10944/24000, FPS 1.
Average step reward is -0.9962055683135986.
Some episodes done, average episode reward is -23.908933751798383.

Evaluation average episode reward is -20.8255740034201.

fixed_agent_order_reversed========================== [6, 11, 5, 8, 4, 7, 10, 3, 2, 9, 1, 12, 13, 0, 14]
Env powergym Task 123Bus Algo happo Exp test updates 153/333 episodes, total num timesteps 11016/24000, FPS 1.
Average step reward is -0.9318205714225769.
Some episodes done, average episode reward is -22.363691708988654.

Process Process-6:
Traceback (most recent call last):
  File "/home/yushixuan/anaconda3/envs/harl_cupy/lib/python3.9/multiprocessing/process.py", line 315, in _bootstrap
    self.run()
  File "/home/yushixuan/anaconda3/envs/harl_cupy/lib/python3.9/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/home/yushixuan/HARL/harl/envs/env_wrappers.py", line 187, in shareworker
    ob, s_ob, available_actions = env.reset()
  File "/home/yushixuan/HARL/harl/envs/powergym/powergym_env.py", line 107, in reset
    obs = self.unwrap(self.env.reset(load_profile_idx=self.rank))
  File "/home/yushixuan/HARL/harl/envs/powergym/powergym/env.py", line 492, in reset
    self.circuit.reset()
  File "/home/yushixuan/HARL/harl/envs/powergym/powergym/circuit.py", line 113, in reset
    self.compile() # this include resetting regulators in dss
  File "/home/yushixuan/HARL/harl/envs/powergym/powergym/circuit.py", line 98, in compile
    self.dss.Text.Command = "compile " + self.dss_file
  File "/home/yushixuan/anaconda3/envs/harl_cupy/lib/python3.9/site-packages/dss/dss_capi_gr/IText.py", line 22, in Command
    self.CheckForError(self._lib.Text_Set_Command(Value))
  File "/home/yushixuan/anaconda3/envs/harl_cupy/lib/python3.9/site-packages/dss/_cffi_api_util.py", line 81, in CheckForError
    raise DSSException(error_num, self._get_string(self._lib.Error_Get_Description()))
dss._cffi_api_util.DSSException: (584, 'WARNING! Daily load shape: "loadshape_S66c" Not Found.')
Traceback (most recent call last):
  File "/home/yushixuan/HARL/examples/train.py", line 102, in <module>
    main()
  File "/home/yushixuan/HARL/examples/train.py", line 97, in main
    runner.run()
  File "/home/yushixuan/HARL/harl/runners/on_policy_base_runner.py", line 290, in run
    self.eval()
  File "/home/yushixuan/anaconda3/envs/harl_cupy/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/yushixuan/HARL/harl/runners/on_policy_base_runner.py", line 602, in eval
    ) = self.eval_envs.step(eval_actions)
  File "/home/yushixuan/HARL/harl/envs/env_wrappers.py", line 131, in step
    return self.step_wait()
  File "/home/yushixuan/HARL/harl/envs/env_wrappers.py", line 305, in step_wait
    results = [remote.recv() for remote in self.remotes]
  File "/home/yushixuan/HARL/harl/envs/env_wrappers.py", line 305, in <listcomp>
    results = [remote.recv() for remote in self.remotes]
  File "/home/yushixuan/anaconda3/envs/harl_cupy/lib/python3.9/multiprocessing/connection.py", line 250, in recv
    buf = self._recv_bytes()
  File "/home/yushixuan/anaconda3/envs/harl_cupy/lib/python3.9/multiprocessing/connection.py", line 414, in _recv_bytes
    buf = self._recv(4)
  File "/home/yushixuan/anaconda3/envs/harl_cupy/lib/python3.9/multiprocessing/connection.py", line 383, in _recv
    raise EOFError
EOFError
